{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d88abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 16:03:09.688112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 16:03:09.847074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:03:09.847095: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 16:03:10.525798: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:03:10.525853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:03:10.525859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# train with fewer data. ideal data\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8014ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data = pd.read_csv(\"encoded_data.csv\")\n",
    "label = pd.read_csv(\"properity.csv\")\n",
    "\n",
    "#conditions = [\"materials\", \"color\", \"distance\", \"mode\"]\n",
    "conditions = [\"materials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdaa650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>materials</th>\n",
       "      <th>names</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089482</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>0.079908</td>\n",
       "      <td>0.065994</td>\n",
       "      <td>0.068682</td>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.084155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079421</td>\n",
       "      <td>0.099464</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.089496</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.076943</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>0.083896</td>\n",
       "      <td>0.071778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089648</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.080056</td>\n",
       "      <td>0.066129</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.057358</td>\n",
       "      <td>0.079231</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079572</td>\n",
       "      <td>0.099675</td>\n",
       "      <td>0.100986</td>\n",
       "      <td>0.071117</td>\n",
       "      <td>0.089673</td>\n",
       "      <td>0.076840</td>\n",
       "      <td>0.077092</td>\n",
       "      <td>0.090783</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>0.071921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089573</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>0.066069</td>\n",
       "      <td>0.068755</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>0.084251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079504</td>\n",
       "      <td>0.099581</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.089594</td>\n",
       "      <td>0.076770</td>\n",
       "      <td>0.077025</td>\n",
       "      <td>0.090701</td>\n",
       "      <td>0.083990</td>\n",
       "      <td>0.071857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089780</td>\n",
       "      <td>0.065748</td>\n",
       "      <td>0.080169</td>\n",
       "      <td>0.066231</td>\n",
       "      <td>0.068912</td>\n",
       "      <td>0.057443</td>\n",
       "      <td>0.079349</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079691</td>\n",
       "      <td>0.099835</td>\n",
       "      <td>0.101146</td>\n",
       "      <td>0.071222</td>\n",
       "      <td>0.089810</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.090920</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.072032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089572</td>\n",
       "      <td>0.065592</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.068751</td>\n",
       "      <td>0.057305</td>\n",
       "      <td>0.079157</td>\n",
       "      <td>0.084244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079503</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.100891</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.089591</td>\n",
       "      <td>0.076767</td>\n",
       "      <td>0.077022</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.083987</td>\n",
       "      <td>0.071854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.056495</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.040799</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.052561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049842</td>\n",
       "      <td>0.060091</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.044831</td>\n",
       "      <td>0.055677</td>\n",
       "      <td>0.047719</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.056346</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.044298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.057807</td>\n",
       "      <td>0.042224</td>\n",
       "      <td>0.052719</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>0.037122</td>\n",
       "      <td>0.050662</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051016</td>\n",
       "      <td>0.061621</td>\n",
       "      <td>0.062811</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>0.048854</td>\n",
       "      <td>0.049612</td>\n",
       "      <td>0.057691</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.045376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.082298</td>\n",
       "      <td>0.060108</td>\n",
       "      <td>0.073495</td>\n",
       "      <td>0.060240</td>\n",
       "      <td>0.063015</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>0.072471</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072998</td>\n",
       "      <td>0.090507</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.065339</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>0.069988</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.065690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.086783</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.066382</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>0.081428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077046</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.097362</td>\n",
       "      <td>0.068890</td>\n",
       "      <td>0.086555</td>\n",
       "      <td>0.073946</td>\n",
       "      <td>0.074464</td>\n",
       "      <td>0.087356</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>0.069469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.089628</td>\n",
       "      <td>0.065518</td>\n",
       "      <td>0.079724</td>\n",
       "      <td>0.065897</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.057059</td>\n",
       "      <td>0.079081</td>\n",
       "      <td>0.084186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.099373</td>\n",
       "      <td>0.100816</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.076473</td>\n",
       "      <td>0.076930</td>\n",
       "      <td>0.090325</td>\n",
       "      <td>0.083894</td>\n",
       "      <td>0.071876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      materials      names  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0             0  Cardboard    0.089482    0.065524    0.079908    0.065994   \n",
       "1             0  Cardboard    0.089648    0.065651    0.080056    0.066129   \n",
       "2             0  Cardboard    0.089573    0.065594    0.079991    0.066069   \n",
       "3             0  Cardboard    0.089780    0.065748    0.080169    0.066231   \n",
       "4             0  Cardboard    0.089572    0.065592    0.079987    0.066065   \n",
       "...         ...        ...         ...         ...         ...         ...   \n",
       "1255          8       Wood    0.056495    0.041267    0.051591    0.040799   \n",
       "1256          8       Wood    0.057807    0.042224    0.052719    0.041781   \n",
       "1257          8       Wood    0.082298    0.060108    0.073495    0.060240   \n",
       "1258          8       Wood    0.086783    0.063415    0.077305    0.063694   \n",
       "1259          8       Wood    0.089628    0.065518    0.079724    0.065897   \n",
       "\n",
       "      spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0       0.068682    0.057246    0.079075    0.084155  ...     0.079421   \n",
       "1       0.068813    0.057358    0.079231    0.084326  ...     0.079572   \n",
       "2       0.068755    0.057308    0.079161    0.084251  ...     0.079504   \n",
       "3       0.068912    0.057443    0.079349    0.084453  ...     0.079691   \n",
       "4       0.068751    0.057305    0.079157    0.084244  ...     0.079503   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255    0.043777    0.036297    0.049496    0.052561  ...     0.049842   \n",
       "1256    0.044764    0.037122    0.050662    0.053808  ...     0.051016   \n",
       "1257    0.063015    0.052411    0.072471    0.077099  ...     0.072998   \n",
       "1258    0.066382    0.055249    0.076510    0.081428  ...     0.077046   \n",
       "1259    0.068524    0.057059    0.079081    0.084186  ...     0.079618   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0        0.099464     0.100780     0.070991     0.089496     0.076682   \n",
       "1        0.099675     0.100986     0.071117     0.089673     0.076840   \n",
       "2        0.099581     0.100894     0.071059     0.089594     0.076770   \n",
       "3        0.099835     0.101146     0.071222     0.089810     0.076957   \n",
       "4        0.099576     0.100891     0.071060     0.089591     0.076767   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255     0.060091     0.061266     0.044831     0.055677     0.047719   \n",
       "1256     0.061621     0.062811     0.045869     0.057010     0.048854   \n",
       "1257     0.090507     0.091938     0.065339     0.081931     0.069988   \n",
       "1258     0.095920     0.097362     0.068890     0.086555     0.073946   \n",
       "1259     0.099373     0.100816     0.071141     0.089498     0.076473   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0        0.076943     0.090597     0.083896     0.071778  \n",
       "1        0.077092     0.090783     0.084066     0.071921  \n",
       "2        0.077025     0.090701     0.083990     0.071857  \n",
       "3        0.077206     0.090920     0.084196     0.072032  \n",
       "4        0.077022     0.090694     0.083987     0.071854  \n",
       "...           ...          ...          ...          ...  \n",
       "1255     0.048485     0.056346     0.051775     0.044298  \n",
       "1256     0.049612     0.057691     0.053035     0.045376  \n",
       "1257     0.070588     0.082698     0.076683     0.065690  \n",
       "1258     0.074464     0.087356     0.081088     0.069469  \n",
       "1259     0.076930     0.090325     0.083894     0.071876  \n",
       "\n",
       "[1260 rows x 66 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cbfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_data = r_data.drop(columns = conditions)\n",
    "#r_data = pd.concat([label, r_data], axis = 1)\n",
    "\n",
    "e_data = e_data.drop(columns = conditions)\n",
    "e_data = pd.concat([label, e_data], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e46bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data = e_data.drop(columns = [\"Hardness\", \"Rigidity\", \"Strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be607259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>names</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089482</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>0.079908</td>\n",
       "      <td>0.065994</td>\n",
       "      <td>0.068682</td>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.084155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079421</td>\n",
       "      <td>0.099464</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.089496</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.076943</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>0.083896</td>\n",
       "      <td>0.071778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089648</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.080056</td>\n",
       "      <td>0.066129</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.057358</td>\n",
       "      <td>0.079231</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079572</td>\n",
       "      <td>0.099675</td>\n",
       "      <td>0.100986</td>\n",
       "      <td>0.071117</td>\n",
       "      <td>0.089673</td>\n",
       "      <td>0.076840</td>\n",
       "      <td>0.077092</td>\n",
       "      <td>0.090783</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>0.071921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089573</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>0.066069</td>\n",
       "      <td>0.068755</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>0.084251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079504</td>\n",
       "      <td>0.099581</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.089594</td>\n",
       "      <td>0.076770</td>\n",
       "      <td>0.077025</td>\n",
       "      <td>0.090701</td>\n",
       "      <td>0.083990</td>\n",
       "      <td>0.071857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089780</td>\n",
       "      <td>0.065748</td>\n",
       "      <td>0.080169</td>\n",
       "      <td>0.066231</td>\n",
       "      <td>0.068912</td>\n",
       "      <td>0.057443</td>\n",
       "      <td>0.079349</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079691</td>\n",
       "      <td>0.099835</td>\n",
       "      <td>0.101146</td>\n",
       "      <td>0.071222</td>\n",
       "      <td>0.089810</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.090920</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.072032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.089572</td>\n",
       "      <td>0.065592</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.068751</td>\n",
       "      <td>0.057305</td>\n",
       "      <td>0.079157</td>\n",
       "      <td>0.084244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079503</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.100891</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.089591</td>\n",
       "      <td>0.076767</td>\n",
       "      <td>0.077022</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.083987</td>\n",
       "      <td>0.071854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.056495</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.040799</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.052561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049842</td>\n",
       "      <td>0.060091</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.044831</td>\n",
       "      <td>0.055677</td>\n",
       "      <td>0.047719</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.056346</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.044298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.057807</td>\n",
       "      <td>0.042224</td>\n",
       "      <td>0.052719</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>0.037122</td>\n",
       "      <td>0.050662</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051016</td>\n",
       "      <td>0.061621</td>\n",
       "      <td>0.062811</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>0.048854</td>\n",
       "      <td>0.049612</td>\n",
       "      <td>0.057691</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.045376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.082298</td>\n",
       "      <td>0.060108</td>\n",
       "      <td>0.073495</td>\n",
       "      <td>0.060240</td>\n",
       "      <td>0.063015</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>0.072471</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072998</td>\n",
       "      <td>0.090507</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.065339</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>0.069988</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.065690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.086783</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.066382</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>0.081428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077046</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.097362</td>\n",
       "      <td>0.068890</td>\n",
       "      <td>0.086555</td>\n",
       "      <td>0.073946</td>\n",
       "      <td>0.074464</td>\n",
       "      <td>0.087356</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>0.069469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.089628</td>\n",
       "      <td>0.065518</td>\n",
       "      <td>0.079724</td>\n",
       "      <td>0.065897</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.057059</td>\n",
       "      <td>0.079081</td>\n",
       "      <td>0.084186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.099373</td>\n",
       "      <td>0.100816</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.076473</td>\n",
       "      <td>0.076930</td>\n",
       "      <td>0.090325</td>\n",
       "      <td>0.083894</td>\n",
       "      <td>0.071876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Density      names  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0         0.7  Cardboard    0.089482    0.065524    0.079908    0.065994   \n",
       "1         0.7  Cardboard    0.089648    0.065651    0.080056    0.066129   \n",
       "2         0.7  Cardboard    0.089573    0.065594    0.079991    0.066069   \n",
       "3         0.7  Cardboard    0.089780    0.065748    0.080169    0.066231   \n",
       "4         0.7  Cardboard    0.089572    0.065592    0.079987    0.066065   \n",
       "...       ...        ...         ...         ...         ...         ...   \n",
       "1255      0.8       Wood    0.056495    0.041267    0.051591    0.040799   \n",
       "1256      0.8       Wood    0.057807    0.042224    0.052719    0.041781   \n",
       "1257      0.8       Wood    0.082298    0.060108    0.073495    0.060240   \n",
       "1258      0.8       Wood    0.086783    0.063415    0.077305    0.063694   \n",
       "1259      0.8       Wood    0.089628    0.065518    0.079724    0.065897   \n",
       "\n",
       "      spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0       0.068682    0.057246    0.079075    0.084155  ...     0.079421   \n",
       "1       0.068813    0.057358    0.079231    0.084326  ...     0.079572   \n",
       "2       0.068755    0.057308    0.079161    0.084251  ...     0.079504   \n",
       "3       0.068912    0.057443    0.079349    0.084453  ...     0.079691   \n",
       "4       0.068751    0.057305    0.079157    0.084244  ...     0.079503   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255    0.043777    0.036297    0.049496    0.052561  ...     0.049842   \n",
       "1256    0.044764    0.037122    0.050662    0.053808  ...     0.051016   \n",
       "1257    0.063015    0.052411    0.072471    0.077099  ...     0.072998   \n",
       "1258    0.066382    0.055249    0.076510    0.081428  ...     0.077046   \n",
       "1259    0.068524    0.057059    0.079081    0.084186  ...     0.079618   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0        0.099464     0.100780     0.070991     0.089496     0.076682   \n",
       "1        0.099675     0.100986     0.071117     0.089673     0.076840   \n",
       "2        0.099581     0.100894     0.071059     0.089594     0.076770   \n",
       "3        0.099835     0.101146     0.071222     0.089810     0.076957   \n",
       "4        0.099576     0.100891     0.071060     0.089591     0.076767   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255     0.060091     0.061266     0.044831     0.055677     0.047719   \n",
       "1256     0.061621     0.062811     0.045869     0.057010     0.048854   \n",
       "1257     0.090507     0.091938     0.065339     0.081931     0.069988   \n",
       "1258     0.095920     0.097362     0.068890     0.086555     0.073946   \n",
       "1259     0.099373     0.100816     0.071141     0.089498     0.076473   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0        0.076943     0.090597     0.083896     0.071778  \n",
       "1        0.077092     0.090783     0.084066     0.071921  \n",
       "2        0.077025     0.090701     0.083990     0.071857  \n",
       "3        0.077206     0.090920     0.084196     0.072032  \n",
       "4        0.077022     0.090694     0.083987     0.071854  \n",
       "...           ...          ...          ...          ...  \n",
       "1255     0.048485     0.056346     0.051775     0.044298  \n",
       "1256     0.049612     0.057691     0.053035     0.045376  \n",
       "1257     0.070588     0.082698     0.076683     0.065690  \n",
       "1258     0.074464     0.087356     0.081088     0.069469  \n",
       "1259     0.076930     0.090325     0.083894     0.071876  \n",
       "\n",
       "[1260 rows x 66 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cd1d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>spectrum_8</th>\n",
       "      <th>spectrum_9</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089482</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>0.079908</td>\n",
       "      <td>0.065994</td>\n",
       "      <td>0.068682</td>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.084155</td>\n",
       "      <td>0.082014</td>\n",
       "      <td>0.087594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079421</td>\n",
       "      <td>0.099464</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.089496</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.076943</td>\n",
       "      <td>0.090597</td>\n",
       "      <td>0.083896</td>\n",
       "      <td>0.071778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089648</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.080056</td>\n",
       "      <td>0.066129</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.057358</td>\n",
       "      <td>0.079231</td>\n",
       "      <td>0.084326</td>\n",
       "      <td>0.082175</td>\n",
       "      <td>0.087770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079572</td>\n",
       "      <td>0.099675</td>\n",
       "      <td>0.100986</td>\n",
       "      <td>0.071117</td>\n",
       "      <td>0.089673</td>\n",
       "      <td>0.076840</td>\n",
       "      <td>0.077092</td>\n",
       "      <td>0.090783</td>\n",
       "      <td>0.084066</td>\n",
       "      <td>0.071921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089573</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.079991</td>\n",
       "      <td>0.066069</td>\n",
       "      <td>0.068755</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>0.084251</td>\n",
       "      <td>0.082103</td>\n",
       "      <td>0.087692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079504</td>\n",
       "      <td>0.099581</td>\n",
       "      <td>0.100894</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>0.089594</td>\n",
       "      <td>0.076770</td>\n",
       "      <td>0.077025</td>\n",
       "      <td>0.090701</td>\n",
       "      <td>0.083990</td>\n",
       "      <td>0.071857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089780</td>\n",
       "      <td>0.065748</td>\n",
       "      <td>0.080169</td>\n",
       "      <td>0.066231</td>\n",
       "      <td>0.068912</td>\n",
       "      <td>0.057443</td>\n",
       "      <td>0.079349</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.087903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079691</td>\n",
       "      <td>0.099835</td>\n",
       "      <td>0.101146</td>\n",
       "      <td>0.071222</td>\n",
       "      <td>0.089810</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.077206</td>\n",
       "      <td>0.090920</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.072032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089572</td>\n",
       "      <td>0.065592</td>\n",
       "      <td>0.079987</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.068751</td>\n",
       "      <td>0.057305</td>\n",
       "      <td>0.079157</td>\n",
       "      <td>0.084244</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079503</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.100891</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>0.089591</td>\n",
       "      <td>0.076767</td>\n",
       "      <td>0.077022</td>\n",
       "      <td>0.090694</td>\n",
       "      <td>0.083987</td>\n",
       "      <td>0.071854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.056495</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.040799</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.036297</td>\n",
       "      <td>0.049496</td>\n",
       "      <td>0.052561</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>0.054555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049842</td>\n",
       "      <td>0.060091</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.044831</td>\n",
       "      <td>0.055677</td>\n",
       "      <td>0.047719</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.056346</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.044298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0.057807</td>\n",
       "      <td>0.042224</td>\n",
       "      <td>0.052719</td>\n",
       "      <td>0.041781</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>0.037122</td>\n",
       "      <td>0.050662</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>0.052347</td>\n",
       "      <td>0.055856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051016</td>\n",
       "      <td>0.061621</td>\n",
       "      <td>0.062811</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.057010</td>\n",
       "      <td>0.048854</td>\n",
       "      <td>0.049612</td>\n",
       "      <td>0.057691</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.045376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0.082298</td>\n",
       "      <td>0.060108</td>\n",
       "      <td>0.073495</td>\n",
       "      <td>0.060240</td>\n",
       "      <td>0.063015</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>0.072471</td>\n",
       "      <td>0.077099</td>\n",
       "      <td>0.074951</td>\n",
       "      <td>0.080172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072998</td>\n",
       "      <td>0.090507</td>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.065339</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>0.069988</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.082698</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.065690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.086783</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.066382</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>0.081428</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.084691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077046</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.097362</td>\n",
       "      <td>0.068890</td>\n",
       "      <td>0.086555</td>\n",
       "      <td>0.073946</td>\n",
       "      <td>0.074464</td>\n",
       "      <td>0.087356</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>0.069469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.089628</td>\n",
       "      <td>0.065518</td>\n",
       "      <td>0.079724</td>\n",
       "      <td>0.065897</td>\n",
       "      <td>0.068524</td>\n",
       "      <td>0.057059</td>\n",
       "      <td>0.079081</td>\n",
       "      <td>0.084186</td>\n",
       "      <td>0.081814</td>\n",
       "      <td>0.087568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.099373</td>\n",
       "      <td>0.100816</td>\n",
       "      <td>0.071141</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.076473</td>\n",
       "      <td>0.076930</td>\n",
       "      <td>0.090325</td>\n",
       "      <td>0.083894</td>\n",
       "      <td>0.071876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spectrum_0  spectrum_1  spectrum_2  spectrum_3  spectrum_4  spectrum_5  \\\n",
       "0       0.089482    0.065524    0.079908    0.065994    0.068682    0.057246   \n",
       "1       0.089648    0.065651    0.080056    0.066129    0.068813    0.057358   \n",
       "2       0.089573    0.065594    0.079991    0.066069    0.068755    0.057308   \n",
       "3       0.089780    0.065748    0.080169    0.066231    0.068912    0.057443   \n",
       "4       0.089572    0.065592    0.079987    0.066065    0.068751    0.057305   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1255    0.056495    0.041267    0.051591    0.040799    0.043777    0.036297   \n",
       "1256    0.057807    0.042224    0.052719    0.041781    0.044764    0.037122   \n",
       "1257    0.082298    0.060108    0.073495    0.060240    0.063015    0.052411   \n",
       "1258    0.086783    0.063415    0.077305    0.063694    0.066382    0.055249   \n",
       "1259    0.089628    0.065518    0.079724    0.065897    0.068524    0.057059   \n",
       "\n",
       "      spectrum_6  spectrum_7  spectrum_8  spectrum_9  ...  spectrum_54  \\\n",
       "0       0.079075    0.084155    0.082014    0.087594  ...     0.079421   \n",
       "1       0.079231    0.084326    0.082175    0.087770  ...     0.079572   \n",
       "2       0.079161    0.084251    0.082103    0.087692  ...     0.079504   \n",
       "3       0.079349    0.084453    0.082300    0.087903  ...     0.079691   \n",
       "4       0.079157    0.084244    0.082100    0.087687  ...     0.079503   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255    0.049496    0.052561    0.051136    0.054555  ...     0.049842   \n",
       "1256    0.050662    0.053808    0.052347    0.055856  ...     0.051016   \n",
       "1257    0.072471    0.077099    0.074951    0.080172  ...     0.072998   \n",
       "1258    0.076510    0.081428    0.079144    0.084691  ...     0.077046   \n",
       "1259    0.079081    0.084186    0.081814    0.087568  ...     0.079618   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0        0.099464     0.100780     0.070991     0.089496     0.076682   \n",
       "1        0.099675     0.100986     0.071117     0.089673     0.076840   \n",
       "2        0.099581     0.100894     0.071059     0.089594     0.076770   \n",
       "3        0.099835     0.101146     0.071222     0.089810     0.076957   \n",
       "4        0.099576     0.100891     0.071060     0.089591     0.076767   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255     0.060091     0.061266     0.044831     0.055677     0.047719   \n",
       "1256     0.061621     0.062811     0.045869     0.057010     0.048854   \n",
       "1257     0.090507     0.091938     0.065339     0.081931     0.069988   \n",
       "1258     0.095920     0.097362     0.068890     0.086555     0.073946   \n",
       "1259     0.099373     0.100816     0.071141     0.089498     0.076473   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0        0.076943     0.090597     0.083896     0.071778  \n",
       "1        0.077092     0.090783     0.084066     0.071921  \n",
       "2        0.077025     0.090701     0.083990     0.071857  \n",
       "3        0.077206     0.090920     0.084196     0.072032  \n",
       "4        0.077022     0.090694     0.083987     0.071854  \n",
       "...           ...          ...          ...          ...  \n",
       "1255     0.048485     0.056346     0.051775     0.044298  \n",
       "1256     0.049612     0.057691     0.053035     0.045376  \n",
       "1257     0.070588     0.082698     0.076683     0.065690  \n",
       "1258     0.074464     0.087356     0.081088     0.069469  \n",
       "1259     0.076930     0.090325     0.083894     0.071876  \n",
       "\n",
       "[1260 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data \n",
    "tep = e_data.loc[:, [ \"names\", \"Density\"]]\n",
    "e_data_sp = e_data.copy().drop(columns = [\"names\", \"Density\"])\n",
    "e_data_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc17f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>spectrum_8</th>\n",
       "      <th>spectrum_9</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.159539</td>\n",
       "      <td>-0.167979</td>\n",
       "      <td>-0.156021</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.163282</td>\n",
       "      <td>-0.170324</td>\n",
       "      <td>-0.167966</td>\n",
       "      <td>-0.171398</td>\n",
       "      <td>-0.166859</td>\n",
       "      <td>-0.168889</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168056</td>\n",
       "      <td>-0.174726</td>\n",
       "      <td>-0.171880</td>\n",
       "      <td>-0.159286</td>\n",
       "      <td>-0.168512</td>\n",
       "      <td>-0.174677</td>\n",
       "      <td>-0.166697</td>\n",
       "      <td>-0.168368</td>\n",
       "      <td>-0.173626</td>\n",
       "      <td>-0.175064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.155491</td>\n",
       "      <td>-0.163888</td>\n",
       "      <td>-0.151789</td>\n",
       "      <td>-0.170874</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>-0.166124</td>\n",
       "      <td>-0.163812</td>\n",
       "      <td>-0.167141</td>\n",
       "      <td>-0.162722</td>\n",
       "      <td>-0.164675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164013</td>\n",
       "      <td>-0.170522</td>\n",
       "      <td>-0.167766</td>\n",
       "      <td>-0.155434</td>\n",
       "      <td>-0.164359</td>\n",
       "      <td>-0.170417</td>\n",
       "      <td>-0.162549</td>\n",
       "      <td>-0.164069</td>\n",
       "      <td>-0.169486</td>\n",
       "      <td>-0.171028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.157321</td>\n",
       "      <td>-0.165719</td>\n",
       "      <td>-0.153670</td>\n",
       "      <td>-0.172709</td>\n",
       "      <td>-0.160965</td>\n",
       "      <td>-0.167987</td>\n",
       "      <td>-0.165663</td>\n",
       "      <td>-0.169021</td>\n",
       "      <td>-0.164574</td>\n",
       "      <td>-0.166545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165834</td>\n",
       "      <td>-0.172392</td>\n",
       "      <td>-0.169611</td>\n",
       "      <td>-0.157204</td>\n",
       "      <td>-0.166214</td>\n",
       "      <td>-0.172299</td>\n",
       "      <td>-0.164401</td>\n",
       "      <td>-0.165967</td>\n",
       "      <td>-0.171334</td>\n",
       "      <td>-0.172838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.152264</td>\n",
       "      <td>-0.160727</td>\n",
       "      <td>-0.148579</td>\n",
       "      <td>-0.167751</td>\n",
       "      <td>-0.155930</td>\n",
       "      <td>-0.162977</td>\n",
       "      <td>-0.160648</td>\n",
       "      <td>-0.163992</td>\n",
       "      <td>-0.159526</td>\n",
       "      <td>-0.161503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160825</td>\n",
       "      <td>-0.167339</td>\n",
       "      <td>-0.164563</td>\n",
       "      <td>-0.152218</td>\n",
       "      <td>-0.161174</td>\n",
       "      <td>-0.167259</td>\n",
       "      <td>-0.159369</td>\n",
       "      <td>-0.160893</td>\n",
       "      <td>-0.166318</td>\n",
       "      <td>-0.167889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.157334</td>\n",
       "      <td>-0.165798</td>\n",
       "      <td>-0.153781</td>\n",
       "      <td>-0.172846</td>\n",
       "      <td>-0.161079</td>\n",
       "      <td>-0.168096</td>\n",
       "      <td>-0.165788</td>\n",
       "      <td>-0.169188</td>\n",
       "      <td>-0.164640</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165856</td>\n",
       "      <td>-0.172498</td>\n",
       "      <td>-0.169677</td>\n",
       "      <td>-0.157178</td>\n",
       "      <td>-0.166301</td>\n",
       "      <td>-0.172392</td>\n",
       "      <td>-0.164478</td>\n",
       "      <td>-0.166123</td>\n",
       "      <td>-0.171411</td>\n",
       "      <td>-0.172927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>-0.964477</td>\n",
       "      <td>-0.953536</td>\n",
       "      <td>-0.965090</td>\n",
       "      <td>-0.947140</td>\n",
       "      <td>-0.957876</td>\n",
       "      <td>-0.952629</td>\n",
       "      <td>-0.956801</td>\n",
       "      <td>-0.957963</td>\n",
       "      <td>-0.958881</td>\n",
       "      <td>-0.958449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.959392</td>\n",
       "      <td>-0.958310</td>\n",
       "      <td>-0.961537</td>\n",
       "      <td>-0.958796</td>\n",
       "      <td>-0.960160</td>\n",
       "      <td>-0.954432</td>\n",
       "      <td>-0.958483</td>\n",
       "      <td>-0.960002</td>\n",
       "      <td>-0.956393</td>\n",
       "      <td>-0.950714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>-0.932452</td>\n",
       "      <td>-0.922550</td>\n",
       "      <td>-0.932838</td>\n",
       "      <td>-0.917040</td>\n",
       "      <td>-0.926403</td>\n",
       "      <td>-0.921795</td>\n",
       "      <td>-0.925702</td>\n",
       "      <td>-0.926920</td>\n",
       "      <td>-0.927807</td>\n",
       "      <td>-0.927347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927971</td>\n",
       "      <td>-0.927850</td>\n",
       "      <td>-0.930666</td>\n",
       "      <td>-0.927080</td>\n",
       "      <td>-0.928958</td>\n",
       "      <td>-0.923881</td>\n",
       "      <td>-0.927146</td>\n",
       "      <td>-0.928929</td>\n",
       "      <td>-0.925677</td>\n",
       "      <td>-0.920288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>-0.334835</td>\n",
       "      <td>-0.343397</td>\n",
       "      <td>-0.339263</td>\n",
       "      <td>-0.351336</td>\n",
       "      <td>-0.344091</td>\n",
       "      <td>-0.350882</td>\n",
       "      <td>-0.344091</td>\n",
       "      <td>-0.347079</td>\n",
       "      <td>-0.348024</td>\n",
       "      <td>-0.346245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339891</td>\n",
       "      <td>-0.352987</td>\n",
       "      <td>-0.348589</td>\n",
       "      <td>-0.332032</td>\n",
       "      <td>-0.345589</td>\n",
       "      <td>-0.354884</td>\n",
       "      <td>-0.343497</td>\n",
       "      <td>-0.350942</td>\n",
       "      <td>-0.349387</td>\n",
       "      <td>-0.346914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>-0.225390</td>\n",
       "      <td>-0.236305</td>\n",
       "      <td>-0.230411</td>\n",
       "      <td>-0.245506</td>\n",
       "      <td>-0.236667</td>\n",
       "      <td>-0.244873</td>\n",
       "      <td>-0.236357</td>\n",
       "      <td>-0.239290</td>\n",
       "      <td>-0.240464</td>\n",
       "      <td>-0.238262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231592</td>\n",
       "      <td>-0.245256</td>\n",
       "      <td>-0.240199</td>\n",
       "      <td>-0.223483</td>\n",
       "      <td>-0.237354</td>\n",
       "      <td>-0.248322</td>\n",
       "      <td>-0.235656</td>\n",
       "      <td>-0.243277</td>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.240253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>-0.155962</td>\n",
       "      <td>-0.168179</td>\n",
       "      <td>-0.161283</td>\n",
       "      <td>-0.177993</td>\n",
       "      <td>-0.168328</td>\n",
       "      <td>-0.177298</td>\n",
       "      <td>-0.167805</td>\n",
       "      <td>-0.170633</td>\n",
       "      <td>-0.171981</td>\n",
       "      <td>-0.169496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162773</td>\n",
       "      <td>-0.176537</td>\n",
       "      <td>-0.171166</td>\n",
       "      <td>-0.154685</td>\n",
       "      <td>-0.168456</td>\n",
       "      <td>-0.180303</td>\n",
       "      <td>-0.167060</td>\n",
       "      <td>-0.174643</td>\n",
       "      <td>-0.173660</td>\n",
       "      <td>-0.172304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spectrum_0  spectrum_1  spectrum_2  spectrum_3  spectrum_4  spectrum_5  \\\n",
       "0      -0.159539   -0.167979   -0.156021   -0.175000   -0.163282   -0.170324   \n",
       "1      -0.155491   -0.163888   -0.151789   -0.170874   -0.159106   -0.166124   \n",
       "2      -0.157321   -0.165719   -0.153670   -0.172709   -0.160965   -0.167987   \n",
       "3      -0.152264   -0.160727   -0.148579   -0.167751   -0.155930   -0.162977   \n",
       "4      -0.157334   -0.165798   -0.153781   -0.172846   -0.161079   -0.168096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1255   -0.964477   -0.953536   -0.965090   -0.947140   -0.957876   -0.952629   \n",
       "1256   -0.932452   -0.922550   -0.932838   -0.917040   -0.926403   -0.921795   \n",
       "1257   -0.334835   -0.343397   -0.339263   -0.351336   -0.344091   -0.350882   \n",
       "1258   -0.225390   -0.236305   -0.230411   -0.245506   -0.236667   -0.244873   \n",
       "1259   -0.155962   -0.168179   -0.161283   -0.177993   -0.168328   -0.177298   \n",
       "\n",
       "      spectrum_6  spectrum_7  spectrum_8  spectrum_9  ...  spectrum_54  \\\n",
       "0      -0.167966   -0.171398   -0.166859   -0.168889  ...    -0.168056   \n",
       "1      -0.163812   -0.167141   -0.162722   -0.164675  ...    -0.164013   \n",
       "2      -0.165663   -0.169021   -0.164574   -0.166545  ...    -0.165834   \n",
       "3      -0.160648   -0.163992   -0.159526   -0.161503  ...    -0.160825   \n",
       "4      -0.165788   -0.169188   -0.164640   -0.166667  ...    -0.165856   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255   -0.956801   -0.957963   -0.958881   -0.958449  ...    -0.959392   \n",
       "1256   -0.925702   -0.926920   -0.927807   -0.927347  ...    -0.927971   \n",
       "1257   -0.344091   -0.347079   -0.348024   -0.346245  ...    -0.339891   \n",
       "1258   -0.236357   -0.239290   -0.240464   -0.238262  ...    -0.231592   \n",
       "1259   -0.167805   -0.170633   -0.171981   -0.169496  ...    -0.162773   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0       -0.174726    -0.171880    -0.159286    -0.168512    -0.174677   \n",
       "1       -0.170522    -0.167766    -0.155434    -0.164359    -0.170417   \n",
       "2       -0.172392    -0.169611    -0.157204    -0.166214    -0.172299   \n",
       "3       -0.167339    -0.164563    -0.152218    -0.161174    -0.167259   \n",
       "4       -0.172498    -0.169677    -0.157178    -0.166301    -0.172392   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255    -0.958310    -0.961537    -0.958796    -0.960160    -0.954432   \n",
       "1256    -0.927850    -0.930666    -0.927080    -0.928958    -0.923881   \n",
       "1257    -0.352987    -0.348589    -0.332032    -0.345589    -0.354884   \n",
       "1258    -0.245256    -0.240199    -0.223483    -0.237354    -0.248322   \n",
       "1259    -0.176537    -0.171166    -0.154685    -0.168456    -0.180303   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0       -0.166697    -0.168368    -0.173626    -0.175064  \n",
       "1       -0.162549    -0.164069    -0.169486    -0.171028  \n",
       "2       -0.164401    -0.165967    -0.171334    -0.172838  \n",
       "3       -0.159369    -0.160893    -0.166318    -0.167889  \n",
       "4       -0.164478    -0.166123    -0.171411    -0.172927  \n",
       "...           ...          ...          ...          ...  \n",
       "1255    -0.958483    -0.960002    -0.956393    -0.950714  \n",
       "1256    -0.927146    -0.928929    -0.925677    -0.920288  \n",
       "1257    -0.343497    -0.350942    -0.349387    -0.346914  \n",
       "1258    -0.235656    -0.243277    -0.242054    -0.240253  \n",
       "1259    -0.167060    -0.174643    -0.173660    -0.172304  \n",
       "\n",
       "[1260 rows x 64 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data_sp = (e_data_sp - e_data_sp.mean())/e_data_sp.std()\n",
    "e_data_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16fc8849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>Density</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.159539</td>\n",
       "      <td>-0.167979</td>\n",
       "      <td>-0.156021</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.163282</td>\n",
       "      <td>-0.170324</td>\n",
       "      <td>-0.167966</td>\n",
       "      <td>-0.171398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168056</td>\n",
       "      <td>-0.174726</td>\n",
       "      <td>-0.171880</td>\n",
       "      <td>-0.159286</td>\n",
       "      <td>-0.168512</td>\n",
       "      <td>-0.174677</td>\n",
       "      <td>-0.166697</td>\n",
       "      <td>-0.168368</td>\n",
       "      <td>-0.173626</td>\n",
       "      <td>-0.175064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.155491</td>\n",
       "      <td>-0.163888</td>\n",
       "      <td>-0.151789</td>\n",
       "      <td>-0.170874</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>-0.166124</td>\n",
       "      <td>-0.163812</td>\n",
       "      <td>-0.167141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164013</td>\n",
       "      <td>-0.170522</td>\n",
       "      <td>-0.167766</td>\n",
       "      <td>-0.155434</td>\n",
       "      <td>-0.164359</td>\n",
       "      <td>-0.170417</td>\n",
       "      <td>-0.162549</td>\n",
       "      <td>-0.164069</td>\n",
       "      <td>-0.169486</td>\n",
       "      <td>-0.171028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.157321</td>\n",
       "      <td>-0.165719</td>\n",
       "      <td>-0.153670</td>\n",
       "      <td>-0.172709</td>\n",
       "      <td>-0.160965</td>\n",
       "      <td>-0.167987</td>\n",
       "      <td>-0.165663</td>\n",
       "      <td>-0.169021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165834</td>\n",
       "      <td>-0.172392</td>\n",
       "      <td>-0.169611</td>\n",
       "      <td>-0.157204</td>\n",
       "      <td>-0.166214</td>\n",
       "      <td>-0.172299</td>\n",
       "      <td>-0.164401</td>\n",
       "      <td>-0.165967</td>\n",
       "      <td>-0.171334</td>\n",
       "      <td>-0.172838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.152264</td>\n",
       "      <td>-0.160727</td>\n",
       "      <td>-0.148579</td>\n",
       "      <td>-0.167751</td>\n",
       "      <td>-0.155930</td>\n",
       "      <td>-0.162977</td>\n",
       "      <td>-0.160648</td>\n",
       "      <td>-0.163992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160825</td>\n",
       "      <td>-0.167339</td>\n",
       "      <td>-0.164563</td>\n",
       "      <td>-0.152218</td>\n",
       "      <td>-0.161174</td>\n",
       "      <td>-0.167259</td>\n",
       "      <td>-0.159369</td>\n",
       "      <td>-0.160893</td>\n",
       "      <td>-0.166318</td>\n",
       "      <td>-0.167889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.157334</td>\n",
       "      <td>-0.165798</td>\n",
       "      <td>-0.153781</td>\n",
       "      <td>-0.172846</td>\n",
       "      <td>-0.161079</td>\n",
       "      <td>-0.168096</td>\n",
       "      <td>-0.165788</td>\n",
       "      <td>-0.169188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165856</td>\n",
       "      <td>-0.172498</td>\n",
       "      <td>-0.169677</td>\n",
       "      <td>-0.157178</td>\n",
       "      <td>-0.166301</td>\n",
       "      <td>-0.172392</td>\n",
       "      <td>-0.164478</td>\n",
       "      <td>-0.166123</td>\n",
       "      <td>-0.171411</td>\n",
       "      <td>-0.172927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.964477</td>\n",
       "      <td>-0.953536</td>\n",
       "      <td>-0.965090</td>\n",
       "      <td>-0.947140</td>\n",
       "      <td>-0.957876</td>\n",
       "      <td>-0.952629</td>\n",
       "      <td>-0.956801</td>\n",
       "      <td>-0.957963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.959392</td>\n",
       "      <td>-0.958310</td>\n",
       "      <td>-0.961537</td>\n",
       "      <td>-0.958796</td>\n",
       "      <td>-0.960160</td>\n",
       "      <td>-0.954432</td>\n",
       "      <td>-0.958483</td>\n",
       "      <td>-0.960002</td>\n",
       "      <td>-0.956393</td>\n",
       "      <td>-0.950714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.932452</td>\n",
       "      <td>-0.922550</td>\n",
       "      <td>-0.932838</td>\n",
       "      <td>-0.917040</td>\n",
       "      <td>-0.926403</td>\n",
       "      <td>-0.921795</td>\n",
       "      <td>-0.925702</td>\n",
       "      <td>-0.926920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927971</td>\n",
       "      <td>-0.927850</td>\n",
       "      <td>-0.930666</td>\n",
       "      <td>-0.927080</td>\n",
       "      <td>-0.928958</td>\n",
       "      <td>-0.923881</td>\n",
       "      <td>-0.927146</td>\n",
       "      <td>-0.928929</td>\n",
       "      <td>-0.925677</td>\n",
       "      <td>-0.920288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.334835</td>\n",
       "      <td>-0.343397</td>\n",
       "      <td>-0.339263</td>\n",
       "      <td>-0.351336</td>\n",
       "      <td>-0.344091</td>\n",
       "      <td>-0.350882</td>\n",
       "      <td>-0.344091</td>\n",
       "      <td>-0.347079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339891</td>\n",
       "      <td>-0.352987</td>\n",
       "      <td>-0.348589</td>\n",
       "      <td>-0.332032</td>\n",
       "      <td>-0.345589</td>\n",
       "      <td>-0.354884</td>\n",
       "      <td>-0.343497</td>\n",
       "      <td>-0.350942</td>\n",
       "      <td>-0.349387</td>\n",
       "      <td>-0.346914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.225390</td>\n",
       "      <td>-0.236305</td>\n",
       "      <td>-0.230411</td>\n",
       "      <td>-0.245506</td>\n",
       "      <td>-0.236667</td>\n",
       "      <td>-0.244873</td>\n",
       "      <td>-0.236357</td>\n",
       "      <td>-0.239290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231592</td>\n",
       "      <td>-0.245256</td>\n",
       "      <td>-0.240199</td>\n",
       "      <td>-0.223483</td>\n",
       "      <td>-0.237354</td>\n",
       "      <td>-0.248322</td>\n",
       "      <td>-0.235656</td>\n",
       "      <td>-0.243277</td>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.240253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.155962</td>\n",
       "      <td>-0.168179</td>\n",
       "      <td>-0.161283</td>\n",
       "      <td>-0.177993</td>\n",
       "      <td>-0.168328</td>\n",
       "      <td>-0.177298</td>\n",
       "      <td>-0.167805</td>\n",
       "      <td>-0.170633</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162773</td>\n",
       "      <td>-0.176537</td>\n",
       "      <td>-0.171166</td>\n",
       "      <td>-0.154685</td>\n",
       "      <td>-0.168456</td>\n",
       "      <td>-0.180303</td>\n",
       "      <td>-0.167060</td>\n",
       "      <td>-0.174643</td>\n",
       "      <td>-0.173660</td>\n",
       "      <td>-0.172304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          names  Density  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0     Cardboard      0.7   -0.159539   -0.167979   -0.156021   -0.175000   \n",
       "1     Cardboard      0.7   -0.155491   -0.163888   -0.151789   -0.170874   \n",
       "2     Cardboard      0.7   -0.157321   -0.165719   -0.153670   -0.172709   \n",
       "3     Cardboard      0.7   -0.152264   -0.160727   -0.148579   -0.167751   \n",
       "4     Cardboard      0.7   -0.157334   -0.165798   -0.153781   -0.172846   \n",
       "...         ...      ...         ...         ...         ...         ...   \n",
       "1255       Wood      0.8   -0.964477   -0.953536   -0.965090   -0.947140   \n",
       "1256       Wood      0.8   -0.932452   -0.922550   -0.932838   -0.917040   \n",
       "1257       Wood      0.8   -0.334835   -0.343397   -0.339263   -0.351336   \n",
       "1258       Wood      0.8   -0.225390   -0.236305   -0.230411   -0.245506   \n",
       "1259       Wood      0.8   -0.155962   -0.168179   -0.161283   -0.177993   \n",
       "\n",
       "      spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0      -0.163282   -0.170324   -0.167966   -0.171398  ...    -0.168056   \n",
       "1      -0.159106   -0.166124   -0.163812   -0.167141  ...    -0.164013   \n",
       "2      -0.160965   -0.167987   -0.165663   -0.169021  ...    -0.165834   \n",
       "3      -0.155930   -0.162977   -0.160648   -0.163992  ...    -0.160825   \n",
       "4      -0.161079   -0.168096   -0.165788   -0.169188  ...    -0.165856   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255   -0.957876   -0.952629   -0.956801   -0.957963  ...    -0.959392   \n",
       "1256   -0.926403   -0.921795   -0.925702   -0.926920  ...    -0.927971   \n",
       "1257   -0.344091   -0.350882   -0.344091   -0.347079  ...    -0.339891   \n",
       "1258   -0.236667   -0.244873   -0.236357   -0.239290  ...    -0.231592   \n",
       "1259   -0.168328   -0.177298   -0.167805   -0.170633  ...    -0.162773   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0       -0.174726    -0.171880    -0.159286    -0.168512    -0.174677   \n",
       "1       -0.170522    -0.167766    -0.155434    -0.164359    -0.170417   \n",
       "2       -0.172392    -0.169611    -0.157204    -0.166214    -0.172299   \n",
       "3       -0.167339    -0.164563    -0.152218    -0.161174    -0.167259   \n",
       "4       -0.172498    -0.169677    -0.157178    -0.166301    -0.172392   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255    -0.958310    -0.961537    -0.958796    -0.960160    -0.954432   \n",
       "1256    -0.927850    -0.930666    -0.927080    -0.928958    -0.923881   \n",
       "1257    -0.352987    -0.348589    -0.332032    -0.345589    -0.354884   \n",
       "1258    -0.245256    -0.240199    -0.223483    -0.237354    -0.248322   \n",
       "1259    -0.176537    -0.171166    -0.154685    -0.168456    -0.180303   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0       -0.166697    -0.168368    -0.173626    -0.175064  \n",
       "1       -0.162549    -0.164069    -0.169486    -0.171028  \n",
       "2       -0.164401    -0.165967    -0.171334    -0.172838  \n",
       "3       -0.159369    -0.160893    -0.166318    -0.167889  \n",
       "4       -0.164478    -0.166123    -0.171411    -0.172927  \n",
       "...           ...          ...          ...          ...  \n",
       "1255    -0.958483    -0.960002    -0.956393    -0.950714  \n",
       "1256    -0.927146    -0.928929    -0.925677    -0.920288  \n",
       "1257    -0.343497    -0.350942    -0.349387    -0.346914  \n",
       "1258    -0.235656    -0.243277    -0.242054    -0.240253  \n",
       "1259    -0.167060    -0.174643    -0.173660    -0.172304  \n",
       "\n",
       "[1260 rows x 66 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data = pd.concat([tep, e_data_sp], axis =1)\n",
    "e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee87d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>Density</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.292534</td>\n",
       "      <td>-0.300286</td>\n",
       "      <td>-0.291942</td>\n",
       "      <td>-0.306939</td>\n",
       "      <td>-0.297430</td>\n",
       "      <td>-0.304344</td>\n",
       "      <td>-0.301487</td>\n",
       "      <td>-0.306735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300214</td>\n",
       "      <td>-0.309393</td>\n",
       "      <td>-0.305264</td>\n",
       "      <td>-0.288061</td>\n",
       "      <td>-0.302434</td>\n",
       "      <td>-0.310094</td>\n",
       "      <td>-0.300604</td>\n",
       "      <td>-0.304811</td>\n",
       "      <td>-0.306966</td>\n",
       "      <td>-0.305819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.287618</td>\n",
       "      <td>-0.295369</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>-0.302013</td>\n",
       "      <td>-0.292422</td>\n",
       "      <td>-0.299344</td>\n",
       "      <td>-0.296504</td>\n",
       "      <td>-0.301664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295328</td>\n",
       "      <td>-0.304382</td>\n",
       "      <td>-0.300315</td>\n",
       "      <td>-0.283320</td>\n",
       "      <td>-0.297450</td>\n",
       "      <td>-0.305057</td>\n",
       "      <td>-0.295628</td>\n",
       "      <td>-0.299704</td>\n",
       "      <td>-0.302012</td>\n",
       "      <td>-0.300959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.282727</td>\n",
       "      <td>-0.290483</td>\n",
       "      <td>-0.281821</td>\n",
       "      <td>-0.297121</td>\n",
       "      <td>-0.287449</td>\n",
       "      <td>-0.294380</td>\n",
       "      <td>-0.291553</td>\n",
       "      <td>-0.296633</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290467</td>\n",
       "      <td>-0.299403</td>\n",
       "      <td>-0.295394</td>\n",
       "      <td>-0.278596</td>\n",
       "      <td>-0.292498</td>\n",
       "      <td>-0.300053</td>\n",
       "      <td>-0.290682</td>\n",
       "      <td>-0.294633</td>\n",
       "      <td>-0.297089</td>\n",
       "      <td>-0.296128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.406851</td>\n",
       "      <td>-0.414486</td>\n",
       "      <td>-0.410242</td>\n",
       "      <td>-0.421234</td>\n",
       "      <td>-0.413865</td>\n",
       "      <td>-0.420654</td>\n",
       "      <td>-0.417174</td>\n",
       "      <td>-0.424429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413702</td>\n",
       "      <td>-0.425751</td>\n",
       "      <td>-0.420115</td>\n",
       "      <td>-0.398176</td>\n",
       "      <td>-0.418260</td>\n",
       "      <td>-0.427389</td>\n",
       "      <td>-0.416313</td>\n",
       "      <td>-0.423566</td>\n",
       "      <td>-0.422016</td>\n",
       "      <td>-0.418531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.412751</td>\n",
       "      <td>-0.420816</td>\n",
       "      <td>-0.417209</td>\n",
       "      <td>-0.427920</td>\n",
       "      <td>-0.420642</td>\n",
       "      <td>-0.427673</td>\n",
       "      <td>-0.423753</td>\n",
       "      <td>-0.431424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419624</td>\n",
       "      <td>-0.432511</td>\n",
       "      <td>-0.426413</td>\n",
       "      <td>-0.403369</td>\n",
       "      <td>-0.424831</td>\n",
       "      <td>-0.434569</td>\n",
       "      <td>-0.422827</td>\n",
       "      <td>-0.430839</td>\n",
       "      <td>-0.428537</td>\n",
       "      <td>-0.424686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.119053</td>\n",
       "      <td>-0.127372</td>\n",
       "      <td>-0.113785</td>\n",
       "      <td>-0.134595</td>\n",
       "      <td>-0.121794</td>\n",
       "      <td>-0.127344</td>\n",
       "      <td>-0.127242</td>\n",
       "      <td>-0.129298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126932</td>\n",
       "      <td>-0.132548</td>\n",
       "      <td>-0.130558</td>\n",
       "      <td>-0.121929</td>\n",
       "      <td>-0.126805</td>\n",
       "      <td>-0.130108</td>\n",
       "      <td>-0.124811</td>\n",
       "      <td>-0.125417</td>\n",
       "      <td>-0.131597</td>\n",
       "      <td>-0.135251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.121945</td>\n",
       "      <td>-0.130221</td>\n",
       "      <td>-0.116683</td>\n",
       "      <td>-0.137422</td>\n",
       "      <td>-0.124655</td>\n",
       "      <td>-0.130178</td>\n",
       "      <td>-0.130098</td>\n",
       "      <td>-0.132142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129781</td>\n",
       "      <td>-0.135418</td>\n",
       "      <td>-0.133434</td>\n",
       "      <td>-0.124814</td>\n",
       "      <td>-0.129670</td>\n",
       "      <td>-0.132948</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>-0.128289</td>\n",
       "      <td>-0.134443</td>\n",
       "      <td>-0.138073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.164673</td>\n",
       "      <td>0.148381</td>\n",
       "      <td>0.159383</td>\n",
       "      <td>0.136714</td>\n",
       "      <td>0.150174</td>\n",
       "      <td>0.139347</td>\n",
       "      <td>0.149821</td>\n",
       "      <td>0.146633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155359</td>\n",
       "      <td>0.143392</td>\n",
       "      <td>0.149755</td>\n",
       "      <td>0.164370</td>\n",
       "      <td>0.150557</td>\n",
       "      <td>0.136975</td>\n",
       "      <td>0.151306</td>\n",
       "      <td>0.145253</td>\n",
       "      <td>0.144289</td>\n",
       "      <td>0.143123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.197028</td>\n",
       "      <td>0.180281</td>\n",
       "      <td>0.191324</td>\n",
       "      <td>0.168492</td>\n",
       "      <td>0.181957</td>\n",
       "      <td>0.170841</td>\n",
       "      <td>0.181880</td>\n",
       "      <td>0.178691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187559</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.182139</td>\n",
       "      <td>0.196616</td>\n",
       "      <td>0.182756</td>\n",
       "      <td>0.168755</td>\n",
       "      <td>0.183308</td>\n",
       "      <td>0.177205</td>\n",
       "      <td>0.176353</td>\n",
       "      <td>0.175129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.175174</td>\n",
       "      <td>0.158710</td>\n",
       "      <td>0.169747</td>\n",
       "      <td>0.146973</td>\n",
       "      <td>0.160502</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.160154</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165761</td>\n",
       "      <td>0.153795</td>\n",
       "      <td>0.160241</td>\n",
       "      <td>0.174931</td>\n",
       "      <td>0.160941</td>\n",
       "      <td>0.147219</td>\n",
       "      <td>0.161655</td>\n",
       "      <td>0.155570</td>\n",
       "      <td>0.154635</td>\n",
       "      <td>0.153449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          names  Density  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0    Cardboard1      0.7   -0.292534   -0.300286   -0.291942   -0.306939   \n",
       "1    Cardboard1      0.7   -0.287618   -0.295369   -0.286865   -0.302013   \n",
       "2    Cardboard1      0.7   -0.282727   -0.290483   -0.281821   -0.297121   \n",
       "3    Cardboard1      0.7   -0.406851   -0.414486   -0.410242   -0.421234   \n",
       "4    Cardboard1      0.7   -0.412751   -0.420816   -0.417209   -0.427920   \n",
       "..          ...      ...         ...         ...         ...         ...   \n",
       "445       Rock1      3.2   -0.119053   -0.127372   -0.113785   -0.134595   \n",
       "446       Rock1      3.2   -0.121945   -0.130221   -0.116683   -0.137422   \n",
       "447       Rock1      3.2    0.164673    0.148381    0.159383    0.136714   \n",
       "448       Rock1      3.2    0.197028    0.180281    0.191324    0.168492   \n",
       "449       Rock1      3.2    0.175174    0.158710    0.169747    0.146973   \n",
       "\n",
       "     spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0     -0.297430   -0.304344   -0.301487   -0.306735  ...    -0.300214   \n",
       "1     -0.292422   -0.299344   -0.296504   -0.301664  ...    -0.295328   \n",
       "2     -0.287449   -0.294380   -0.291553   -0.296633  ...    -0.290467   \n",
       "3     -0.413865   -0.420654   -0.417174   -0.424429  ...    -0.413702   \n",
       "4     -0.420642   -0.427673   -0.423753   -0.431424  ...    -0.419624   \n",
       "..          ...         ...         ...         ...  ...          ...   \n",
       "445   -0.121794   -0.127344   -0.127242   -0.129298  ...    -0.126932   \n",
       "446   -0.124655   -0.130178   -0.130098   -0.132142  ...    -0.129781   \n",
       "447    0.150174    0.139347    0.149821    0.146633  ...     0.155359   \n",
       "448    0.181957    0.170841    0.181880    0.178691  ...     0.187559   \n",
       "449    0.160502    0.149579    0.160154    0.156880  ...     0.165761   \n",
       "\n",
       "     spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0      -0.309393    -0.305264    -0.288061    -0.302434    -0.310094   \n",
       "1      -0.304382    -0.300315    -0.283320    -0.297450    -0.305057   \n",
       "2      -0.299403    -0.295394    -0.278596    -0.292498    -0.300053   \n",
       "3      -0.425751    -0.420115    -0.398176    -0.418260    -0.427389   \n",
       "4      -0.432511    -0.426413    -0.403369    -0.424831    -0.434569   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "445    -0.132548    -0.130558    -0.121929    -0.126805    -0.130108   \n",
       "446    -0.135418    -0.133434    -0.124814    -0.129670    -0.132948   \n",
       "447     0.143392     0.149755     0.164370     0.150557     0.136975   \n",
       "448     0.175653     0.182139     0.196616     0.182756     0.168755   \n",
       "449     0.153795     0.160241     0.174931     0.160941     0.147219   \n",
       "\n",
       "     spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0      -0.300604    -0.304811    -0.306966    -0.305819  \n",
       "1      -0.295628    -0.299704    -0.302012    -0.300959  \n",
       "2      -0.290682    -0.294633    -0.297089    -0.296128  \n",
       "3      -0.416313    -0.423566    -0.422016    -0.418531  \n",
       "4      -0.422827    -0.430839    -0.428537    -0.424686  \n",
       "..           ...          ...          ...          ...  \n",
       "445    -0.124811    -0.125417    -0.131597    -0.135251  \n",
       "446    -0.127669    -0.128289    -0.134443    -0.138073  \n",
       "447     0.151306     0.145253     0.144289     0.143123  \n",
       "448     0.183308     0.177205     0.176353     0.175129  \n",
       "449     0.161655     0.155570     0.154635     0.153449  \n",
       "\n",
       "[450 rows x 66 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [\"Cardboard1\", \"Ceramic1\", \"Glass1\", \"Plastic1\", \"Rock1\"]\n",
    "e_test = e_data[(e_data[\"names\"]==\"Cardboard1\" )|(e_data[\"names\"]==\"Ceramic1\" )|(e_data[\"names\"]==\"Glass1\") |(e_data[\"names\"]==\"Plastic1\") |(e_data[\"names\"]==\"Rock1\")].reset_index(drop=True)\n",
    "e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ff2945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>Density</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.159539</td>\n",
       "      <td>-0.167979</td>\n",
       "      <td>-0.156021</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.163282</td>\n",
       "      <td>-0.170324</td>\n",
       "      <td>-0.167966</td>\n",
       "      <td>-0.171398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168056</td>\n",
       "      <td>-0.174726</td>\n",
       "      <td>-0.171880</td>\n",
       "      <td>-0.159286</td>\n",
       "      <td>-0.168512</td>\n",
       "      <td>-0.174677</td>\n",
       "      <td>-0.166697</td>\n",
       "      <td>-0.168368</td>\n",
       "      <td>-0.173626</td>\n",
       "      <td>-0.175064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.155491</td>\n",
       "      <td>-0.163888</td>\n",
       "      <td>-0.151789</td>\n",
       "      <td>-0.170874</td>\n",
       "      <td>-0.159106</td>\n",
       "      <td>-0.166124</td>\n",
       "      <td>-0.163812</td>\n",
       "      <td>-0.167141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164013</td>\n",
       "      <td>-0.170522</td>\n",
       "      <td>-0.167766</td>\n",
       "      <td>-0.155434</td>\n",
       "      <td>-0.164359</td>\n",
       "      <td>-0.170417</td>\n",
       "      <td>-0.162549</td>\n",
       "      <td>-0.164069</td>\n",
       "      <td>-0.169486</td>\n",
       "      <td>-0.171028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.157321</td>\n",
       "      <td>-0.165719</td>\n",
       "      <td>-0.153670</td>\n",
       "      <td>-0.172709</td>\n",
       "      <td>-0.160965</td>\n",
       "      <td>-0.167987</td>\n",
       "      <td>-0.165663</td>\n",
       "      <td>-0.169021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165834</td>\n",
       "      <td>-0.172392</td>\n",
       "      <td>-0.169611</td>\n",
       "      <td>-0.157204</td>\n",
       "      <td>-0.166214</td>\n",
       "      <td>-0.172299</td>\n",
       "      <td>-0.164401</td>\n",
       "      <td>-0.165967</td>\n",
       "      <td>-0.171334</td>\n",
       "      <td>-0.172838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.152264</td>\n",
       "      <td>-0.160727</td>\n",
       "      <td>-0.148579</td>\n",
       "      <td>-0.167751</td>\n",
       "      <td>-0.155930</td>\n",
       "      <td>-0.162977</td>\n",
       "      <td>-0.160648</td>\n",
       "      <td>-0.163992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160825</td>\n",
       "      <td>-0.167339</td>\n",
       "      <td>-0.164563</td>\n",
       "      <td>-0.152218</td>\n",
       "      <td>-0.161174</td>\n",
       "      <td>-0.167259</td>\n",
       "      <td>-0.159369</td>\n",
       "      <td>-0.160893</td>\n",
       "      <td>-0.166318</td>\n",
       "      <td>-0.167889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.157334</td>\n",
       "      <td>-0.165798</td>\n",
       "      <td>-0.153781</td>\n",
       "      <td>-0.172846</td>\n",
       "      <td>-0.161079</td>\n",
       "      <td>-0.168096</td>\n",
       "      <td>-0.165788</td>\n",
       "      <td>-0.169188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165856</td>\n",
       "      <td>-0.172498</td>\n",
       "      <td>-0.169677</td>\n",
       "      <td>-0.157178</td>\n",
       "      <td>-0.166301</td>\n",
       "      <td>-0.172392</td>\n",
       "      <td>-0.164478</td>\n",
       "      <td>-0.166123</td>\n",
       "      <td>-0.171411</td>\n",
       "      <td>-0.172927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.964477</td>\n",
       "      <td>-0.953536</td>\n",
       "      <td>-0.965090</td>\n",
       "      <td>-0.947140</td>\n",
       "      <td>-0.957876</td>\n",
       "      <td>-0.952629</td>\n",
       "      <td>-0.956801</td>\n",
       "      <td>-0.957963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.959392</td>\n",
       "      <td>-0.958310</td>\n",
       "      <td>-0.961537</td>\n",
       "      <td>-0.958796</td>\n",
       "      <td>-0.960160</td>\n",
       "      <td>-0.954432</td>\n",
       "      <td>-0.958483</td>\n",
       "      <td>-0.960002</td>\n",
       "      <td>-0.956393</td>\n",
       "      <td>-0.950714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.932452</td>\n",
       "      <td>-0.922550</td>\n",
       "      <td>-0.932838</td>\n",
       "      <td>-0.917040</td>\n",
       "      <td>-0.926403</td>\n",
       "      <td>-0.921795</td>\n",
       "      <td>-0.925702</td>\n",
       "      <td>-0.926920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927971</td>\n",
       "      <td>-0.927850</td>\n",
       "      <td>-0.930666</td>\n",
       "      <td>-0.927080</td>\n",
       "      <td>-0.928958</td>\n",
       "      <td>-0.923881</td>\n",
       "      <td>-0.927146</td>\n",
       "      <td>-0.928929</td>\n",
       "      <td>-0.925677</td>\n",
       "      <td>-0.920288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.334835</td>\n",
       "      <td>-0.343397</td>\n",
       "      <td>-0.339263</td>\n",
       "      <td>-0.351336</td>\n",
       "      <td>-0.344091</td>\n",
       "      <td>-0.350882</td>\n",
       "      <td>-0.344091</td>\n",
       "      <td>-0.347079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339891</td>\n",
       "      <td>-0.352987</td>\n",
       "      <td>-0.348589</td>\n",
       "      <td>-0.332032</td>\n",
       "      <td>-0.345589</td>\n",
       "      <td>-0.354884</td>\n",
       "      <td>-0.343497</td>\n",
       "      <td>-0.350942</td>\n",
       "      <td>-0.349387</td>\n",
       "      <td>-0.346914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.225390</td>\n",
       "      <td>-0.236305</td>\n",
       "      <td>-0.230411</td>\n",
       "      <td>-0.245506</td>\n",
       "      <td>-0.236667</td>\n",
       "      <td>-0.244873</td>\n",
       "      <td>-0.236357</td>\n",
       "      <td>-0.239290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231592</td>\n",
       "      <td>-0.245256</td>\n",
       "      <td>-0.240199</td>\n",
       "      <td>-0.223483</td>\n",
       "      <td>-0.237354</td>\n",
       "      <td>-0.248322</td>\n",
       "      <td>-0.235656</td>\n",
       "      <td>-0.243277</td>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.240253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.155962</td>\n",
       "      <td>-0.168179</td>\n",
       "      <td>-0.161283</td>\n",
       "      <td>-0.177993</td>\n",
       "      <td>-0.168328</td>\n",
       "      <td>-0.177298</td>\n",
       "      <td>-0.167805</td>\n",
       "      <td>-0.170633</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162773</td>\n",
       "      <td>-0.176537</td>\n",
       "      <td>-0.171166</td>\n",
       "      <td>-0.154685</td>\n",
       "      <td>-0.168456</td>\n",
       "      <td>-0.180303</td>\n",
       "      <td>-0.167060</td>\n",
       "      <td>-0.174643</td>\n",
       "      <td>-0.173660</td>\n",
       "      <td>-0.172304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         names  Density  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0    Cardboard      0.7   -0.159539   -0.167979   -0.156021   -0.175000   \n",
       "1    Cardboard      0.7   -0.155491   -0.163888   -0.151789   -0.170874   \n",
       "2    Cardboard      0.7   -0.157321   -0.165719   -0.153670   -0.172709   \n",
       "3    Cardboard      0.7   -0.152264   -0.160727   -0.148579   -0.167751   \n",
       "4    Cardboard      0.7   -0.157334   -0.165798   -0.153781   -0.172846   \n",
       "..         ...      ...         ...         ...         ...         ...   \n",
       "805       Wood      0.8   -0.964477   -0.953536   -0.965090   -0.947140   \n",
       "806       Wood      0.8   -0.932452   -0.922550   -0.932838   -0.917040   \n",
       "807       Wood      0.8   -0.334835   -0.343397   -0.339263   -0.351336   \n",
       "808       Wood      0.8   -0.225390   -0.236305   -0.230411   -0.245506   \n",
       "809       Wood      0.8   -0.155962   -0.168179   -0.161283   -0.177993   \n",
       "\n",
       "     spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0     -0.163282   -0.170324   -0.167966   -0.171398  ...    -0.168056   \n",
       "1     -0.159106   -0.166124   -0.163812   -0.167141  ...    -0.164013   \n",
       "2     -0.160965   -0.167987   -0.165663   -0.169021  ...    -0.165834   \n",
       "3     -0.155930   -0.162977   -0.160648   -0.163992  ...    -0.160825   \n",
       "4     -0.161079   -0.168096   -0.165788   -0.169188  ...    -0.165856   \n",
       "..          ...         ...         ...         ...  ...          ...   \n",
       "805   -0.957876   -0.952629   -0.956801   -0.957963  ...    -0.959392   \n",
       "806   -0.926403   -0.921795   -0.925702   -0.926920  ...    -0.927971   \n",
       "807   -0.344091   -0.350882   -0.344091   -0.347079  ...    -0.339891   \n",
       "808   -0.236667   -0.244873   -0.236357   -0.239290  ...    -0.231592   \n",
       "809   -0.168328   -0.177298   -0.167805   -0.170633  ...    -0.162773   \n",
       "\n",
       "     spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0      -0.174726    -0.171880    -0.159286    -0.168512    -0.174677   \n",
       "1      -0.170522    -0.167766    -0.155434    -0.164359    -0.170417   \n",
       "2      -0.172392    -0.169611    -0.157204    -0.166214    -0.172299   \n",
       "3      -0.167339    -0.164563    -0.152218    -0.161174    -0.167259   \n",
       "4      -0.172498    -0.169677    -0.157178    -0.166301    -0.172392   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "805    -0.958310    -0.961537    -0.958796    -0.960160    -0.954432   \n",
       "806    -0.927850    -0.930666    -0.927080    -0.928958    -0.923881   \n",
       "807    -0.352987    -0.348589    -0.332032    -0.345589    -0.354884   \n",
       "808    -0.245256    -0.240199    -0.223483    -0.237354    -0.248322   \n",
       "809    -0.176537    -0.171166    -0.154685    -0.168456    -0.180303   \n",
       "\n",
       "     spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0      -0.166697    -0.168368    -0.173626    -0.175064  \n",
       "1      -0.162549    -0.164069    -0.169486    -0.171028  \n",
       "2      -0.164401    -0.165967    -0.171334    -0.172838  \n",
       "3      -0.159369    -0.160893    -0.166318    -0.167889  \n",
       "4      -0.164478    -0.166123    -0.171411    -0.172927  \n",
       "..           ...          ...          ...          ...  \n",
       "805    -0.958483    -0.960002    -0.956393    -0.950714  \n",
       "806    -0.927146    -0.928929    -0.925677    -0.920288  \n",
       "807    -0.343497    -0.350942    -0.349387    -0.346914  \n",
       "808    -0.235656    -0.243277    -0.242054    -0.240253  \n",
       "809    -0.167060    -0.174643    -0.173660    -0.172304  \n",
       "\n",
       "[810 rows x 66 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train = e_data[(e_data[\"names\"]!=\"Cardboard1\" )&(e_data[\"names\"]!=\"Ceramic1\" )&(e_data[\"names\"]!=\"Glass1\")&(e_data[\"names\"]!=\"Plastic1\") &(e_data[\"names\"]!=\"Rock1\")].reset_index(drop=True)\n",
    "e_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fca8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_test_name = e_test[\"names\"].to_numpy()\n",
    "e_test = e_test.drop(columns = [\"names\"])\n",
    "e_train = e_train.drop(columns = [\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad2f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_test_y = e_test[\"Density\"]\n",
    "e_test_x = e_test.drop(columns = [\"Density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c13ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train_y = e_train[\"Density\"]\n",
    "e_train_x = e_train.drop(columns = [\"Density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f85fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra, x_val, y_tra, y_val = train_test_split(e_train_x, e_train_y, test_size=0.15, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61fbd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_tra.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29dd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n",
    "# Note that, in this example, the we prior distribution is not trainable,\n",
    "# as we fix its parameters.\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.DistributionLambda(\n",
    "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return prior_model\n",
    "\n",
    "\n",
    "# Define variational posterior weight distribution as multivariate Gaussian.\n",
    "# Note that the learnable parameters for this distribution are the means,\n",
    "# variances, and covariances.\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.VariableLayer(\n",
    "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "            ),\n",
    "            tfp.layers.MultivariateNormalTriL(n),\n",
    "        ]\n",
    "    )\n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "learning_rate = 0.0001\n",
    "encoding_dim = 32\n",
    "\n",
    "def bnn():\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(tfp.layers.DenseVariational(\n",
    "            units=encoding_dim,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1 / train_size,\n",
    "            activation=\"sigmoid\",\n",
    "        ))        \n",
    "    \n",
    "    \n",
    "    model.add(tfp.layers.DenseVariational(\n",
    "            units=128,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1 / train_size,\n",
    "            activation=\"sigmoid\",\n",
    "        ))\n",
    "    model.add(tfp.layers.DenseVariational(\n",
    "            units = 32,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1 / train_size,\n",
    "            activation=\"sigmoid\",\n",
    "        )) \n",
    "    \n",
    "    \n",
    "    model.add(tfp.layers.DenseVariational(\n",
    "            units=8,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1 / train_size,\n",
    "            activation=\"sigmoid\",\n",
    "        ))    \n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(tfp.layers.IndependentNormal(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e680f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b359c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 16:03:13.751379: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-24 16:03:13.751398: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-24 16:03:13.751414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (WH-330-3053-R05): /proc/driver/nvidia/version does not exist\n",
      "2023-02-24 16:03:13.751605: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = bnn()\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "    loss=negative_loglikelihood,\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e92893e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/Hui/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 59s 3s/step - loss: 12.6222 - root_mean_squared_error: 3.0792 - val_loss: 11.7909 - val_root_mean_squared_error: 3.0089\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 12.3302 - root_mean_squared_error: 3.3141 - val_loss: 7.5116 - val_root_mean_squared_error: 2.8295\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 11.0294 - root_mean_squared_error: 3.2066 - val_loss: 7.8132 - val_root_mean_squared_error: 2.8497\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 10.9264 - root_mean_squared_error: 3.1640 - val_loss: 8.6653 - val_root_mean_squared_error: 3.0941\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 13.1141 - root_mean_squared_error: 3.1484 - val_loss: 13.0013 - val_root_mean_squared_error: 2.9991\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 13.4340 - root_mean_squared_error: 3.1242 - val_loss: 8.0207 - val_root_mean_squared_error: 2.8563\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 9.7529 - root_mean_squared_error: 3.1380 - val_loss: 8.8383 - val_root_mean_squared_error: 3.0074\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 9.2777 - root_mean_squared_error: 3.2387 - val_loss: 11.0587 - val_root_mean_squared_error: 3.0046\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 10.2045 - root_mean_squared_error: 3.3051 - val_loss: 9.3553 - val_root_mean_squared_error: 2.8691\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 9.6208 - root_mean_squared_error: 3.2284 - val_loss: 8.9874 - val_root_mean_squared_error: 2.9597\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 10.1383 - root_mean_squared_error: 3.2371 - val_loss: 9.2916 - val_root_mean_squared_error: 2.8822\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 9.9269 - root_mean_squared_error: 3.1860 - val_loss: 11.7187 - val_root_mean_squared_error: 2.8874\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 10.6890 - root_mean_squared_error: 3.2063 - val_loss: 10.0770 - val_root_mean_squared_error: 2.9719\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 10.2882 - root_mean_squared_error: 3.1827 - val_loss: 9.9696 - val_root_mean_squared_error: 3.4495\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 10.5672 - root_mean_squared_error: 3.1078 - val_loss: 10.4546 - val_root_mean_squared_error: 2.9861\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 10.9907 - root_mean_squared_error: 3.2633 - val_loss: 11.6899 - val_root_mean_squared_error: 3.2202\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 11.0225 - root_mean_squared_error: 3.5453 - val_loss: 12.7072 - val_root_mean_squared_error: 3.5505\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 11.4883 - root_mean_squared_error: 3.4464 - val_loss: 11.6224 - val_root_mean_squared_error: 3.0151\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 11.5052 - root_mean_squared_error: 3.4395 - val_loss: 11.4190 - val_root_mean_squared_error: 2.6995\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 12.0781 - root_mean_squared_error: 3.4552 - val_loss: 11.7000 - val_root_mean_squared_error: 3.4218\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 12.4928 - root_mean_squared_error: 3.4669 - val_loss: 12.9618 - val_root_mean_squared_error: 3.5317\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 12.4911 - root_mean_squared_error: 3.7491 - val_loss: 12.4877 - val_root_mean_squared_error: 3.0303\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 13.0266 - root_mean_squared_error: 3.6530 - val_loss: 12.8044 - val_root_mean_squared_error: 3.3018\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 13.4719 - root_mean_squared_error: 3.6352 - val_loss: 13.3041 - val_root_mean_squared_error: 2.9331\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 13.5901 - root_mean_squared_error: 3.6273 - val_loss: 13.3369 - val_root_mean_squared_error: 3.5626\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 13.8794 - root_mean_squared_error: 3.4304 - val_loss: 13.9148 - val_root_mean_squared_error: 3.5078\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 14.1601 - root_mean_squared_error: 3.4293 - val_loss: 14.3332 - val_root_mean_squared_error: 3.5729\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 14.8071 - root_mean_squared_error: 3.4741 - val_loss: 14.5991 - val_root_mean_squared_error: 3.3160\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 14.8040 - root_mean_squared_error: 3.6513 - val_loss: 15.0646 - val_root_mean_squared_error: 3.5851\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 15.3278 - root_mean_squared_error: 3.4436 - val_loss: 15.3760 - val_root_mean_squared_error: 3.0938\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 15.5998 - root_mean_squared_error: 3.5045 - val_loss: 15.5953 - val_root_mean_squared_error: 3.7539\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 15.7642 - root_mean_squared_error: 3.7150 - val_loss: 15.9215 - val_root_mean_squared_error: 3.6079\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 16.2295 - root_mean_squared_error: 3.5996 - val_loss: 15.9131 - val_root_mean_squared_error: 3.5934\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 16.4864 - root_mean_squared_error: 3.6756 - val_loss: 16.2095 - val_root_mean_squared_error: 3.8058\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 16.8282 - root_mean_squared_error: 3.7396 - val_loss: 16.5014 - val_root_mean_squared_error: 3.4998\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 17.2771 - root_mean_squared_error: 3.6236 - val_loss: 16.9916 - val_root_mean_squared_error: 3.5000\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 17.2306 - root_mean_squared_error: 3.4717 - val_loss: 16.7253 - val_root_mean_squared_error: 3.4515\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 17.7495 - root_mean_squared_error: 3.6427 - val_loss: 18.1459 - val_root_mean_squared_error: 3.2903\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 17.8625 - root_mean_squared_error: 3.5988 - val_loss: 17.5657 - val_root_mean_squared_error: 3.7494\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 17.9518 - root_mean_squared_error: 3.5536 - val_loss: 18.4131 - val_root_mean_squared_error: 3.5024\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 18.4742 - root_mean_squared_error: 3.6891 - val_loss: 18.9128 - val_root_mean_squared_error: 3.4166\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 18.5202 - root_mean_squared_error: 3.6740 - val_loss: 18.5073 - val_root_mean_squared_error: 3.9970\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 18.8742 - root_mean_squared_error: 3.5692 - val_loss: 19.1460 - val_root_mean_squared_error: 3.6211\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 19.2683 - root_mean_squared_error: 3.6661 - val_loss: 18.3358 - val_root_mean_squared_error: 3.8022\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 19.4016 - root_mean_squared_error: 3.6518 - val_loss: 18.9804 - val_root_mean_squared_error: 3.4044\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 19.7660 - root_mean_squared_error: 3.5979 - val_loss: 19.8603 - val_root_mean_squared_error: 3.3231\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 19.9844 - root_mean_squared_error: 3.5039 - val_loss: 19.6311 - val_root_mean_squared_error: 3.5921\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 55s 2s/step - loss: 20.4383 - root_mean_squared_error: 3.4514 - val_loss: 19.8596 - val_root_mean_squared_error: 3.7012\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 20.5202 - root_mean_squared_error: 3.5922 - val_loss: 20.5555 - val_root_mean_squared_error: 3.3877\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 20.6295 - root_mean_squared_error: 3.6357 - val_loss: 21.6573 - val_root_mean_squared_error: 3.1221\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 21.2192 - root_mean_squared_error: 3.4758 - val_loss: 20.8630 - val_root_mean_squared_error: 3.8357\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 21.0904 - root_mean_squared_error: 3.5527 - val_loss: 21.4490 - val_root_mean_squared_error: 3.1448\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 21.6538 - root_mean_squared_error: 3.6145 - val_loss: 22.0129 - val_root_mean_squared_error: 3.2824\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 21.8711 - root_mean_squared_error: 3.5585 - val_loss: 21.7511 - val_root_mean_squared_error: 3.3417\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 22.0421 - root_mean_squared_error: 3.4426 - val_loss: 21.9925 - val_root_mean_squared_error: 3.6776\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 22.1715 - root_mean_squared_error: 3.3645 - val_loss: 21.6451 - val_root_mean_squared_error: 3.5931\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 22.8133 - root_mean_squared_error: 3.6368 - val_loss: 22.4470 - val_root_mean_squared_error: 4.1073\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 23.0645 - root_mean_squared_error: 3.5356 - val_loss: 21.9481 - val_root_mean_squared_error: 3.4636\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 23.2763 - root_mean_squared_error: 3.5459 - val_loss: 22.4039 - val_root_mean_squared_error: 3.2910\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 22.9702 - root_mean_squared_error: 3.7809 - val_loss: 23.5624 - val_root_mean_squared_error: 3.5747\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 23.1883 - root_mean_squared_error: 3.5533 - val_loss: 23.4556 - val_root_mean_squared_error: 3.4270\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 23.5219 - root_mean_squared_error: 3.6619 - val_loss: 22.4154 - val_root_mean_squared_error: 3.9482\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 23.8098 - root_mean_squared_error: 3.5410 - val_loss: 23.8821 - val_root_mean_squared_error: 3.6182\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 23.7878 - root_mean_squared_error: 3.6322 - val_loss: 23.7784 - val_root_mean_squared_error: 3.6093\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 24.3380 - root_mean_squared_error: 3.5340 - val_loss: 24.3037 - val_root_mean_squared_error: 3.9256\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 24.2421 - root_mean_squared_error: 3.6100 - val_loss: 24.9142 - val_root_mean_squared_error: 3.4606\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 24.8190 - root_mean_squared_error: 3.7118 - val_loss: 24.1385 - val_root_mean_squared_error: 3.4315\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 25.0931 - root_mean_squared_error: 3.4966 - val_loss: 25.7518 - val_root_mean_squared_error: 3.2215\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 25.3460 - root_mean_squared_error: 3.5533 - val_loss: 26.1583 - val_root_mean_squared_error: 3.7032\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 25.2068 - root_mean_squared_error: 3.5281 - val_loss: 25.2786 - val_root_mean_squared_error: 3.6208\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 25.4961 - root_mean_squared_error: 3.5552 - val_loss: 25.7678 - val_root_mean_squared_error: 3.4989\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 25.6864 - root_mean_squared_error: 3.4873 - val_loss: 25.9820 - val_root_mean_squared_error: 3.2695\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 26.0635 - root_mean_squared_error: 3.6419 - val_loss: 25.8561 - val_root_mean_squared_error: 3.7731\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 25.8605 - root_mean_squared_error: 3.5494 - val_loss: 26.2195 - val_root_mean_squared_error: 3.5114\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 26.3138 - root_mean_squared_error: 3.6401 - val_loss: 26.1302 - val_root_mean_squared_error: 3.6554\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 26.3502 - root_mean_squared_error: 3.4085 - val_loss: 26.6765 - val_root_mean_squared_error: 3.4304\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 26.5231 - root_mean_squared_error: 3.5604 - val_loss: 27.0581 - val_root_mean_squared_error: 3.1406\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 26.7081 - root_mean_squared_error: 3.5323 - val_loss: 26.9453 - val_root_mean_squared_error: 3.5924\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 26.9229 - root_mean_squared_error: 3.4589 - val_loss: 27.3233 - val_root_mean_squared_error: 3.1592\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 27.1766 - root_mean_squared_error: 3.2530 - val_loss: 26.5685 - val_root_mean_squared_error: 3.7092\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 27.1170 - root_mean_squared_error: 3.4738 - val_loss: 27.9485 - val_root_mean_squared_error: 3.3251\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 27.7245 - root_mean_squared_error: 3.4348 - val_loss: 27.9371 - val_root_mean_squared_error: 3.6543\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 27.7481 - root_mean_squared_error: 3.6167 - val_loss: 28.6110 - val_root_mean_squared_error: 3.2613\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 27.5529 - root_mean_squared_error: 3.5775 - val_loss: 28.3795 - val_root_mean_squared_error: 3.4149\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 28.3706 - root_mean_squared_error: 3.3800 - val_loss: 28.4474 - val_root_mean_squared_error: 3.5841\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 28.8155 - root_mean_squared_error: 3.4204 - val_loss: 28.1941 - val_root_mean_squared_error: 3.8682\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 28.4851 - root_mean_squared_error: 3.5719 - val_loss: 28.1362 - val_root_mean_squared_error: 2.9312\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 28.9266 - root_mean_squared_error: 3.6570 - val_loss: 28.6937 - val_root_mean_squared_error: 3.4172\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 28.9670 - root_mean_squared_error: 3.4751 - val_loss: 27.5559 - val_root_mean_squared_error: 3.7043\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 29.3320 - root_mean_squared_error: 3.5005 - val_loss: 29.0842 - val_root_mean_squared_error: 3.2575\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 29.3486 - root_mean_squared_error: 3.5492 - val_loss: 28.2946 - val_root_mean_squared_error: 3.7268\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 29.4945 - root_mean_squared_error: 3.5454 - val_loss: 29.2156 - val_root_mean_squared_error: 3.5783\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 29.4246 - root_mean_squared_error: 3.4723 - val_loss: 29.3659 - val_root_mean_squared_error: 3.3939\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 29.5382 - root_mean_squared_error: 3.6659 - val_loss: 29.6251 - val_root_mean_squared_error: 3.5523\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 55s 2s/step - loss: 30.2947 - root_mean_squared_error: 3.4786 - val_loss: 29.8515 - val_root_mean_squared_error: 3.5658\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 30.1580 - root_mean_squared_error: 3.4754 - val_loss: 29.9785 - val_root_mean_squared_error: 3.4190\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 30.0122 - root_mean_squared_error: 3.5227 - val_loss: 29.8531 - val_root_mean_squared_error: 2.9767\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 30.5786 - root_mean_squared_error: 3.4475 - val_loss: 31.0445 - val_root_mean_squared_error: 3.8663\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 30.7501 - root_mean_squared_error: 3.3633 - val_loss: 30.6655 - val_root_mean_squared_error: 3.5122\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 30.5302 - root_mean_squared_error: 3.5006 - val_loss: 30.8172 - val_root_mean_squared_error: 3.3584\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 30.2984 - root_mean_squared_error: 3.6082 - val_loss: 29.8980 - val_root_mean_squared_error: 3.1259\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 30.8892 - root_mean_squared_error: 3.5803 - val_loss: 31.4246 - val_root_mean_squared_error: 3.7364\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 31.2250 - root_mean_squared_error: 3.5249 - val_loss: 31.7369 - val_root_mean_squared_error: 3.5529\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 31.2646 - root_mean_squared_error: 3.6481 - val_loss: 31.2404 - val_root_mean_squared_error: 3.3110\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 31.4924 - root_mean_squared_error: 3.4793 - val_loss: 31.9342 - val_root_mean_squared_error: 3.3447\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 31.5103 - root_mean_squared_error: 3.4265 - val_loss: 31.0208 - val_root_mean_squared_error: 3.2108\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 32.2134 - root_mean_squared_error: 3.6463 - val_loss: 31.3734 - val_root_mean_squared_error: 3.3746\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 55s 3s/step - loss: 31.7587 - root_mean_squared_error: 3.4696 - val_loss: 32.3341 - val_root_mean_squared_error: 3.4589\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 32.0291 - root_mean_squared_error: 3.5123 - val_loss: 32.2884 - val_root_mean_squared_error: 3.2077\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 31.8307 - root_mean_squared_error: 3.3737 - val_loss: 32.6654 - val_root_mean_squared_error: 3.2366\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 32.1000 - root_mean_squared_error: 3.4832 - val_loss: 32.4502 - val_root_mean_squared_error: 3.6260\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 32.6279 - root_mean_squared_error: 3.4830 - val_loss: 32.3966 - val_root_mean_squared_error: 3.3637\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 32.3892 - root_mean_squared_error: 3.5792 - val_loss: 32.2993 - val_root_mean_squared_error: 3.3109\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 32.8163 - root_mean_squared_error: 3.5724 - val_loss: 32.5799 - val_root_mean_squared_error: 3.4802\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 32.8866 - root_mean_squared_error: 3.5030 - val_loss: 33.1473 - val_root_mean_squared_error: 3.4277\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 33.0974 - root_mean_squared_error: 3.3801 - val_loss: 32.0492 - val_root_mean_squared_error: 2.9231\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 32.9726 - root_mean_squared_error: 3.4210 - val_loss: 33.0784 - val_root_mean_squared_error: 3.4453\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 33.2530 - root_mean_squared_error: 3.3870 - val_loss: 32.2633 - val_root_mean_squared_error: 3.4172\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 33.5329 - root_mean_squared_error: 3.4962 - val_loss: 32.5996 - val_root_mean_squared_error: 3.8987\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 33.7464 - root_mean_squared_error: 3.5371 - val_loss: 32.9284 - val_root_mean_squared_error: 3.0184\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 33.7131 - root_mean_squared_error: 3.5543 - val_loss: 34.1175 - val_root_mean_squared_error: 3.6114\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 34.0559 - root_mean_squared_error: 3.5358 - val_loss: 33.7428 - val_root_mean_squared_error: 4.0323\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 33.8185 - root_mean_squared_error: 3.4726 - val_loss: 34.2492 - val_root_mean_squared_error: 3.4069\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 33.8799 - root_mean_squared_error: 3.4979 - val_loss: 34.0656 - val_root_mean_squared_error: 3.3416\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 34.2536 - root_mean_squared_error: 3.5189 - val_loss: 33.8259 - val_root_mean_squared_error: 3.3260\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 34.1007 - root_mean_squared_error: 3.5239 - val_loss: 34.3809 - val_root_mean_squared_error: 3.4059\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 34.0701 - root_mean_squared_error: 3.5236 - val_loss: 34.7039 - val_root_mean_squared_error: 3.5282\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 34.3536 - root_mean_squared_error: 3.4642 - val_loss: 34.7380 - val_root_mean_squared_error: 3.2685\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 34.7108 - root_mean_squared_error: 3.5853 - val_loss: 34.0229 - val_root_mean_squared_error: 3.1302\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 34.4798 - root_mean_squared_error: 3.4103 - val_loss: 34.9734 - val_root_mean_squared_error: 3.3014\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 35.4102 - root_mean_squared_error: 3.4987 - val_loss: 35.2693 - val_root_mean_squared_error: 3.2601\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 35.0234 - root_mean_squared_error: 3.5003 - val_loss: 34.6369 - val_root_mean_squared_error: 3.2358\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 34.7270 - root_mean_squared_error: 3.3528 - val_loss: 34.9846 - val_root_mean_squared_error: 3.2913\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 35.1967 - root_mean_squared_error: 3.5478 - val_loss: 35.9305 - val_root_mean_squared_error: 3.1942\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 35.3263 - root_mean_squared_error: 3.5252 - val_loss: 34.0449 - val_root_mean_squared_error: 3.8418\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 35.1919 - root_mean_squared_error: 3.5249 - val_loss: 35.8913 - val_root_mean_squared_error: 3.6583\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 35.2883 - root_mean_squared_error: 3.4912 - val_loss: 36.0746 - val_root_mean_squared_error: 3.5337\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 35.1055 - root_mean_squared_error: 3.5505 - val_loss: 35.2326 - val_root_mean_squared_error: 3.3023\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 35.2447 - root_mean_squared_error: 3.4399 - val_loss: 36.2436 - val_root_mean_squared_error: 3.2722\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 35.6963 - root_mean_squared_error: 3.4877 - val_loss: 35.5320 - val_root_mean_squared_error: 3.1088\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 35.9293 - root_mean_squared_error: 3.2868 - val_loss: 36.3140 - val_root_mean_squared_error: 3.1510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 36.1304 - root_mean_squared_error: 3.4544 - val_loss: 35.9186 - val_root_mean_squared_error: 3.5739\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 35.8321 - root_mean_squared_error: 3.4949 - val_loss: 36.5938 - val_root_mean_squared_error: 3.8389\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 36.3106 - root_mean_squared_error: 3.4579 - val_loss: 36.3124 - val_root_mean_squared_error: 3.5071\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 36.6027 - root_mean_squared_error: 3.5766 - val_loss: 36.1611 - val_root_mean_squared_error: 3.3263\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 36.6258 - root_mean_squared_error: 3.4442 - val_loss: 36.5610 - val_root_mean_squared_error: 3.3739\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 36.9886 - root_mean_squared_error: 3.4591 - val_loss: 36.5766 - val_root_mean_squared_error: 3.6752\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 36.5102 - root_mean_squared_error: 3.4755 - val_loss: 37.5095 - val_root_mean_squared_error: 3.6021\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.0349 - root_mean_squared_error: 3.5239 - val_loss: 37.6174 - val_root_mean_squared_error: 2.7083\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.0830 - root_mean_squared_error: 3.5200 - val_loss: 37.1177 - val_root_mean_squared_error: 3.6111\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.2107 - root_mean_squared_error: 3.3073 - val_loss: 36.8422 - val_root_mean_squared_error: 3.6816\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 37.1231 - root_mean_squared_error: 3.4351 - val_loss: 37.1163 - val_root_mean_squared_error: 3.3496\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.4492 - root_mean_squared_error: 3.4584 - val_loss: 37.9440 - val_root_mean_squared_error: 3.1467\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.9097 - root_mean_squared_error: 3.3287 - val_loss: 36.7997 - val_root_mean_squared_error: 3.0743\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.2307 - root_mean_squared_error: 3.5571 - val_loss: 37.4290 - val_root_mean_squared_error: 3.4484\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.1030 - root_mean_squared_error: 3.3930 - val_loss: 36.5119 - val_root_mean_squared_error: 3.1751\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.4347 - root_mean_squared_error: 3.5214 - val_loss: 37.1358 - val_root_mean_squared_error: 3.2618\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.7105 - root_mean_squared_error: 3.5651 - val_loss: 37.0950 - val_root_mean_squared_error: 3.4845\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.8321 - root_mean_squared_error: 3.4255 - val_loss: 38.3242 - val_root_mean_squared_error: 3.5932\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 37.9576 - root_mean_squared_error: 3.3770 - val_loss: 37.5022 - val_root_mean_squared_error: 3.5844\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 38.0875 - root_mean_squared_error: 3.4777 - val_loss: 38.5840 - val_root_mean_squared_error: 3.3596\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 37.7937 - root_mean_squared_error: 3.3439 - val_loss: 37.7001 - val_root_mean_squared_error: 3.5319\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 38.3567 - root_mean_squared_error: 3.4373 - val_loss: 37.6384 - val_root_mean_squared_error: 3.6183\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 37.5424 - root_mean_squared_error: 3.4830 - val_loss: 38.4583 - val_root_mean_squared_error: 3.1112\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 38.5172 - root_mean_squared_error: 3.4057 - val_loss: 40.0558 - val_root_mean_squared_error: 3.2797\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 38.4443 - root_mean_squared_error: 3.3845 - val_loss: 38.5338 - val_root_mean_squared_error: 3.2663\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 38.1227 - root_mean_squared_error: 3.3564 - val_loss: 38.5434 - val_root_mean_squared_error: 3.0007\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 38.4387 - root_mean_squared_error: 3.5242 - val_loss: 38.9084 - val_root_mean_squared_error: 3.2964\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.0788 - root_mean_squared_error: 3.4602 - val_loss: 38.7523 - val_root_mean_squared_error: 3.2055\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.4073 - root_mean_squared_error: 3.4235 - val_loss: 38.7586 - val_root_mean_squared_error: 3.5160\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.1409 - root_mean_squared_error: 3.3199 - val_loss: 39.3748 - val_root_mean_squared_error: 3.6457\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.1284 - root_mean_squared_error: 3.2607 - val_loss: 39.0090 - val_root_mean_squared_error: 2.9280\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 38.9881 - root_mean_squared_error: 3.5208 - val_loss: 39.1536 - val_root_mean_squared_error: 3.4508\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.3814 - root_mean_squared_error: 3.3163 - val_loss: 38.1553 - val_root_mean_squared_error: 3.3260\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 38.6985 - root_mean_squared_error: 3.4988 - val_loss: 38.5159 - val_root_mean_squared_error: 3.3121\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 38.9132 - root_mean_squared_error: 3.4621 - val_loss: 37.9227 - val_root_mean_squared_error: 3.5385\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 39.0671 - root_mean_squared_error: 3.3983 - val_loss: 39.5650 - val_root_mean_squared_error: 3.1355\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 39.2221 - root_mean_squared_error: 3.2860 - val_loss: 39.8375 - val_root_mean_squared_error: 3.4330\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.6462 - root_mean_squared_error: 3.3228 - val_loss: 39.8192 - val_root_mean_squared_error: 3.4264\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.4372 - root_mean_squared_error: 3.5377 - val_loss: 40.1120 - val_root_mean_squared_error: 3.1916\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 39.8925 - root_mean_squared_error: 3.5127 - val_loss: 39.1311 - val_root_mean_squared_error: 2.9852\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.0552 - root_mean_squared_error: 3.5881 - val_loss: 39.6421 - val_root_mean_squared_error: 3.4120\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 39.6429 - root_mean_squared_error: 3.3460 - val_loss: 39.5010 - val_root_mean_squared_error: 3.2209\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.1494 - root_mean_squared_error: 3.3125 - val_loss: 41.4232 - val_root_mean_squared_error: 2.9934\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 40.0963 - root_mean_squared_error: 3.4372 - val_loss: 40.0963 - val_root_mean_squared_error: 3.1362\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.1721 - root_mean_squared_error: 3.6264 - val_loss: 39.7292 - val_root_mean_squared_error: 3.1263\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 39.9261 - root_mean_squared_error: 3.4327 - val_loss: 39.3693 - val_root_mean_squared_error: 3.4693\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 40.4292 - root_mean_squared_error: 3.3602 - val_loss: 40.0902 - val_root_mean_squared_error: 3.2607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.3625 - root_mean_squared_error: 3.3416 - val_loss: 39.6416 - val_root_mean_squared_error: 3.3542\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.2212 - root_mean_squared_error: 3.5089 - val_loss: 41.5967 - val_root_mean_squared_error: 3.6460\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 39.9394 - root_mean_squared_error: 3.5377 - val_loss: 40.7947 - val_root_mean_squared_error: 3.4717\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.9038 - root_mean_squared_error: 3.4335 - val_loss: 40.5884 - val_root_mean_squared_error: 3.1093\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.6495 - root_mean_squared_error: 3.5053 - val_loss: 41.7889 - val_root_mean_squared_error: 3.4574\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.9502 - root_mean_squared_error: 3.4260 - val_loss: 41.0986 - val_root_mean_squared_error: 3.4318\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.5489 - root_mean_squared_error: 3.3819 - val_loss: 41.0876 - val_root_mean_squared_error: 2.9745\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 40.4582 - root_mean_squared_error: 3.4711 - val_loss: 40.9401 - val_root_mean_squared_error: 3.1309\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.0889 - root_mean_squared_error: 3.3765 - val_loss: 40.9422 - val_root_mean_squared_error: 3.3574\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 40.6731 - root_mean_squared_error: 3.4574 - val_loss: 41.3438 - val_root_mean_squared_error: 3.3199\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.3782 - root_mean_squared_error: 3.5283 - val_loss: 42.1441 - val_root_mean_squared_error: 3.4346\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.5236 - root_mean_squared_error: 3.3499 - val_loss: 41.1798 - val_root_mean_squared_error: 2.9869\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.7099 - root_mean_squared_error: 3.4206 - val_loss: 41.6675 - val_root_mean_squared_error: 3.3358\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.2864 - root_mean_squared_error: 3.5186 - val_loss: 40.6903 - val_root_mean_squared_error: 3.5069\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.1832 - root_mean_squared_error: 3.7331 - val_loss: 41.6651 - val_root_mean_squared_error: 3.3764\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.1224 - root_mean_squared_error: 3.4466 - val_loss: 41.2741 - val_root_mean_squared_error: 3.2141\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 41.4246 - root_mean_squared_error: 3.5078 - val_loss: 41.8691 - val_root_mean_squared_error: 3.2506\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 40.8622 - root_mean_squared_error: 3.4328 - val_loss: 41.4824 - val_root_mean_squared_error: 3.1819\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 41.4410 - root_mean_squared_error: 3.3864 - val_loss: 41.5251 - val_root_mean_squared_error: 3.3984\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.8054 - root_mean_squared_error: 3.5448 - val_loss: 41.5787 - val_root_mean_squared_error: 3.4316\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.1247 - root_mean_squared_error: 3.5149 - val_loss: 40.8944 - val_root_mean_squared_error: 3.4992\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 41.6737 - root_mean_squared_error: 3.5178 - val_loss: 41.3960 - val_root_mean_squared_error: 3.0337\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 41.4678 - root_mean_squared_error: 3.3888 - val_loss: 41.6529 - val_root_mean_squared_error: 3.2892\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 42.3023 - root_mean_squared_error: 3.2333 - val_loss: 42.0505 - val_root_mean_squared_error: 3.3555\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.3039 - root_mean_squared_error: 3.4907 - val_loss: 42.1604 - val_root_mean_squared_error: 2.7990\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 41.9809 - root_mean_squared_error: 3.3738 - val_loss: 40.7343 - val_root_mean_squared_error: 3.3074\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.1596 - root_mean_squared_error: 3.5105 - val_loss: 42.2953 - val_root_mean_squared_error: 3.1958\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 41.7621 - root_mean_squared_error: 3.4298 - val_loss: 42.5577 - val_root_mean_squared_error: 3.3064\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 42.4243 - root_mean_squared_error: 3.3835 - val_loss: 43.4953 - val_root_mean_squared_error: 3.3046\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.4509 - root_mean_squared_error: 3.4342 - val_loss: 43.7358 - val_root_mean_squared_error: 2.9694\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.2755 - root_mean_squared_error: 3.3880 - val_loss: 43.0400 - val_root_mean_squared_error: 3.0832\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.2275 - root_mean_squared_error: 3.4685 - val_loss: 41.5361 - val_root_mean_squared_error: 3.3644\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.1972 - root_mean_squared_error: 3.6184 - val_loss: 43.0245 - val_root_mean_squared_error: 3.0305\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.3403 - root_mean_squared_error: 3.3871 - val_loss: 43.0465 - val_root_mean_squared_error: 3.0235\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.1787 - root_mean_squared_error: 3.3841 - val_loss: 43.0762 - val_root_mean_squared_error: 3.6019\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 43.0371 - root_mean_squared_error: 3.5027 - val_loss: 42.9085 - val_root_mean_squared_error: 3.1187\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.9638 - root_mean_squared_error: 3.3671 - val_loss: 43.6951 - val_root_mean_squared_error: 3.2368\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.2904 - root_mean_squared_error: 3.3673 - val_loss: 43.1349 - val_root_mean_squared_error: 3.3570\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.7192 - root_mean_squared_error: 3.4029 - val_loss: 43.3081 - val_root_mean_squared_error: 3.4720\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.7441 - root_mean_squared_error: 3.4248 - val_loss: 42.2241 - val_root_mean_squared_error: 3.2556\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.7768 - root_mean_squared_error: 3.3621 - val_loss: 42.9771 - val_root_mean_squared_error: 2.9028\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.7494 - root_mean_squared_error: 3.4157 - val_loss: 42.6093 - val_root_mean_squared_error: 3.1857\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.9931 - root_mean_squared_error: 3.4406 - val_loss: 42.5599 - val_root_mean_squared_error: 3.4142\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.2923 - root_mean_squared_error: 3.2678 - val_loss: 42.5051 - val_root_mean_squared_error: 3.1280\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 42.7813 - root_mean_squared_error: 3.4458 - val_loss: 43.0867 - val_root_mean_squared_error: 3.5671\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.4536 - root_mean_squared_error: 3.3180 - val_loss: 43.3586 - val_root_mean_squared_error: 3.5574\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.3709 - root_mean_squared_error: 3.3253 - val_loss: 42.7217 - val_root_mean_squared_error: 3.3218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 43.9729 - root_mean_squared_error: 3.2647 - val_loss: 43.7398 - val_root_mean_squared_error: 3.1323\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.6199 - root_mean_squared_error: 3.2663 - val_loss: 43.5132 - val_root_mean_squared_error: 3.6666\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.3571 - root_mean_squared_error: 3.2029 - val_loss: 44.3750 - val_root_mean_squared_error: 3.4045\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.5579 - root_mean_squared_error: 3.4157 - val_loss: 43.7115 - val_root_mean_squared_error: 3.3896\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 43.3100 - root_mean_squared_error: 3.3459 - val_loss: 42.9258 - val_root_mean_squared_error: 3.1080\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.0535 - root_mean_squared_error: 3.3365 - val_loss: 43.9724 - val_root_mean_squared_error: 3.2196\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 43.5675 - root_mean_squared_error: 3.4212 - val_loss: 44.0570 - val_root_mean_squared_error: 3.1174\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.7429 - root_mean_squared_error: 3.5677 - val_loss: 44.1372 - val_root_mean_squared_error: 3.3833\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 43.9327 - root_mean_squared_error: 3.4325 - val_loss: 44.2475 - val_root_mean_squared_error: 3.7134\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.7720 - root_mean_squared_error: 3.3710 - val_loss: 43.7646 - val_root_mean_squared_error: 3.4208\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.5415 - root_mean_squared_error: 3.5251 - val_loss: 44.7926 - val_root_mean_squared_error: 3.4588\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.6190 - root_mean_squared_error: 3.5790 - val_loss: 44.1965 - val_root_mean_squared_error: 2.9810\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.9526 - root_mean_squared_error: 3.5039 - val_loss: 43.6280 - val_root_mean_squared_error: 3.0104\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 43.5983 - root_mean_squared_error: 3.4132 - val_loss: 44.1747 - val_root_mean_squared_error: 3.1676\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.6981 - root_mean_squared_error: 3.3500 - val_loss: 43.8993 - val_root_mean_squared_error: 3.2864\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.8229 - root_mean_squared_error: 3.4705 - val_loss: 42.5668 - val_root_mean_squared_error: 3.0763\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.0816 - root_mean_squared_error: 3.4402 - val_loss: 43.6135 - val_root_mean_squared_error: 3.4593\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.5012 - root_mean_squared_error: 3.5194 - val_loss: 43.7290 - val_root_mean_squared_error: 3.1192\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.9722 - root_mean_squared_error: 3.5315 - val_loss: 42.8625 - val_root_mean_squared_error: 3.5312\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.5055 - root_mean_squared_error: 3.4884 - val_loss: 43.4537 - val_root_mean_squared_error: 3.0585\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.1370 - root_mean_squared_error: 3.4431 - val_loss: 43.6437 - val_root_mean_squared_error: 2.8730\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.6472 - root_mean_squared_error: 3.4053 - val_loss: 44.8172 - val_root_mean_squared_error: 3.2758\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 43.9342 - root_mean_squared_error: 3.4425 - val_loss: 44.5001 - val_root_mean_squared_error: 3.7394\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.4009 - root_mean_squared_error: 3.5292 - val_loss: 44.5517 - val_root_mean_squared_error: 3.0245\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.3358 - root_mean_squared_error: 3.4256 - val_loss: 43.8336 - val_root_mean_squared_error: 3.3008\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.1799 - root_mean_squared_error: 3.2687 - val_loss: 44.6152 - val_root_mean_squared_error: 3.3074\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 44.6286 - root_mean_squared_error: 3.4325 - val_loss: 45.2417 - val_root_mean_squared_error: 3.4794\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.7025 - root_mean_squared_error: 3.3896 - val_loss: 44.7107 - val_root_mean_squared_error: 3.1756\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.1864 - root_mean_squared_error: 3.4184 - val_loss: 43.9067 - val_root_mean_squared_error: 3.2152\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.4528 - root_mean_squared_error: 3.4141 - val_loss: 45.1829 - val_root_mean_squared_error: 3.2056\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.3862 - root_mean_squared_error: 3.6028 - val_loss: 44.0779 - val_root_mean_squared_error: 3.6309\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.7570 - root_mean_squared_error: 3.4040 - val_loss: 44.3108 - val_root_mean_squared_error: 3.1917\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 44.8673 - root_mean_squared_error: 3.4539 - val_loss: 44.4510 - val_root_mean_squared_error: 3.5868\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.1032 - root_mean_squared_error: 3.4436 - val_loss: 44.5789 - val_root_mean_squared_error: 3.3494\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.6025 - root_mean_squared_error: 3.5032 - val_loss: 43.6779 - val_root_mean_squared_error: 3.1346\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.9493 - root_mean_squared_error: 3.5728 - val_loss: 44.0900 - val_root_mean_squared_error: 3.0607\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.1560 - root_mean_squared_error: 3.3426 - val_loss: 45.0306 - val_root_mean_squared_error: 3.3906\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.7645 - root_mean_squared_error: 3.6185 - val_loss: 45.0453 - val_root_mean_squared_error: 3.3722\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.0872 - root_mean_squared_error: 3.3680 - val_loss: 43.6402 - val_root_mean_squared_error: 3.0295\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.3589 - root_mean_squared_error: 3.3832 - val_loss: 43.7032 - val_root_mean_squared_error: 2.9713\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.0841 - root_mean_squared_error: 3.4810 - val_loss: 45.8956 - val_root_mean_squared_error: 3.8311\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.5689 - root_mean_squared_error: 3.4729 - val_loss: 44.9266 - val_root_mean_squared_error: 3.4982\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.2436 - root_mean_squared_error: 3.3494 - val_loss: 44.6615 - val_root_mean_squared_error: 3.7088\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.0227 - root_mean_squared_error: 3.5336 - val_loss: 44.2114 - val_root_mean_squared_error: 3.2045\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.5517 - root_mean_squared_error: 3.3394 - val_loss: 45.9851 - val_root_mean_squared_error: 3.5715\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 44.9733 - root_mean_squared_error: 3.3851 - val_loss: 45.2203 - val_root_mean_squared_error: 3.3015\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 45.0429 - root_mean_squared_error: 3.4941 - val_loss: 46.0734 - val_root_mean_squared_error: 3.1877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.7553 - root_mean_squared_error: 3.5230 - val_loss: 44.8994 - val_root_mean_squared_error: 3.4269\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 45.2975 - root_mean_squared_error: 3.4338 - val_loss: 44.8848 - val_root_mean_squared_error: 3.7552\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.6735 - root_mean_squared_error: 3.3489 - val_loss: 45.9967 - val_root_mean_squared_error: 3.4247\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 45.3294 - root_mean_squared_error: 3.4851 - val_loss: 45.6010 - val_root_mean_squared_error: 3.3133\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 45.5340 - root_mean_squared_error: 3.4061 - val_loss: 44.9736 - val_root_mean_squared_error: 3.0835\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.0072 - root_mean_squared_error: 3.4399 - val_loss: 45.6853 - val_root_mean_squared_error: 2.9275\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.5368 - root_mean_squared_error: 3.2497 - val_loss: 45.2979 - val_root_mean_squared_error: 3.1551\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.4387 - root_mean_squared_error: 3.4919 - val_loss: 45.6905 - val_root_mean_squared_error: 3.5881\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.7452 - root_mean_squared_error: 3.4315 - val_loss: 44.8559 - val_root_mean_squared_error: 3.0169\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.9689 - root_mean_squared_error: 3.5185 - val_loss: 45.5264 - val_root_mean_squared_error: 3.2579\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.6878 - root_mean_squared_error: 3.3848 - val_loss: 46.3565 - val_root_mean_squared_error: 3.4418\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.1256 - root_mean_squared_error: 3.4201 - val_loss: 46.3568 - val_root_mean_squared_error: 3.2387\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.8723 - root_mean_squared_error: 3.4031 - val_loss: 47.4293 - val_root_mean_squared_error: 3.5289\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.0236 - root_mean_squared_error: 3.5193 - val_loss: 45.3784 - val_root_mean_squared_error: 3.3419\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.3885 - root_mean_squared_error: 3.2623 - val_loss: 45.7433 - val_root_mean_squared_error: 3.3042\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.5793 - root_mean_squared_error: 3.5249 - val_loss: 46.2257 - val_root_mean_squared_error: 3.2970\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.4468 - root_mean_squared_error: 3.6109 - val_loss: 45.0495 - val_root_mean_squared_error: 3.3985\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.9200 - root_mean_squared_error: 3.2733 - val_loss: 45.5332 - val_root_mean_squared_error: 3.7116\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.2895 - root_mean_squared_error: 3.6032 - val_loss: 45.2432 - val_root_mean_squared_error: 3.1121\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 45.8678 - root_mean_squared_error: 3.5513 - val_loss: 47.0785 - val_root_mean_squared_error: 3.1852\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.6552 - root_mean_squared_error: 3.6197 - val_loss: 45.1739 - val_root_mean_squared_error: 3.2320\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.1568 - root_mean_squared_error: 3.4999 - val_loss: 45.9693 - val_root_mean_squared_error: 3.7401\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.7315 - root_mean_squared_error: 3.5027 - val_loss: 47.0334 - val_root_mean_squared_error: 3.2420\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.0500 - root_mean_squared_error: 3.4040 - val_loss: 45.1610 - val_root_mean_squared_error: 3.0242\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 45.8679 - root_mean_squared_error: 3.3709 - val_loss: 46.3480 - val_root_mean_squared_error: 3.2314\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.2521 - root_mean_squared_error: 3.5604 - val_loss: 46.4382 - val_root_mean_squared_error: 3.4379\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.0426 - root_mean_squared_error: 3.4134 - val_loss: 46.3473 - val_root_mean_squared_error: 3.4715\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.2486 - root_mean_squared_error: 3.3209 - val_loss: 46.8849 - val_root_mean_squared_error: 3.5160\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.1863 - root_mean_squared_error: 3.4398 - val_loss: 46.4927 - val_root_mean_squared_error: 3.0496\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.3299 - root_mean_squared_error: 3.4168 - val_loss: 46.5968 - val_root_mean_squared_error: 3.5437\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.5598 - root_mean_squared_error: 3.5870 - val_loss: 46.2029 - val_root_mean_squared_error: 3.2597\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.8553 - root_mean_squared_error: 3.5040 - val_loss: 46.2208 - val_root_mean_squared_error: 3.3996\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.2997 - root_mean_squared_error: 3.5143 - val_loss: 46.8401 - val_root_mean_squared_error: 3.4859\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.6528 - root_mean_squared_error: 3.3897 - val_loss: 45.7699 - val_root_mean_squared_error: 3.3170\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.3990 - root_mean_squared_error: 3.5482 - val_loss: 46.3411 - val_root_mean_squared_error: 2.6449\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6616 - root_mean_squared_error: 3.3647 - val_loss: 45.9168 - val_root_mean_squared_error: 3.1452\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6235 - root_mean_squared_error: 3.4523 - val_loss: 46.7279 - val_root_mean_squared_error: 3.5199\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.2365 - root_mean_squared_error: 3.4756 - val_loss: 46.5532 - val_root_mean_squared_error: 3.4961\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.5285 - root_mean_squared_error: 3.5652 - val_loss: 46.0158 - val_root_mean_squared_error: 3.2386\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6558 - root_mean_squared_error: 3.3751 - val_loss: 47.3426 - val_root_mean_squared_error: 3.9255\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.7839 - root_mean_squared_error: 3.5738 - val_loss: 47.7196 - val_root_mean_squared_error: 2.9600\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.3998 - root_mean_squared_error: 3.4032 - val_loss: 45.5163 - val_root_mean_squared_error: 3.2647\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.3211 - root_mean_squared_error: 3.5102 - val_loss: 46.0572 - val_root_mean_squared_error: 3.2019\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.1117 - root_mean_squared_error: 3.4254 - val_loss: 46.5981 - val_root_mean_squared_error: 3.1847\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.1333 - root_mean_squared_error: 3.3977 - val_loss: 47.0673 - val_root_mean_squared_error: 3.4391\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6227 - root_mean_squared_error: 3.5470 - val_loss: 45.7693 - val_root_mean_squared_error: 2.8191\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.4109 - root_mean_squared_error: 3.4679 - val_loss: 47.1651 - val_root_mean_squared_error: 3.5296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6708 - root_mean_squared_error: 3.3984 - val_loss: 47.7924 - val_root_mean_squared_error: 3.4809\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.9361 - root_mean_squared_error: 3.5381 - val_loss: 46.2626 - val_root_mean_squared_error: 3.3444\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.7857 - root_mean_squared_error: 3.4259 - val_loss: 47.7098 - val_root_mean_squared_error: 3.6810\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6710 - root_mean_squared_error: 3.4454 - val_loss: 48.8624 - val_root_mean_squared_error: 3.3005\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6819 - root_mean_squared_error: 3.3165 - val_loss: 46.3969 - val_root_mean_squared_error: 3.7152\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.8266 - root_mean_squared_error: 3.4716 - val_loss: 46.9606 - val_root_mean_squared_error: 3.7046\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.0828 - root_mean_squared_error: 3.3746 - val_loss: 47.6637 - val_root_mean_squared_error: 3.2855\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.2128 - root_mean_squared_error: 3.3023 - val_loss: 46.0715 - val_root_mean_squared_error: 3.4638\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.8694 - root_mean_squared_error: 3.6251 - val_loss: 49.2948 - val_root_mean_squared_error: 3.2359\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.7787 - root_mean_squared_error: 3.5564 - val_loss: 46.3118 - val_root_mean_squared_error: 3.5376\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.7403 - root_mean_squared_error: 3.4960 - val_loss: 47.2543 - val_root_mean_squared_error: 3.3451\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 46.6764 - root_mean_squared_error: 3.4869 - val_loss: 47.2594 - val_root_mean_squared_error: 3.1980\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.1384 - root_mean_squared_error: 3.4107 - val_loss: 45.4804 - val_root_mean_squared_error: 3.0572\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.2785 - root_mean_squared_error: 3.4175 - val_loss: 46.3328 - val_root_mean_squared_error: 3.3229\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.9422 - root_mean_squared_error: 3.5320 - val_loss: 46.8056 - val_root_mean_squared_error: 3.6972\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.4071 - root_mean_squared_error: 3.4610 - val_loss: 47.4091 - val_root_mean_squared_error: 3.1657\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.2140 - root_mean_squared_error: 3.4571 - val_loss: 47.4534 - val_root_mean_squared_error: 3.1970\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.1517 - root_mean_squared_error: 3.4791 - val_loss: 45.6756 - val_root_mean_squared_error: 3.2507\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.8643 - root_mean_squared_error: 3.3167 - val_loss: 47.1971 - val_root_mean_squared_error: 3.4381\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.5244 - root_mean_squared_error: 3.4569 - val_loss: 47.4630 - val_root_mean_squared_error: 3.3520\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.5640 - root_mean_squared_error: 3.3972 - val_loss: 48.8709 - val_root_mean_squared_error: 3.6462\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.1557 - root_mean_squared_error: 3.4111 - val_loss: 47.6772 - val_root_mean_squared_error: 3.4254\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.4698 - root_mean_squared_error: 3.5670 - val_loss: 47.3423 - val_root_mean_squared_error: 2.9933\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.4817 - root_mean_squared_error: 3.3457 - val_loss: 47.7156 - val_root_mean_squared_error: 3.3892\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.3486 - root_mean_squared_error: 3.4001 - val_loss: 46.9982 - val_root_mean_squared_error: 3.0380\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.6810 - root_mean_squared_error: 3.5588 - val_loss: 46.9398 - val_root_mean_squared_error: 3.4244\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.3635 - root_mean_squared_error: 3.3846 - val_loss: 46.4227 - val_root_mean_squared_error: 3.6366\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 46.9094 - root_mean_squared_error: 3.3220 - val_loss: 46.9167 - val_root_mean_squared_error: 3.3663\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.3446 - root_mean_squared_error: 3.3386 - val_loss: 46.9842 - val_root_mean_squared_error: 3.2272\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.5986 - root_mean_squared_error: 3.5208 - val_loss: 47.8076 - val_root_mean_squared_error: 3.6595\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.4825 - root_mean_squared_error: 3.3369 - val_loss: 47.6661 - val_root_mean_squared_error: 3.3196\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.4838 - root_mean_squared_error: 3.4600 - val_loss: 48.0982 - val_root_mean_squared_error: 3.0341\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.9751 - root_mean_squared_error: 3.3633 - val_loss: 48.0007 - val_root_mean_squared_error: 3.0051\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.4846 - root_mean_squared_error: 3.6137 - val_loss: 46.5756 - val_root_mean_squared_error: 3.5037\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.0116 - root_mean_squared_error: 3.4142 - val_loss: 48.6196 - val_root_mean_squared_error: 3.0875\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.6228 - root_mean_squared_error: 3.2519 - val_loss: 47.5388 - val_root_mean_squared_error: 3.4479\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.6873 - root_mean_squared_error: 3.3444 - val_loss: 47.5333 - val_root_mean_squared_error: 3.5325\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.8257 - root_mean_squared_error: 3.3234 - val_loss: 48.1262 - val_root_mean_squared_error: 3.1829\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.7528 - root_mean_squared_error: 3.3527 - val_loss: 47.6497 - val_root_mean_squared_error: 3.3150\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.3991 - root_mean_squared_error: 3.3609 - val_loss: 48.0840 - val_root_mean_squared_error: 3.5962\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.9584 - root_mean_squared_error: 3.5401 - val_loss: 48.3498 - val_root_mean_squared_error: 3.2641\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.1200 - root_mean_squared_error: 3.4463 - val_loss: 47.4441 - val_root_mean_squared_error: 3.2217\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.5480 - root_mean_squared_error: 3.4358 - val_loss: 48.0137 - val_root_mean_squared_error: 3.4994\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.8078 - root_mean_squared_error: 3.2443 - val_loss: 47.9241 - val_root_mean_squared_error: 3.3292\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.4588 - root_mean_squared_error: 3.3979 - val_loss: 47.5193 - val_root_mean_squared_error: 3.1247\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.0446 - root_mean_squared_error: 3.3399 - val_loss: 47.2304 - val_root_mean_squared_error: 3.3990\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.7106 - root_mean_squared_error: 3.4670 - val_loss: 47.4110 - val_root_mean_squared_error: 3.1957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.2110 - root_mean_squared_error: 3.3747 - val_loss: 46.8311 - val_root_mean_squared_error: 3.4195\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.2007 - root_mean_squared_error: 3.3402 - val_loss: 48.7810 - val_root_mean_squared_error: 3.1021\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.5550 - root_mean_squared_error: 3.4332 - val_loss: 48.4702 - val_root_mean_squared_error: 2.9946\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.3913 - root_mean_squared_error: 3.4257 - val_loss: 47.4145 - val_root_mean_squared_error: 3.1261\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.3067 - root_mean_squared_error: 3.4071 - val_loss: 47.6982 - val_root_mean_squared_error: 3.1110\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.2979 - root_mean_squared_error: 3.5552 - val_loss: 48.8720 - val_root_mean_squared_error: 3.1753\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.8486 - root_mean_squared_error: 3.4269 - val_loss: 47.0077 - val_root_mean_squared_error: 3.2971\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.3309 - root_mean_squared_error: 3.4764 - val_loss: 47.8326 - val_root_mean_squared_error: 3.3925\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.6003 - root_mean_squared_error: 3.3716 - val_loss: 48.4134 - val_root_mean_squared_error: 3.0255\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.3482 - root_mean_squared_error: 3.2705 - val_loss: 46.7429 - val_root_mean_squared_error: 3.3428\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.0021 - root_mean_squared_error: 3.4171 - val_loss: 47.3666 - val_root_mean_squared_error: 3.6276\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.6942 - root_mean_squared_error: 3.3454 - val_loss: 47.6123 - val_root_mean_squared_error: 3.5762\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.7584 - root_mean_squared_error: 3.3556 - val_loss: 47.4455 - val_root_mean_squared_error: 3.1452\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.8305 - root_mean_squared_error: 3.4325 - val_loss: 48.0733 - val_root_mean_squared_error: 3.4261\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.0498 - root_mean_squared_error: 3.5352 - val_loss: 48.2627 - val_root_mean_squared_error: 3.2774\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.1648 - root_mean_squared_error: 3.4832 - val_loss: 48.7873 - val_root_mean_squared_error: 3.2546\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.9909 - root_mean_squared_error: 3.5271 - val_loss: 47.8499 - val_root_mean_squared_error: 3.6774\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.2242 - root_mean_squared_error: 3.3238 - val_loss: 47.0667 - val_root_mean_squared_error: 3.4466\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.9290 - root_mean_squared_error: 3.4606 - val_loss: 47.7492 - val_root_mean_squared_error: 3.5729\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.2829 - root_mean_squared_error: 3.3025 - val_loss: 47.6448 - val_root_mean_squared_error: 3.5432\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.3991 - root_mean_squared_error: 3.4564 - val_loss: 48.4910 - val_root_mean_squared_error: 3.4203\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.8489 - root_mean_squared_error: 3.4679 - val_loss: 48.9613 - val_root_mean_squared_error: 3.3057\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.7651 - root_mean_squared_error: 3.4323 - val_loss: 48.2270 - val_root_mean_squared_error: 2.9533\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.0483 - root_mean_squared_error: 3.3497 - val_loss: 47.4507 - val_root_mean_squared_error: 3.0423\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.4654 - root_mean_squared_error: 3.3032 - val_loss: 47.3104 - val_root_mean_squared_error: 3.3823\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.3974 - root_mean_squared_error: 3.3499 - val_loss: 47.6901 - val_root_mean_squared_error: 3.5287\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 47.8752 - root_mean_squared_error: 3.5409 - val_loss: 47.3983 - val_root_mean_squared_error: 3.2273\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.1180 - root_mean_squared_error: 3.3734 - val_loss: 47.7690 - val_root_mean_squared_error: 3.4980\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.4716 - root_mean_squared_error: 3.4532 - val_loss: 47.6663 - val_root_mean_squared_error: 3.1323\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.0663 - root_mean_squared_error: 3.4640 - val_loss: 47.7699 - val_root_mean_squared_error: 3.2649\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.0521 - root_mean_squared_error: 3.3329 - val_loss: 48.2346 - val_root_mean_squared_error: 3.3616\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.6099 - root_mean_squared_error: 3.4829 - val_loss: 47.8593 - val_root_mean_squared_error: 3.3971\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.7530 - root_mean_squared_error: 3.3984 - val_loss: 48.3122 - val_root_mean_squared_error: 3.5858\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.6040 - root_mean_squared_error: 3.3551 - val_loss: 48.4705 - val_root_mean_squared_error: 3.1482\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.3020 - root_mean_squared_error: 3.2859 - val_loss: 48.3858 - val_root_mean_squared_error: 3.1914\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.2408 - root_mean_squared_error: 3.2910 - val_loss: 47.6328 - val_root_mean_squared_error: 3.2740\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 47.8973 - root_mean_squared_error: 3.3258 - val_loss: 47.7047 - val_root_mean_squared_error: 3.3612\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.6995 - root_mean_squared_error: 3.4463 - val_loss: 49.3198 - val_root_mean_squared_error: 3.4541\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.2740 - root_mean_squared_error: 3.4675 - val_loss: 48.7093 - val_root_mean_squared_error: 3.2606\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.8934 - root_mean_squared_error: 3.4411 - val_loss: 47.6529 - val_root_mean_squared_error: 3.5291\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.1801 - root_mean_squared_error: 3.4221 - val_loss: 48.3499 - val_root_mean_squared_error: 3.1131\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.5382 - root_mean_squared_error: 3.3796 - val_loss: 48.0757 - val_root_mean_squared_error: 3.3256\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.1061 - root_mean_squared_error: 3.3679 - val_loss: 48.0247 - val_root_mean_squared_error: 3.1626\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.5274 - root_mean_squared_error: 3.4490 - val_loss: 47.2314 - val_root_mean_squared_error: 3.3498\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.7434 - root_mean_squared_error: 3.3330 - val_loss: 49.2327 - val_root_mean_squared_error: 3.5498\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.7360 - root_mean_squared_error: 3.4982 - val_loss: 47.9564 - val_root_mean_squared_error: 3.3519\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.5776 - root_mean_squared_error: 3.3661 - val_loss: 47.9387 - val_root_mean_squared_error: 3.0670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 424/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.6895 - root_mean_squared_error: 3.4154 - val_loss: 48.8041 - val_root_mean_squared_error: 3.2100\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.5580 - root_mean_squared_error: 3.3893 - val_loss: 47.5571 - val_root_mean_squared_error: 2.9784\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8877 - root_mean_squared_error: 3.3861 - val_loss: 49.1304 - val_root_mean_squared_error: 3.2766\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8674 - root_mean_squared_error: 3.5372 - val_loss: 48.3280 - val_root_mean_squared_error: 3.2112\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.5654 - root_mean_squared_error: 3.4528 - val_loss: 48.0521 - val_root_mean_squared_error: 3.1123\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.4537 - root_mean_squared_error: 3.3551 - val_loss: 48.7489 - val_root_mean_squared_error: 3.3159\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.1088 - root_mean_squared_error: 3.3814 - val_loss: 49.2082 - val_root_mean_squared_error: 3.6341\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0908 - root_mean_squared_error: 3.4441 - val_loss: 48.3987 - val_root_mean_squared_error: 3.4975\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.2062 - root_mean_squared_error: 3.4123 - val_loss: 49.0799 - val_root_mean_squared_error: 3.3562\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.7739 - root_mean_squared_error: 3.4117 - val_loss: 48.5653 - val_root_mean_squared_error: 3.5034\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.9864 - root_mean_squared_error: 3.4034 - val_loss: 48.9528 - val_root_mean_squared_error: 3.5077\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.9042 - root_mean_squared_error: 3.3603 - val_loss: 48.5641 - val_root_mean_squared_error: 3.3346\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0184 - root_mean_squared_error: 3.1715 - val_loss: 48.8594 - val_root_mean_squared_error: 3.1814\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.1526 - root_mean_squared_error: 3.3957 - val_loss: 49.1347 - val_root_mean_squared_error: 3.3067\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.4354 - root_mean_squared_error: 3.6294 - val_loss: 49.9036 - val_root_mean_squared_error: 2.8611\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.3294 - root_mean_squared_error: 3.4594 - val_loss: 49.1212 - val_root_mean_squared_error: 3.2307\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.4514 - root_mean_squared_error: 3.4292 - val_loss: 48.3532 - val_root_mean_squared_error: 3.4317\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.1114 - root_mean_squared_error: 3.4899 - val_loss: 49.3589 - val_root_mean_squared_error: 3.1971\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8606 - root_mean_squared_error: 3.4735 - val_loss: 48.3455 - val_root_mean_squared_error: 3.0966\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.3393 - root_mean_squared_error: 3.4420 - val_loss: 48.7269 - val_root_mean_squared_error: 3.1049\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2423 - root_mean_squared_error: 3.3935 - val_loss: 48.2617 - val_root_mean_squared_error: 3.3448\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.9540 - root_mean_squared_error: 3.4197 - val_loss: 48.9643 - val_root_mean_squared_error: 3.2961\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0900 - root_mean_squared_error: 3.5821 - val_loss: 49.4988 - val_root_mean_squared_error: 3.3890\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.7626 - root_mean_squared_error: 3.3678 - val_loss: 48.0620 - val_root_mean_squared_error: 3.3657\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8810 - root_mean_squared_error: 3.3229 - val_loss: 48.8824 - val_root_mean_squared_error: 3.8349\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2431 - root_mean_squared_error: 3.3988 - val_loss: 49.6185 - val_root_mean_squared_error: 3.1242\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8837 - root_mean_squared_error: 3.5163 - val_loss: 48.8889 - val_root_mean_squared_error: 3.3386\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.4867 - root_mean_squared_error: 3.3729 - val_loss: 49.5852 - val_root_mean_squared_error: 3.1186\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2475 - root_mean_squared_error: 3.3769 - val_loss: 48.8910 - val_root_mean_squared_error: 3.3589\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 55s 3s/step - loss: 49.1855 - root_mean_squared_error: 3.5347 - val_loss: 49.7608 - val_root_mean_squared_error: 3.3014\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8201 - root_mean_squared_error: 3.4583 - val_loss: 50.0968 - val_root_mean_squared_error: 3.5827\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.5453 - root_mean_squared_error: 3.4604 - val_loss: 49.6888 - val_root_mean_squared_error: 3.1613\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.7759 - root_mean_squared_error: 3.5351 - val_loss: 48.9718 - val_root_mean_squared_error: 3.2865\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0930 - root_mean_squared_error: 3.3443 - val_loss: 47.2679 - val_root_mean_squared_error: 3.4648\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.6513 - root_mean_squared_error: 3.3424 - val_loss: 50.0687 - val_root_mean_squared_error: 3.3406\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.5562 - root_mean_squared_error: 3.4145 - val_loss: 49.1160 - val_root_mean_squared_error: 3.3562\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8641 - root_mean_squared_error: 3.5496 - val_loss: 49.4627 - val_root_mean_squared_error: 3.3500\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.6516 - root_mean_squared_error: 3.4109 - val_loss: 50.1646 - val_root_mean_squared_error: 3.4393\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2722 - root_mean_squared_error: 3.5069 - val_loss: 47.6490 - val_root_mean_squared_error: 2.9006\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.3582 - root_mean_squared_error: 3.3792 - val_loss: 49.1240 - val_root_mean_squared_error: 3.4204\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0598 - root_mean_squared_error: 3.3108 - val_loss: 48.7126 - val_root_mean_squared_error: 3.1220\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2400 - root_mean_squared_error: 3.4176 - val_loss: 48.6884 - val_root_mean_squared_error: 3.2316\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.1513 - root_mean_squared_error: 3.4205 - val_loss: 48.3527 - val_root_mean_squared_error: 3.1068\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0836 - root_mean_squared_error: 3.4544 - val_loss: 48.8923 - val_root_mean_squared_error: 3.1842\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2802 - root_mean_squared_error: 3.3369 - val_loss: 48.9046 - val_root_mean_squared_error: 3.4186\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 48.9548 - root_mean_squared_error: 3.3909 - val_loss: 49.5993 - val_root_mean_squared_error: 3.1277\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.6753 - root_mean_squared_error: 3.4779 - val_loss: 49.6937 - val_root_mean_squared_error: 3.1564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.3455 - root_mean_squared_error: 3.4622 - val_loss: 49.1043 - val_root_mean_squared_error: 3.4245\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8008 - root_mean_squared_error: 3.3550 - val_loss: 49.9223 - val_root_mean_squared_error: 3.1563\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.2747 - root_mean_squared_error: 3.4268 - val_loss: 48.6011 - val_root_mean_squared_error: 3.3139\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.7790 - root_mean_squared_error: 3.2981 - val_loss: 49.5649 - val_root_mean_squared_error: 3.7450\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2498 - root_mean_squared_error: 3.3772 - val_loss: 49.8294 - val_root_mean_squared_error: 3.4271\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 48.8931 - root_mean_squared_error: 3.4437 - val_loss: 50.1685 - val_root_mean_squared_error: 3.4369\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0290 - root_mean_squared_error: 3.3981 - val_loss: 49.8907 - val_root_mean_squared_error: 3.4032\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.1084 - root_mean_squared_error: 3.4308 - val_loss: 48.9924 - val_root_mean_squared_error: 3.3353\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.5565 - root_mean_squared_error: 3.4583 - val_loss: 48.6859 - val_root_mean_squared_error: 3.3445\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.7284 - root_mean_squared_error: 3.4344 - val_loss: 50.1637 - val_root_mean_squared_error: 2.8938\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.0856 - root_mean_squared_error: 3.3860 - val_loss: 50.2303 - val_root_mean_squared_error: 3.3160\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.1842 - root_mean_squared_error: 3.3708 - val_loss: 49.3245 - val_root_mean_squared_error: 3.5606\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.6569 - root_mean_squared_error: 3.2766 - val_loss: 49.1915 - val_root_mean_squared_error: 3.1079\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.4936 - root_mean_squared_error: 3.4184 - val_loss: 48.8425 - val_root_mean_squared_error: 3.1684\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.7176 - root_mean_squared_error: 3.4319 - val_loss: 49.6539 - val_root_mean_squared_error: 3.2396\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.4102 - root_mean_squared_error: 3.3872 - val_loss: 49.3656 - val_root_mean_squared_error: 3.0659\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.2781 - root_mean_squared_error: 3.3928 - val_loss: 49.0154 - val_root_mean_squared_error: 3.2877\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.3449 - root_mean_squared_error: 3.4440 - val_loss: 49.3504 - val_root_mean_squared_error: 3.3105\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.3334 - root_mean_squared_error: 3.2914 - val_loss: 48.3910 - val_root_mean_squared_error: 3.4545\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.8426 - root_mean_squared_error: 3.4813 - val_loss: 49.5087 - val_root_mean_squared_error: 3.4922\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.6920 - root_mean_squared_error: 3.4997 - val_loss: 49.8894 - val_root_mean_squared_error: 3.3793\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.2049 - root_mean_squared_error: 3.3825 - val_loss: 48.2716 - val_root_mean_squared_error: 3.1264\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.6750 - root_mean_squared_error: 3.5366 - val_loss: 49.5423 - val_root_mean_squared_error: 3.1628\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.3509 - root_mean_squared_error: 3.4970 - val_loss: 50.6495 - val_root_mean_squared_error: 3.4890\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.2485 - root_mean_squared_error: 3.3661 - val_loss: 50.3658 - val_root_mean_squared_error: 3.0383\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.3441 - root_mean_squared_error: 3.3188 - val_loss: 50.2888 - val_root_mean_squared_error: 3.5243\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 54s 2s/step - loss: 49.5872 - root_mean_squared_error: 3.4129 - val_loss: 48.6647 - val_root_mean_squared_error: 3.2733\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.7856 - root_mean_squared_error: 3.3485 - val_loss: 50.3238 - val_root_mean_squared_error: 3.3108\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.4117 - root_mean_squared_error: 3.2079 - val_loss: 47.2374 - val_root_mean_squared_error: 3.5484\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 55s 2s/step - loss: 49.3213 - root_mean_squared_error: 3.3445 - val_loss: 48.8601 - val_root_mean_squared_error: 3.5280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c2c10cc90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_tra, y_tra, epochs=EPOCHS, batch_size = BATCH_SIZE , validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b72aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=     spectrum_0  spectrum_1  spectrum_2  spectrum_3  spectrum_4  spectrum_5  \\\n",
      "0     -0.292534   -0.300286   -0.291942   -0.306939   -0.297430   -0.304344   \n",
      "1     -0.287618   -0.295369   -0.286865   -0.302013   -0.292422   -0.299344   \n",
      "2     -0.282727   -0.290483   -0.281821   -0.297121   -0.287449   -0.294380   \n",
      "3     -0.406851   -0.414486   -0.410242   -0.421234   -0.413865   -0.420654   \n",
      "4     -0.412751   -0.420816   -0.417209   -0.427920   -0.420642   -0.427673   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "445   -0.119053   -0.127372   -0.113785   -0.134595   -0.121794   -0.127344   \n",
      "446   -0.121945   -0.130221   -0.116683   -0.137422   -0.124655   -0.130178   \n",
      "447    0.164673    0.148381    0.159383    0.136714    0.150174    0.139347   \n",
      "448    0.197028    0.180281    0.191324    0.168492    0.181957    0.170841   \n",
      "449    0.175174    0.158710    0.169747    0.146973    0.160502    0.149579   \n",
      "\n",
      "     spectrum_6  spectrum_7  spectrum_8  spectrum_9  ...  spectrum_54  \\\n",
      "0     -0.301487   -0.306735   -0.300421   -0.303807  ...    -0.300214   \n",
      "1     -0.296504   -0.301664   -0.295461   -0.298774  ...    -0.295328   \n",
      "2     -0.291553   -0.296633   -0.290531   -0.293773  ...    -0.290467   \n",
      "3     -0.417174   -0.424429   -0.415848   -0.420783  ...    -0.413702   \n",
      "4     -0.423753   -0.431424   -0.422461   -0.427609  ...    -0.419624   \n",
      "..          ...         ...         ...         ...  ...          ...   \n",
      "445   -0.127242   -0.129298   -0.125123   -0.126844  ...    -0.126932   \n",
      "446   -0.130098   -0.132142   -0.128002   -0.129705  ...    -0.129781   \n",
      "447    0.149821    0.146633    0.147839    0.148796  ...     0.155359   \n",
      "448    0.181880    0.178691    0.179817    0.180915  ...     0.187559   \n",
      "449    0.160154    0.156880    0.158256    0.159118  ...     0.165761   \n",
      "\n",
      "     spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
      "0      -0.309393    -0.305264    -0.288061    -0.302434    -0.310094   \n",
      "1      -0.304382    -0.300315    -0.283320    -0.297450    -0.305057   \n",
      "2      -0.299403    -0.295394    -0.278596    -0.292498    -0.300053   \n",
      "3      -0.425751    -0.420115    -0.398176    -0.418260    -0.427389   \n",
      "4      -0.432511    -0.426413    -0.403369    -0.424831    -0.434569   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "445    -0.132548    -0.130558    -0.121929    -0.126805    -0.130108   \n",
      "446    -0.135418    -0.133434    -0.124814    -0.129670    -0.132948   \n",
      "447     0.143392     0.149755     0.164370     0.150557     0.136975   \n",
      "448     0.175653     0.182139     0.196616     0.182756     0.168755   \n",
      "449     0.153795     0.160241     0.174931     0.160941     0.147219   \n",
      "\n",
      "     spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
      "0      -0.300604    -0.304811    -0.306966    -0.305819  \n",
      "1      -0.295628    -0.299704    -0.302012    -0.300959  \n",
      "2      -0.290682    -0.294633    -0.297089    -0.296128  \n",
      "3      -0.416313    -0.423566    -0.422016    -0.418531  \n",
      "4      -0.422827    -0.430839    -0.428537    -0.424686  \n",
      "..           ...          ...          ...          ...  \n",
      "445    -0.124811    -0.125417    -0.131597    -0.135251  \n",
      "446    -0.127669    -0.128289    -0.134443    -0.138073  \n",
      "447     0.151306     0.145253     0.144289     0.143123  \n",
      "448     0.183308     0.177205     0.176353     0.175129  \n",
      "449     0.161655     0.155570     0.154635     0.153449  \n",
      "\n",
      "[450 rows x 64 columns]. Consider rewriting this model with the Functional API.\n"
     ]
    }
   ],
   "source": [
    "prediction_distribution = model(e_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3d287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
    "prediction_stdv = prediction_distribution.stddev().numpy()\n",
    "\n",
    "# The 95% CI is computed as mean ± (1.96 * stdv)\n",
    "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
    "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
    "prediction_stdv = prediction_stdv.tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ffb48d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 8s 363ms/step - loss: 49.2356 - root_mean_squared_error: 3.4907\n",
      "15/15 [==============================] - 5s 364ms/step - loss: 49.0103 - root_mean_squared_error: 2.8336\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.38 - 7.02] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.36 - 6.96] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.28, stddev: 2.36, 95% CI: [-2.35 - 6.9] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.35, stddev: 2.42, 95% CI: [-2.39 - 7.08] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.06] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.41, 95% CI: [-2.38 - 7.05] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.2, stddev: 2.29, 95% CI: [-2.29 - 6.68] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 1.87, stddev: 2.01, 95% CI: [-2.08 - 5.81] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.44, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.31, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.12] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.07] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.41, 95% CI: [-2.38 - 7.05] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.28, stddev: 2.36, 95% CI: [-2.35 - 6.91] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.25, stddev: 2.33, 95% CI: [-2.33 - 6.82] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.41 - 7.13] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.14] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.35, stddev: 2.42, 95% CI: [-2.4 - 7.11] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.12] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.11] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.03] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.38 - 7.02] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.38 - 7.02] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.35 - 6.93] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.06] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.41 - 7.13] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.41 - 7.13] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.01] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.11] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.06] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.31, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.25, stddev: 2.34, 95% CI: [-2.33 - 6.83] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.27, stddev: 2.36, 95% CI: [-2.34 - 6.89] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.42, 95% CI: [-2.39 - 7.08] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.38 - 7.02] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.35 - 6.93] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.03] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.03] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.41, 95% CI: [-2.39 - 7.05] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.06] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.35 - 6.93] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.41, 95% CI: [-2.38 - 7.05] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.01] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.41 - 7.13] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.44, 95% CI: [-2.41 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.37 - 7.01] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.41, 95% CI: [-2.38 - 7.05] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.11] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.06] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.42, 95% CI: [-2.39 - 7.08] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.35, stddev: 2.42, 95% CI: [-2.4 - 7.1] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.11] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.31, stddev: 2.38, 95% CI: [-2.37 - 6.98] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.31, stddev: 2.38, 95% CI: [-2.36 - 6.98] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.06] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.36 - 6.97] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 0.7\n",
      "Cardboard1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 0.7\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.11 - 6.01] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.11 - 6.04] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.08] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.91, stddev: 2.03, 95% CI: [-2.08 - 5.9] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.08 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.08 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.11 - 6.04] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.11 - 5.97] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.12 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.08] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.12 - 6.07] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.05, 95% CI: [-2.09 - 5.94] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.91, stddev: 2.03, 95% CI: [-2.07 - 5.89] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.95, stddev: 2.06, 95% CI: [-2.1 - 5.99] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.09 - 5.97] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.07] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.11 - 5.97] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.11 - 5.98] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.09 - 5.97] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.12 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.11 - 5.98] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.0] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.13 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.12 - 6.06] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.08] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.13 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.97, stddev: 2.08, 95% CI: [-2.11 - 6.05] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.96, stddev: 2.07, 95% CI: [-2.11 - 6.02] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.1 - 5.97] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.97] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.07] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.05, 95% CI: [-2.09 - 5.95] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.1 - 5.94] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.05, 95% CI: [-2.1 - 5.95] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.12 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.98] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.13 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.07] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.94] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.1 - 5.98] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.11 - 6.01] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.09 - 5.92] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.93] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.08] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.13 - 6.09] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.12 - 6.06] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.12 - 6.07] - Actual: 4.0\n",
      "Ceramic1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.08] - Actual: 4.0\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.11 - 5.97] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.93, stddev: 2.06, 95% CI: [-2.1 - 5.96] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.1] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.09] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.03] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.09] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.98] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.13 - 6.07] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.09] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.1] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.13 - 6.04] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.1] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.13 - 6.07] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.96, stddev: 2.09, 95% CI: [-2.13 - 6.06] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.96, stddev: 2.09, 95% CI: [-2.13 - 6.05] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.13 - 6.07] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.13 - 6.06] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.94] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.07, 95% CI: [-2.12 - 6.01] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 5.99] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.94, stddev: 2.07, 95% CI: [-2.11 - 6.0] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.12 - 6.04] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.97, stddev: 2.1, 95% CI: [-2.14 - 6.08] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.12 - 6.03] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.96, stddev: 2.09, 95% CI: [-2.13 - 6.06] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.13 - 6.07] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.03] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.02] - Actual: 0.2\n",
      "Glass1: Prediction mean: 1.95, stddev: 2.08, 95% CI: [-2.12 - 6.03] - Actual: 0.2\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.03 - 5.72] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.14, stddev: 2.23, 95% CI: [-2.23 - 6.5] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.11, stddev: 2.22, 95% CI: [-2.23 - 6.46] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.18, stddev: 2.28, 95% CI: [-2.28 - 6.64] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.11] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.35, stddev: 2.42, 95% CI: [-2.39 - 7.08] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.36 - 6.97] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.36, stddev: 2.43, 95% CI: [-2.4 - 7.11] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.19] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.38 - 7.02] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.14, stddev: 2.24, 95% CI: [-2.25 - 6.53] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.15] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.32, stddev: 2.4, 95% CI: [-2.38 - 7.02] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.36 - 6.97] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.01] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.31, stddev: 2.39, 95% CI: [-2.37 - 6.99] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.35, stddev: 2.42, 95% CI: [-2.39 - 7.09] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.1, stddev: 2.2, 95% CI: [-2.22 - 6.41] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.13, stddev: 2.23, 95% CI: [-2.24 - 6.5] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.21, stddev: 2.3, 95% CI: [-2.29 - 6.72] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.19] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.18] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.19] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.46, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.37, stddev: 2.44, 95% CI: [-2.41 - 7.16] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.1 - 5.93] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.91, stddev: 2.05, 95% CI: [-2.1 - 5.92] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.9, stddev: 2.04, 95% CI: [-2.1 - 5.89] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.22, stddev: 2.3, 95% CI: [-2.3 - 6.73] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.17, stddev: 2.26, 95% CI: [-2.26 - 6.61] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.13, stddev: 2.23, 95% CI: [-2.23 - 6.49] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.36 - 6.96] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.36 - 6.96] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.35, stddev: 2.42, 95% CI: [-2.4 - 7.1] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.91, stddev: 2.05, 95% CI: [-2.1 - 5.92] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.91, stddev: 2.05, 95% CI: [-2.1 - 5.92] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.9, stddev: 2.04, 95% CI: [-2.1 - 5.9] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.35 - 6.92] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.28, stddev: 2.36, 95% CI: [-2.35 - 6.91] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.45, 95% CI: [-2.42 - 7.17] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.39, stddev: 2.45, 95% CI: [-2.42 - 7.2] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.38, stddev: 2.44, 95% CI: [-2.42 - 7.17] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.17, stddev: 2.26, 95% CI: [-2.26 - 6.61] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.23, stddev: 2.32, 95% CI: [-2.31 - 6.78] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.18, stddev: 2.27, 95% CI: [-2.27 - 6.62] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.35 - 6.93] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.34, stddev: 2.42, 95% CI: [-2.39 - 7.08] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.31, stddev: 2.39, 95% CI: [-2.37 - 6.99] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.1, stddev: 2.2, 95% CI: [-2.22 - 6.41] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.15, stddev: 2.25, 95% CI: [-2.26 - 6.56] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.1, stddev: 2.2, 95% CI: [-2.22 - 6.42] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.91, stddev: 2.05, 95% CI: [-2.1 - 5.92] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.14 - 6.11] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.13 - 6.08] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.01] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.31, stddev: 2.38, 95% CI: [-2.36 - 6.98] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.12, stddev: 2.22, 95% CI: [-2.24 - 6.48] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.19, stddev: 2.28, 95% CI: [-2.28 - 6.66] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.26, stddev: 2.35, 95% CI: [-2.34 - 6.86] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.12 - 6.05] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.1 - 5.94] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.9, stddev: 2.04, 95% CI: [-2.1 - 5.89] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.35 - 6.92] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.31, stddev: 2.38, 95% CI: [-2.36 - 6.98] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.37, stddev: 2.43, 95% CI: [-2.41 - 7.14] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 2.34, stddev: 2.41, 95% CI: [-2.39 - 7.07] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.1 - 5.98] - Actual: 0.9\n",
      "Plastic1: Prediction mean: 1.87, stddev: 2.01, 95% CI: [-2.07 - 5.8] - Actual: 0.9\n",
      "Rock1: Prediction mean: 2.08, stddev: 2.19, 95% CI: [-2.21 - 6.38] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.09, stddev: 2.19, 95% CI: [-2.21 - 6.38] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.09, stddev: 2.19, 95% CI: [-2.21 - 6.38] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.3, stddev: 2.38, 95% CI: [-2.37 - 6.97] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.27, stddev: 2.35, 95% CI: [-2.34 - 6.88] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.1, stddev: 2.21, 95% CI: [-2.22 - 6.43] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.93, stddev: 2.05, 95% CI: [-2.09 - 5.96] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.07] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.11] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.03] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.14, stddev: 2.24, 95% CI: [-2.25 - 6.52] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.14, stddev: 2.24, 95% CI: [-2.25 - 6.53] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.16, stddev: 2.26, 95% CI: [-2.27 - 6.59] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.29, stddev: 2.37, 95% CI: [-2.36 - 6.94] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.24, stddev: 2.33, 95% CI: [-2.32 - 6.8] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.02, stddev: 2.13, 95% CI: [-2.15 - 6.19] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.09, stddev: 2.18, 95% CI: [-2.2 - 6.37] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.03 - 5.71] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.07, stddev: 2.17, 95% CI: [-2.18 - 6.32] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.11, stddev: 2.21, 95% CI: [-2.21 - 6.44] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.12, stddev: 2.22, 95% CI: [-2.22 - 6.47] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.04] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.4, 95% CI: [-2.38 - 7.03] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.17, stddev: 2.26, 95% CI: [-2.26 - 6.61] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.82, stddev: 1.96, 95% CI: [-2.02 - 5.67] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.96, stddev: 2.08, 95% CI: [-2.11 - 6.03] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.03, stddev: 2.14, 95% CI: [-2.16 - 6.22] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.18, stddev: 2.27, 95% CI: [-2.26 - 6.62] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.33, stddev: 2.41, 95% CI: [-2.38 - 7.05] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.15, stddev: 2.24, 95% CI: [-2.24 - 6.53] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.01] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.17, stddev: 2.25, 95% CI: [-2.25 - 6.58] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.32, stddev: 2.39, 95% CI: [-2.37 - 7.01] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.12, stddev: 2.21, 95% CI: [-2.22 - 6.45] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.94, stddev: 2.06, 95% CI: [-2.09 - 5.97] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.82, stddev: 1.96, 95% CI: [-2.02 - 5.67] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.03, stddev: 2.14, 95% CI: [-2.16 - 6.23] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.12, stddev: 2.22, 95% CI: [-2.22 - 6.46] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 1.98, 95% CI: [-2.04 - 5.74] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.29, stddev: 2.36, 95% CI: [-2.35 - 6.92] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.31, stddev: 2.38, 95% CI: [-2.36 - 6.99] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.15, stddev: 2.24, 95% CI: [-2.24 - 6.54] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.83, stddev: 1.97, 95% CI: [-2.03 - 5.69] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 1.98, 95% CI: [-2.04 - 5.74] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 1.98, 95% CI: [-2.04 - 5.73] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.87, stddev: 2.01, 95% CI: [-2.06 - 5.8] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.82, stddev: 1.96, 95% CI: [-2.02 - 5.66] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 1.98, 95% CI: [-2.04 - 5.74] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 1.99, 95% CI: [-2.04 - 5.75] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.87, stddev: 2.01, 95% CI: [-2.07 - 5.81] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.04 - 5.71] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 1.99, 95% CI: [-2.05 - 5.75] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.85, stddev: 2.0, 95% CI: [-2.06 - 5.77] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.86, stddev: 2.01, 95% CI: [-2.07 - 5.8] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.12, stddev: 2.21, 95% CI: [-2.22 - 6.46] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.01, stddev: 2.11, 95% CI: [-2.14 - 6.15] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.15, stddev: 2.24, 95% CI: [-2.24 - 6.55] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.92, stddev: 2.05, 95% CI: [-2.09 - 5.94] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.98, stddev: 2.1, 95% CI: [-2.12 - 6.09] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.83, stddev: 1.97, 95% CI: [-2.02 - 5.69] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.97, 95% CI: [-2.02 - 5.7] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.82, stddev: 1.96, 95% CI: [-2.02 - 5.66] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.07, stddev: 2.17, 95% CI: [-2.18 - 6.33] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.93, stddev: 2.05, 95% CI: [-2.09 - 5.95] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.89, stddev: 2.02, 95% CI: [-2.06 - 5.85] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.11] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.11] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.86, stddev: 2.0, 95% CI: [-2.06 - 5.78] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.86, stddev: 2.0, 95% CI: [-2.06 - 5.78] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.86, stddev: 2.01, 95% CI: [-2.07 - 5.8] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.83, stddev: 1.97, 95% CI: [-2.03 - 5.69] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.04 - 5.72] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.92, stddev: 2.04, 95% CI: [-2.08 - 5.93] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.05 - 5.73] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.04 - 5.72] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.84, stddev: 1.98, 95% CI: [-2.05 - 5.73] - Actual: 3.2\n",
      "Rock1: Prediction mean: 2.31, stddev: 2.39, 95% CI: [-2.37 - 7.0] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.88, stddev: 2.01, 95% CI: [-2.06 - 5.82] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.83, stddev: 1.97, 95% CI: [-2.03 - 5.7] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.87, stddev: 2.0, 95% CI: [-2.06 - 5.8] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.83, stddev: 1.96, 95% CI: [-2.02 - 5.67] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.82, stddev: 1.96, 95% CI: [-2.02 - 5.66] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.97, stddev: 2.09, 95% CI: [-2.12 - 6.06] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.99, stddev: 2.1, 95% CI: [-2.13 - 6.1] - Actual: 3.2\n",
      "Rock1: Prediction mean: 1.98, stddev: 2.09, 95% CI: [-2.12 - 6.08] - Actual: 3.2\n"
     ]
    }
   ],
   "source": [
    "#line_test = []\n",
    "v = model.evaluate(x_tra, y_tra)\n",
    "t = model.evaluate(e_test_x, e_test_y)\n",
    "line_test = [str(v), str(t)]\n",
    "for idx in range(len(e_test_x)):\n",
    "    s = e_test_name[idx]+\": \" + f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \" + f\"stddev: {round(prediction_stdv[idx][0], 2)}, \" + f\"95% CI: [{round(lower[idx][0], 2)} - {round(upper[idx][0], 2)}]\" + f\" - Actual: {e_test_y[idx]}\"\n",
    "    line_test.append(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55ac358",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bnn_density_prob.txt', 'w') as f:\n",
    "    for line in line_test:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02853895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('bnn_density_prob.h5')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323db4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
