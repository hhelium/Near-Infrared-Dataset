{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d88abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 10:33:17.452478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 10:33:18.157088: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib64:/home/rilab/DTran/.other/Cpp_libraries/librealsense-2.47.0/install/lib:/home/rilab/anaconda3/envs/huiEnv/lib/\n",
      "2023-03-03 10:33:18.157159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib64:/home/rilab/DTran/.other/Cpp_libraries/librealsense-2.47.0/install/lib:/home/rilab/anaconda3/envs/huiEnv/lib/\n",
      "2023-03-03 10:33:18.157166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# train with fewer data. ideal data\n",
    "import os\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a25c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8014ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_data = pd.read_csv(\"encoded_data_64.csv\")\n",
    "label = pd.read_csv(\"properity.csv\")\n",
    "\n",
    "conditions = [\"materials\", \"color\", \"distance\", \"mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cbfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_data = r_data.drop(columns = conditions)\n",
    "#r_data = pd.concat([label, r_data], axis = 1)\n",
    "\n",
    "#e_data = e_data.drop(columns = conditions)\n",
    "e_data = e_data.drop(columns = \"materials\")\n",
    "\n",
    "e_data = pd.concat([label, e_data], axis = 1)\n",
    "\n",
    "e_data = e_data.drop(columns = [\"Hardness\", \"Rigidity\", \"Strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be607259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>names</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.069635</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.069555</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.013689</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.068797</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7</td>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.013690</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.177916</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.031733</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.171001</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.031593</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.068658</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.049947</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.067426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.044816</td>\n",
       "      <td>0.018668</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.021492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.042997</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.014174</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>0.044808</td>\n",
       "      <td>0.029736</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.063586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077216</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.014934</td>\n",
       "      <td>0.015921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.8</td>\n",
       "      <td>Wood</td>\n",
       "      <td>0.033415</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.042073</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.016336</td>\n",
       "      <td>0.061270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076010</td>\n",
       "      <td>0.035204</td>\n",
       "      <td>0.011117</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.022488</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.013483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Density      names  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0         0.7  Cardboard    0.068472    0.000798    0.001004    0.001597   \n",
       "1         0.7  Cardboard    0.069635    0.000800    0.001007    0.001591   \n",
       "2         0.7  Cardboard    0.069555    0.000802    0.001009    0.001596   \n",
       "3         0.7  Cardboard    0.068797    0.000795    0.001000    0.001586   \n",
       "4         0.7  Cardboard    0.069200    0.000808    0.001017    0.001614   \n",
       "...       ...        ...         ...         ...         ...         ...   \n",
       "1255      0.8       Wood    0.177916    0.001669    0.002065    0.002501   \n",
       "1256      0.8       Wood    0.171001    0.001768    0.002182    0.002713   \n",
       "1257      0.8       Wood    0.068658    0.018290    0.019366    0.061013   \n",
       "1258      0.8       Wood    0.042997    0.013600    0.014174    0.054635   \n",
       "1259      0.8       Wood    0.033415    0.011559    0.011936    0.051234   \n",
       "\n",
       "      spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0       0.001190    0.001387    0.001140    0.001251  ...     0.001410   \n",
       "1       0.001185    0.001386    0.001142    0.001244  ...     0.001401   \n",
       "2       0.001189    0.001389    0.001144    0.001248  ...     0.001405   \n",
       "3       0.001182    0.001379    0.001135    0.001241  ...     0.001398   \n",
       "4       0.001203    0.001403    0.001154    0.001265  ...     0.001424   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255    0.001866    0.002422    0.002268    0.001885  ...     0.001998   \n",
       "1256    0.002029    0.002593    0.002403    0.002062  ...     0.002193   \n",
       "1257    0.049947    0.036245    0.025003    0.067426  ...     0.078777   \n",
       "1258    0.044808    0.029736    0.019006    0.063586  ...     0.077216   \n",
       "1259    0.042073    0.026621    0.016336    0.061270  ...     0.076010   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0        0.001230     0.001109     0.001239     0.002353     0.013542   \n",
       "1        0.001227     0.001113     0.001243     0.002366     0.013688   \n",
       "2        0.001230     0.001115     0.001245     0.002369     0.013689   \n",
       "3        0.001222     0.001105     0.001234     0.002347     0.013557   \n",
       "4        0.001244     0.001123     0.001254     0.002381     0.013690   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255     0.002049     0.002333     0.002529     0.005113     0.031733   \n",
       "1256     0.002215     0.002454     0.002665     0.005292     0.031593   \n",
       "1257     0.044816     0.018668     0.021073     0.024119     0.041496   \n",
       "1258     0.038426     0.013364     0.015166     0.016858     0.027898   \n",
       "1259     0.035204     0.011117     0.012640     0.013839     0.022488   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0        0.000959     0.001235     0.001654     0.001062  \n",
       "1        0.000963     0.001240     0.001661     0.001065  \n",
       "2        0.000965     0.001242     0.001664     0.001067  \n",
       "3        0.000956     0.001231     0.001649     0.001058  \n",
       "4        0.000971     0.001250     0.001674     0.001075  \n",
       "...           ...          ...          ...          ...  \n",
       "1255     0.002108     0.002574     0.003430     0.002144  \n",
       "1256     0.002209     0.002700     0.003582     0.002268  \n",
       "1257     0.015430     0.019119     0.020862     0.021492  \n",
       "1258     0.010784     0.013917     0.014934     0.015921  \n",
       "1259     0.008854     0.011695     0.012428     0.013483  \n",
       "\n",
       "[1260 rows x 66 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1cd1d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>spectrum_8</th>\n",
       "      <th>spectrum_9</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068472</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069635</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069555</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.013689</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068797</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.013690</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.177916</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>0.031733</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0.171001</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.010187</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.031593</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0.068658</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>0.061013</td>\n",
       "      <td>0.049947</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>0.025003</td>\n",
       "      <td>0.067426</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>0.087812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.044816</td>\n",
       "      <td>0.018668</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.020862</td>\n",
       "      <td>0.021492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.042997</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.014174</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>0.044808</td>\n",
       "      <td>0.029736</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.063586</td>\n",
       "      <td>0.018080</td>\n",
       "      <td>0.088403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077216</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>0.015166</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.014934</td>\n",
       "      <td>0.015921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.033415</td>\n",
       "      <td>0.011559</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.042073</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.016336</td>\n",
       "      <td>0.061270</td>\n",
       "      <td>0.014725</td>\n",
       "      <td>0.088203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076010</td>\n",
       "      <td>0.035204</td>\n",
       "      <td>0.011117</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.022488</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.013483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spectrum_0  spectrum_1  spectrum_2  spectrum_3  spectrum_4  spectrum_5  \\\n",
       "0       0.068472    0.000798    0.001004    0.001597    0.001190    0.001387   \n",
       "1       0.069635    0.000800    0.001007    0.001591    0.001185    0.001386   \n",
       "2       0.069555    0.000802    0.001009    0.001596    0.001189    0.001389   \n",
       "3       0.068797    0.000795    0.001000    0.001586    0.001182    0.001379   \n",
       "4       0.069200    0.000808    0.001017    0.001614    0.001203    0.001403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1255    0.177916    0.001669    0.002065    0.002501    0.001866    0.002422   \n",
       "1256    0.171001    0.001768    0.002182    0.002713    0.002029    0.002593   \n",
       "1257    0.068658    0.018290    0.019366    0.061013    0.049947    0.036245   \n",
       "1258    0.042997    0.013600    0.014174    0.054635    0.044808    0.029736   \n",
       "1259    0.033415    0.011559    0.011936    0.051234    0.042073    0.026621   \n",
       "\n",
       "      spectrum_6  spectrum_7  spectrum_8  spectrum_9  ...  spectrum_54  \\\n",
       "0       0.001140    0.001251    0.004277    0.001168  ...     0.001410   \n",
       "1       0.001142    0.001244    0.004314    0.001159  ...     0.001401   \n",
       "2       0.001144    0.001248    0.004316    0.001163  ...     0.001405   \n",
       "3       0.001135    0.001241    0.004274    0.001158  ...     0.001398   \n",
       "4       0.001154    0.001265    0.004328    0.001180  ...     0.001424   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255    0.002268    0.001885    0.010040    0.001635  ...     0.001998   \n",
       "1256    0.002403    0.002062    0.010187    0.001808  ...     0.002193   \n",
       "1257    0.025003    0.067426    0.026343    0.087812  ...     0.078777   \n",
       "1258    0.019006    0.063586    0.018080    0.088403  ...     0.077216   \n",
       "1259    0.016336    0.061270    0.014725    0.088203  ...     0.076010   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0        0.001230     0.001109     0.001239     0.002353     0.013542   \n",
       "1        0.001227     0.001113     0.001243     0.002366     0.013688   \n",
       "2        0.001230     0.001115     0.001245     0.002369     0.013689   \n",
       "3        0.001222     0.001105     0.001234     0.002347     0.013557   \n",
       "4        0.001244     0.001123     0.001254     0.002381     0.013690   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255     0.002049     0.002333     0.002529     0.005113     0.031733   \n",
       "1256     0.002215     0.002454     0.002665     0.005292     0.031593   \n",
       "1257     0.044816     0.018668     0.021073     0.024119     0.041496   \n",
       "1258     0.038426     0.013364     0.015166     0.016858     0.027898   \n",
       "1259     0.035204     0.011117     0.012640     0.013839     0.022488   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0        0.000959     0.001235     0.001654     0.001062  \n",
       "1        0.000963     0.001240     0.001661     0.001065  \n",
       "2        0.000965     0.001242     0.001664     0.001067  \n",
       "3        0.000956     0.001231     0.001649     0.001058  \n",
       "4        0.000971     0.001250     0.001674     0.001075  \n",
       "...           ...          ...          ...          ...  \n",
       "1255     0.002108     0.002574     0.003430     0.002144  \n",
       "1256     0.002209     0.002700     0.003582     0.002268  \n",
       "1257     0.015430     0.019119     0.020862     0.021492  \n",
       "1258     0.010784     0.013917     0.014934     0.015921  \n",
       "1259     0.008854     0.011695     0.012428     0.013483  \n",
       "\n",
       "[1260 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data \n",
    "tep = e_data.loc[:, [ \"names\", \"Density\"]]\n",
    "e_data_sp = e_data.copy().drop(columns = ['names', \"Density\"])\n",
    "e_data_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc17f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>spectrum_8</th>\n",
       "      <th>spectrum_9</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.115912</td>\n",
       "      <td>-0.728643</td>\n",
       "      <td>-0.749686</td>\n",
       "      <td>-0.736475</td>\n",
       "      <td>-0.715072</td>\n",
       "      <td>-0.743912</td>\n",
       "      <td>-0.749965</td>\n",
       "      <td>-0.716269</td>\n",
       "      <td>-0.981183</td>\n",
       "      <td>-0.710028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717374</td>\n",
       "      <td>-0.724037</td>\n",
       "      <td>-0.766685</td>\n",
       "      <td>-0.776774</td>\n",
       "      <td>-0.872160</td>\n",
       "      <td>-1.148111</td>\n",
       "      <td>-0.777188</td>\n",
       "      <td>-0.768012</td>\n",
       "      <td>-0.804966</td>\n",
       "      <td>-0.751950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.107021</td>\n",
       "      <td>-0.728564</td>\n",
       "      <td>-0.749586</td>\n",
       "      <td>-0.736574</td>\n",
       "      <td>-0.715155</td>\n",
       "      <td>-0.743936</td>\n",
       "      <td>-0.749900</td>\n",
       "      <td>-0.716370</td>\n",
       "      <td>-0.980187</td>\n",
       "      <td>-0.710138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717494</td>\n",
       "      <td>-0.724098</td>\n",
       "      <td>-0.766545</td>\n",
       "      <td>-0.776651</td>\n",
       "      <td>-0.871762</td>\n",
       "      <td>-1.145388</td>\n",
       "      <td>-0.777012</td>\n",
       "      <td>-0.767850</td>\n",
       "      <td>-0.804733</td>\n",
       "      <td>-0.751868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.107631</td>\n",
       "      <td>-0.728518</td>\n",
       "      <td>-0.749531</td>\n",
       "      <td>-0.736507</td>\n",
       "      <td>-0.715099</td>\n",
       "      <td>-0.743868</td>\n",
       "      <td>-0.749841</td>\n",
       "      <td>-0.716316</td>\n",
       "      <td>-0.980121</td>\n",
       "      <td>-0.710092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717439</td>\n",
       "      <td>-0.724039</td>\n",
       "      <td>-0.766487</td>\n",
       "      <td>-0.776585</td>\n",
       "      <td>-0.871679</td>\n",
       "      <td>-1.145386</td>\n",
       "      <td>-0.776958</td>\n",
       "      <td>-0.767793</td>\n",
       "      <td>-0.804666</td>\n",
       "      <td>-0.751810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.113432</td>\n",
       "      <td>-0.728751</td>\n",
       "      <td>-0.749813</td>\n",
       "      <td>-0.736657</td>\n",
       "      <td>-0.715224</td>\n",
       "      <td>-0.744087</td>\n",
       "      <td>-0.750108</td>\n",
       "      <td>-0.716417</td>\n",
       "      <td>-0.981245</td>\n",
       "      <td>-0.710159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717527</td>\n",
       "      <td>-0.724194</td>\n",
       "      <td>-0.766816</td>\n",
       "      <td>-0.776929</td>\n",
       "      <td>-0.872326</td>\n",
       "      <td>-1.147828</td>\n",
       "      <td>-0.777308</td>\n",
       "      <td>-0.768141</td>\n",
       "      <td>-0.805111</td>\n",
       "      <td>-0.752090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.110347</td>\n",
       "      <td>-0.728303</td>\n",
       "      <td>-0.749277</td>\n",
       "      <td>-0.736200</td>\n",
       "      <td>-0.714843</td>\n",
       "      <td>-0.743558</td>\n",
       "      <td>-0.749570</td>\n",
       "      <td>-0.716072</td>\n",
       "      <td>-0.979809</td>\n",
       "      <td>-0.709883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717189</td>\n",
       "      <td>-0.723770</td>\n",
       "      <td>-0.766220</td>\n",
       "      <td>-0.776280</td>\n",
       "      <td>-0.871300</td>\n",
       "      <td>-1.145354</td>\n",
       "      <td>-0.776703</td>\n",
       "      <td>-0.767523</td>\n",
       "      <td>-0.804350</td>\n",
       "      <td>-0.751541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>-0.279136</td>\n",
       "      <td>-0.699879</td>\n",
       "      <td>-0.715272</td>\n",
       "      <td>-0.721783</td>\n",
       "      <td>-0.702883</td>\n",
       "      <td>-0.720305</td>\n",
       "      <td>-0.718238</td>\n",
       "      <td>-0.706856</td>\n",
       "      <td>-0.827207</td>\n",
       "      <td>-0.704335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709624</td>\n",
       "      <td>-0.708081</td>\n",
       "      <td>-0.725917</td>\n",
       "      <td>-0.734978</td>\n",
       "      <td>-0.789022</td>\n",
       "      <td>-0.810365</td>\n",
       "      <td>-0.732375</td>\n",
       "      <td>-0.724863</td>\n",
       "      <td>-0.749657</td>\n",
       "      <td>-0.718353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>-0.332007</td>\n",
       "      <td>-0.696639</td>\n",
       "      <td>-0.711489</td>\n",
       "      <td>-0.718338</td>\n",
       "      <td>-0.699946</td>\n",
       "      <td>-0.716421</td>\n",
       "      <td>-0.714437</td>\n",
       "      <td>-0.704214</td>\n",
       "      <td>-0.823262</td>\n",
       "      <td>-0.702229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707049</td>\n",
       "      <td>-0.704845</td>\n",
       "      <td>-0.721901</td>\n",
       "      <td>-0.730574</td>\n",
       "      <td>-0.783617</td>\n",
       "      <td>-0.812966</td>\n",
       "      <td>-0.728413</td>\n",
       "      <td>-0.720826</td>\n",
       "      <td>-0.744929</td>\n",
       "      <td>-0.714494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>-1.114494</td>\n",
       "      <td>-0.151241</td>\n",
       "      <td>-0.154344</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>0.164377</td>\n",
       "      <td>0.050968</td>\n",
       "      <td>-0.078866</td>\n",
       "      <td>0.267358</td>\n",
       "      <td>-0.391611</td>\n",
       "      <td>0.345947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302501</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>-0.181908</td>\n",
       "      <td>-0.134064</td>\n",
       "      <td>-0.216604</td>\n",
       "      <td>-0.629094</td>\n",
       "      <td>-0.212757</td>\n",
       "      <td>-0.191827</td>\n",
       "      <td>-0.206766</td>\n",
       "      <td>-0.117266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>-1.310687</td>\n",
       "      <td>-0.306069</td>\n",
       "      <td>-0.322686</td>\n",
       "      <td>0.125911</td>\n",
       "      <td>0.071682</td>\n",
       "      <td>-0.097453</td>\n",
       "      <td>-0.247510</td>\n",
       "      <td>0.210284</td>\n",
       "      <td>-0.612372</td>\n",
       "      <td>0.353154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281922</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>-0.358556</td>\n",
       "      <td>-0.325485</td>\n",
       "      <td>-0.435267</td>\n",
       "      <td>-0.881566</td>\n",
       "      <td>-0.393971</td>\n",
       "      <td>-0.359431</td>\n",
       "      <td>-0.391404</td>\n",
       "      <td>-0.290328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>-1.383952</td>\n",
       "      <td>-0.373431</td>\n",
       "      <td>-0.395249</td>\n",
       "      <td>0.070616</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>-0.168501</td>\n",
       "      <td>-0.322609</td>\n",
       "      <td>0.175854</td>\n",
       "      <td>-0.702019</td>\n",
       "      <td>0.350717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266022</td>\n",
       "      <td>-0.061889</td>\n",
       "      <td>-0.433370</td>\n",
       "      <td>-0.407333</td>\n",
       "      <td>-0.526199</td>\n",
       "      <td>-0.982008</td>\n",
       "      <td>-0.469225</td>\n",
       "      <td>-0.431036</td>\n",
       "      <td>-0.469437</td>\n",
       "      <td>-0.366072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spectrum_0  spectrum_1  spectrum_2  spectrum_3  spectrum_4  spectrum_5  \\\n",
       "0      -1.115912   -0.728643   -0.749686   -0.736475   -0.715072   -0.743912   \n",
       "1      -1.107021   -0.728564   -0.749586   -0.736574   -0.715155   -0.743936   \n",
       "2      -1.107631   -0.728518   -0.749531   -0.736507   -0.715099   -0.743868   \n",
       "3      -1.113432   -0.728751   -0.749813   -0.736657   -0.715224   -0.744087   \n",
       "4      -1.110347   -0.728303   -0.749277   -0.736200   -0.714843   -0.743558   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1255   -0.279136   -0.699879   -0.715272   -0.721783   -0.702883   -0.720305   \n",
       "1256   -0.332007   -0.696639   -0.711489   -0.718338   -0.699946   -0.716421   \n",
       "1257   -1.114494   -0.151241   -0.154344    0.229621    0.164377    0.050968   \n",
       "1258   -1.310687   -0.306069   -0.322686    0.125911    0.071682   -0.097453   \n",
       "1259   -1.383952   -0.373431   -0.395249    0.070616    0.022354   -0.168501   \n",
       "\n",
       "      spectrum_6  spectrum_7  spectrum_8  spectrum_9  ...  spectrum_54  \\\n",
       "0      -0.749965   -0.716269   -0.981183   -0.710028  ...    -0.717374   \n",
       "1      -0.749900   -0.716370   -0.980187   -0.710138  ...    -0.717494   \n",
       "2      -0.749841   -0.716316   -0.980121   -0.710092  ...    -0.717439   \n",
       "3      -0.750108   -0.716417   -0.981245   -0.710159  ...    -0.717527   \n",
       "4      -0.749570   -0.716072   -0.979809   -0.709883  ...    -0.717189   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255   -0.718238   -0.706856   -0.827207   -0.704335  ...    -0.709624   \n",
       "1256   -0.714437   -0.704214   -0.823262   -0.702229  ...    -0.707049   \n",
       "1257   -0.078866    0.267358   -0.391611    0.345947  ...     0.302501   \n",
       "1258   -0.247510    0.210284   -0.612372    0.353154  ...     0.281922   \n",
       "1259   -0.322609    0.175854   -0.702019    0.350717  ...     0.266022   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0       -0.724037    -0.766685    -0.776774    -0.872160    -1.148111   \n",
       "1       -0.724098    -0.766545    -0.776651    -0.871762    -1.145388   \n",
       "2       -0.724039    -0.766487    -0.776585    -0.871679    -1.145386   \n",
       "3       -0.724194    -0.766816    -0.776929    -0.872326    -1.147828   \n",
       "4       -0.723770    -0.766220    -0.776280    -0.871300    -1.145354   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255    -0.708081    -0.725917    -0.734978    -0.789022    -0.810365   \n",
       "1256    -0.704845    -0.721901    -0.730574    -0.783617    -0.812966   \n",
       "1257     0.125445    -0.181908    -0.134064    -0.216604    -0.629094   \n",
       "1258     0.000922    -0.358556    -0.325485    -0.435267    -0.881566   \n",
       "1259    -0.061889    -0.433370    -0.407333    -0.526199    -0.982008   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0       -0.777188    -0.768012    -0.804966    -0.751950  \n",
       "1       -0.777012    -0.767850    -0.804733    -0.751868  \n",
       "2       -0.776958    -0.767793    -0.804666    -0.751810  \n",
       "3       -0.777308    -0.768141    -0.805111    -0.752090  \n",
       "4       -0.776703    -0.767523    -0.804350    -0.751541  \n",
       "...           ...          ...          ...          ...  \n",
       "1255    -0.732375    -0.724863    -0.749657    -0.718353  \n",
       "1256    -0.728413    -0.720826    -0.744929    -0.714494  \n",
       "1257    -0.212757    -0.191827    -0.206766    -0.117266  \n",
       "1258    -0.393971    -0.359431    -0.391404    -0.290328  \n",
       "1259    -0.469225    -0.431036    -0.469437    -0.366072  \n",
       "\n",
       "[1260 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data_sp = (e_data_sp - e_data_sp.mean())/e_data_sp.std()\n",
    "e_data_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fc8849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>Density</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.115912</td>\n",
       "      <td>-0.728643</td>\n",
       "      <td>-0.749686</td>\n",
       "      <td>-0.736475</td>\n",
       "      <td>-0.715072</td>\n",
       "      <td>-0.743912</td>\n",
       "      <td>-0.749965</td>\n",
       "      <td>-0.716269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717374</td>\n",
       "      <td>-0.724037</td>\n",
       "      <td>-0.766685</td>\n",
       "      <td>-0.776774</td>\n",
       "      <td>-0.872160</td>\n",
       "      <td>-1.148111</td>\n",
       "      <td>-0.777188</td>\n",
       "      <td>-0.768012</td>\n",
       "      <td>-0.804966</td>\n",
       "      <td>-0.751950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.107021</td>\n",
       "      <td>-0.728564</td>\n",
       "      <td>-0.749586</td>\n",
       "      <td>-0.736574</td>\n",
       "      <td>-0.715155</td>\n",
       "      <td>-0.743936</td>\n",
       "      <td>-0.749900</td>\n",
       "      <td>-0.716370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717494</td>\n",
       "      <td>-0.724098</td>\n",
       "      <td>-0.766545</td>\n",
       "      <td>-0.776651</td>\n",
       "      <td>-0.871762</td>\n",
       "      <td>-1.145388</td>\n",
       "      <td>-0.777012</td>\n",
       "      <td>-0.767850</td>\n",
       "      <td>-0.804733</td>\n",
       "      <td>-0.751868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.107631</td>\n",
       "      <td>-0.728518</td>\n",
       "      <td>-0.749531</td>\n",
       "      <td>-0.736507</td>\n",
       "      <td>-0.715099</td>\n",
       "      <td>-0.743868</td>\n",
       "      <td>-0.749841</td>\n",
       "      <td>-0.716316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717439</td>\n",
       "      <td>-0.724039</td>\n",
       "      <td>-0.766487</td>\n",
       "      <td>-0.776585</td>\n",
       "      <td>-0.871679</td>\n",
       "      <td>-1.145386</td>\n",
       "      <td>-0.776958</td>\n",
       "      <td>-0.767793</td>\n",
       "      <td>-0.804666</td>\n",
       "      <td>-0.751810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.113432</td>\n",
       "      <td>-0.728751</td>\n",
       "      <td>-0.749813</td>\n",
       "      <td>-0.736657</td>\n",
       "      <td>-0.715224</td>\n",
       "      <td>-0.744087</td>\n",
       "      <td>-0.750108</td>\n",
       "      <td>-0.716417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717527</td>\n",
       "      <td>-0.724194</td>\n",
       "      <td>-0.766816</td>\n",
       "      <td>-0.776929</td>\n",
       "      <td>-0.872326</td>\n",
       "      <td>-1.147828</td>\n",
       "      <td>-0.777308</td>\n",
       "      <td>-0.768141</td>\n",
       "      <td>-0.805111</td>\n",
       "      <td>-0.752090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.110347</td>\n",
       "      <td>-0.728303</td>\n",
       "      <td>-0.749277</td>\n",
       "      <td>-0.736200</td>\n",
       "      <td>-0.714843</td>\n",
       "      <td>-0.743558</td>\n",
       "      <td>-0.749570</td>\n",
       "      <td>-0.716072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717189</td>\n",
       "      <td>-0.723770</td>\n",
       "      <td>-0.766220</td>\n",
       "      <td>-0.776280</td>\n",
       "      <td>-0.871300</td>\n",
       "      <td>-1.145354</td>\n",
       "      <td>-0.776703</td>\n",
       "      <td>-0.767523</td>\n",
       "      <td>-0.804350</td>\n",
       "      <td>-0.751541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.279136</td>\n",
       "      <td>-0.699879</td>\n",
       "      <td>-0.715272</td>\n",
       "      <td>-0.721783</td>\n",
       "      <td>-0.702883</td>\n",
       "      <td>-0.720305</td>\n",
       "      <td>-0.718238</td>\n",
       "      <td>-0.706856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709624</td>\n",
       "      <td>-0.708081</td>\n",
       "      <td>-0.725917</td>\n",
       "      <td>-0.734978</td>\n",
       "      <td>-0.789022</td>\n",
       "      <td>-0.810365</td>\n",
       "      <td>-0.732375</td>\n",
       "      <td>-0.724863</td>\n",
       "      <td>-0.749657</td>\n",
       "      <td>-0.718353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.332007</td>\n",
       "      <td>-0.696639</td>\n",
       "      <td>-0.711489</td>\n",
       "      <td>-0.718338</td>\n",
       "      <td>-0.699946</td>\n",
       "      <td>-0.716421</td>\n",
       "      <td>-0.714437</td>\n",
       "      <td>-0.704214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707049</td>\n",
       "      <td>-0.704845</td>\n",
       "      <td>-0.721901</td>\n",
       "      <td>-0.730574</td>\n",
       "      <td>-0.783617</td>\n",
       "      <td>-0.812966</td>\n",
       "      <td>-0.728413</td>\n",
       "      <td>-0.720826</td>\n",
       "      <td>-0.744929</td>\n",
       "      <td>-0.714494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.114494</td>\n",
       "      <td>-0.151241</td>\n",
       "      <td>-0.154344</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>0.164377</td>\n",
       "      <td>0.050968</td>\n",
       "      <td>-0.078866</td>\n",
       "      <td>0.267358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302501</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>-0.181908</td>\n",
       "      <td>-0.134064</td>\n",
       "      <td>-0.216604</td>\n",
       "      <td>-0.629094</td>\n",
       "      <td>-0.212757</td>\n",
       "      <td>-0.191827</td>\n",
       "      <td>-0.206766</td>\n",
       "      <td>-0.117266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.310687</td>\n",
       "      <td>-0.306069</td>\n",
       "      <td>-0.322686</td>\n",
       "      <td>0.125911</td>\n",
       "      <td>0.071682</td>\n",
       "      <td>-0.097453</td>\n",
       "      <td>-0.247510</td>\n",
       "      <td>0.210284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281922</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>-0.358556</td>\n",
       "      <td>-0.325485</td>\n",
       "      <td>-0.435267</td>\n",
       "      <td>-0.881566</td>\n",
       "      <td>-0.393971</td>\n",
       "      <td>-0.359431</td>\n",
       "      <td>-0.391404</td>\n",
       "      <td>-0.290328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.383952</td>\n",
       "      <td>-0.373431</td>\n",
       "      <td>-0.395249</td>\n",
       "      <td>0.070616</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>-0.168501</td>\n",
       "      <td>-0.322609</td>\n",
       "      <td>0.175854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266022</td>\n",
       "      <td>-0.061889</td>\n",
       "      <td>-0.433370</td>\n",
       "      <td>-0.407333</td>\n",
       "      <td>-0.526199</td>\n",
       "      <td>-0.982008</td>\n",
       "      <td>-0.469225</td>\n",
       "      <td>-0.431036</td>\n",
       "      <td>-0.469437</td>\n",
       "      <td>-0.366072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          names  Density  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0     Cardboard      0.7   -1.115912   -0.728643   -0.749686   -0.736475   \n",
       "1     Cardboard      0.7   -1.107021   -0.728564   -0.749586   -0.736574   \n",
       "2     Cardboard      0.7   -1.107631   -0.728518   -0.749531   -0.736507   \n",
       "3     Cardboard      0.7   -1.113432   -0.728751   -0.749813   -0.736657   \n",
       "4     Cardboard      0.7   -1.110347   -0.728303   -0.749277   -0.736200   \n",
       "...         ...      ...         ...         ...         ...         ...   \n",
       "1255       Wood      0.8   -0.279136   -0.699879   -0.715272   -0.721783   \n",
       "1256       Wood      0.8   -0.332007   -0.696639   -0.711489   -0.718338   \n",
       "1257       Wood      0.8   -1.114494   -0.151241   -0.154344    0.229621   \n",
       "1258       Wood      0.8   -1.310687   -0.306069   -0.322686    0.125911   \n",
       "1259       Wood      0.8   -1.383952   -0.373431   -0.395249    0.070616   \n",
       "\n",
       "      spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0      -0.715072   -0.743912   -0.749965   -0.716269  ...    -0.717374   \n",
       "1      -0.715155   -0.743936   -0.749900   -0.716370  ...    -0.717494   \n",
       "2      -0.715099   -0.743868   -0.749841   -0.716316  ...    -0.717439   \n",
       "3      -0.715224   -0.744087   -0.750108   -0.716417  ...    -0.717527   \n",
       "4      -0.714843   -0.743558   -0.749570   -0.716072  ...    -0.717189   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1255   -0.702883   -0.720305   -0.718238   -0.706856  ...    -0.709624   \n",
       "1256   -0.699946   -0.716421   -0.714437   -0.704214  ...    -0.707049   \n",
       "1257    0.164377    0.050968   -0.078866    0.267358  ...     0.302501   \n",
       "1258    0.071682   -0.097453   -0.247510    0.210284  ...     0.281922   \n",
       "1259    0.022354   -0.168501   -0.322609    0.175854  ...     0.266022   \n",
       "\n",
       "      spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0       -0.724037    -0.766685    -0.776774    -0.872160    -1.148111   \n",
       "1       -0.724098    -0.766545    -0.776651    -0.871762    -1.145388   \n",
       "2       -0.724039    -0.766487    -0.776585    -0.871679    -1.145386   \n",
       "3       -0.724194    -0.766816    -0.776929    -0.872326    -1.147828   \n",
       "4       -0.723770    -0.766220    -0.776280    -0.871300    -1.145354   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1255    -0.708081    -0.725917    -0.734978    -0.789022    -0.810365   \n",
       "1256    -0.704845    -0.721901    -0.730574    -0.783617    -0.812966   \n",
       "1257     0.125445    -0.181908    -0.134064    -0.216604    -0.629094   \n",
       "1258     0.000922    -0.358556    -0.325485    -0.435267    -0.881566   \n",
       "1259    -0.061889    -0.433370    -0.407333    -0.526199    -0.982008   \n",
       "\n",
       "      spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0       -0.777188    -0.768012    -0.804966    -0.751950  \n",
       "1       -0.777012    -0.767850    -0.804733    -0.751868  \n",
       "2       -0.776958    -0.767793    -0.804666    -0.751810  \n",
       "3       -0.777308    -0.768141    -0.805111    -0.752090  \n",
       "4       -0.776703    -0.767523    -0.804350    -0.751541  \n",
       "...           ...          ...          ...          ...  \n",
       "1255    -0.732375    -0.724863    -0.749657    -0.718353  \n",
       "1256    -0.728413    -0.720826    -0.744929    -0.714494  \n",
       "1257    -0.212757    -0.191827    -0.206766    -0.117266  \n",
       "1258    -0.393971    -0.359431    -0.391404    -0.290328  \n",
       "1259    -0.469225    -0.431036    -0.469437    -0.366072  \n",
       "\n",
       "[1260 rows x 66 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_data = pd.concat([tep, e_data_sp], axis =1)\n",
    "e_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee87d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>Density</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.222089</td>\n",
       "      <td>-0.731775</td>\n",
       "      <td>-0.753505</td>\n",
       "      <td>-0.737678</td>\n",
       "      <td>-0.716104</td>\n",
       "      <td>-0.746348</td>\n",
       "      <td>-0.753370</td>\n",
       "      <td>-0.716866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717742</td>\n",
       "      <td>-0.725527</td>\n",
       "      <td>-0.771206</td>\n",
       "      <td>-0.781378</td>\n",
       "      <td>-0.881510</td>\n",
       "      <td>-1.188793</td>\n",
       "      <td>-0.782123</td>\n",
       "      <td>-0.772902</td>\n",
       "      <td>-0.811379</td>\n",
       "      <td>-0.755607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.222889</td>\n",
       "      <td>-0.732071</td>\n",
       "      <td>-0.753856</td>\n",
       "      <td>-0.738047</td>\n",
       "      <td>-0.716408</td>\n",
       "      <td>-0.746739</td>\n",
       "      <td>-0.753741</td>\n",
       "      <td>-0.717155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718031</td>\n",
       "      <td>-0.725854</td>\n",
       "      <td>-0.771593</td>\n",
       "      <td>-0.781812</td>\n",
       "      <td>-0.882156</td>\n",
       "      <td>-1.190055</td>\n",
       "      <td>-0.782506</td>\n",
       "      <td>-0.773298</td>\n",
       "      <td>-0.811857</td>\n",
       "      <td>-0.755978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.223594</td>\n",
       "      <td>-0.732304</td>\n",
       "      <td>-0.754133</td>\n",
       "      <td>-0.738336</td>\n",
       "      <td>-0.716645</td>\n",
       "      <td>-0.747047</td>\n",
       "      <td>-0.754034</td>\n",
       "      <td>-0.717382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.718257</td>\n",
       "      <td>-0.726112</td>\n",
       "      <td>-0.771899</td>\n",
       "      <td>-0.782154</td>\n",
       "      <td>-0.882668</td>\n",
       "      <td>-1.191074</td>\n",
       "      <td>-0.782807</td>\n",
       "      <td>-0.773610</td>\n",
       "      <td>-0.812235</td>\n",
       "      <td>-0.756272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.253176</td>\n",
       "      <td>-0.715537</td>\n",
       "      <td>-0.734651</td>\n",
       "      <td>-0.712910</td>\n",
       "      <td>-0.695747</td>\n",
       "      <td>-0.722589</td>\n",
       "      <td>-0.732372</td>\n",
       "      <td>-0.696245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.696845</td>\n",
       "      <td>-0.704232</td>\n",
       "      <td>-0.750996</td>\n",
       "      <td>-0.758113</td>\n",
       "      <td>-0.850779</td>\n",
       "      <td>-1.152454</td>\n",
       "      <td>-0.762656</td>\n",
       "      <td>-0.752706</td>\n",
       "      <td>-0.787926</td>\n",
       "      <td>-0.735098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardboard1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.286577</td>\n",
       "      <td>-0.697994</td>\n",
       "      <td>-0.714912</td>\n",
       "      <td>-0.681122</td>\n",
       "      <td>-0.669507</td>\n",
       "      <td>-0.694998</td>\n",
       "      <td>-0.709313</td>\n",
       "      <td>-0.667894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667802</td>\n",
       "      <td>-0.677601</td>\n",
       "      <td>-0.730416</td>\n",
       "      <td>-0.733973</td>\n",
       "      <td>-0.822202</td>\n",
       "      <td>-1.129839</td>\n",
       "      <td>-0.743140</td>\n",
       "      <td>-0.732515</td>\n",
       "      <td>-0.765458</td>\n",
       "      <td>-0.713138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.890006</td>\n",
       "      <td>-0.713101</td>\n",
       "      <td>-0.731134</td>\n",
       "      <td>-0.724364</td>\n",
       "      <td>-0.704868</td>\n",
       "      <td>-0.728169</td>\n",
       "      <td>-0.732157</td>\n",
       "      <td>-0.707552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709225</td>\n",
       "      <td>-0.712104</td>\n",
       "      <td>-0.745627</td>\n",
       "      <td>-0.754517</td>\n",
       "      <td>-0.833966</td>\n",
       "      <td>-1.030883</td>\n",
       "      <td>-0.755103</td>\n",
       "      <td>-0.745954</td>\n",
       "      <td>-0.777340</td>\n",
       "      <td>-0.733455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.885503</td>\n",
       "      <td>-0.712588</td>\n",
       "      <td>-0.730527</td>\n",
       "      <td>-0.723925</td>\n",
       "      <td>-0.704497</td>\n",
       "      <td>-0.727629</td>\n",
       "      <td>-0.731567</td>\n",
       "      <td>-0.707226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708916</td>\n",
       "      <td>-0.711679</td>\n",
       "      <td>-0.744948</td>\n",
       "      <td>-0.753794</td>\n",
       "      <td>-0.832799</td>\n",
       "      <td>-1.027887</td>\n",
       "      <td>-0.754397</td>\n",
       "      <td>-0.745252</td>\n",
       "      <td>-0.776474</td>\n",
       "      <td>-0.732846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-1.431018</td>\n",
       "      <td>-0.394084</td>\n",
       "      <td>-0.422334</td>\n",
       "      <td>0.090835</td>\n",
       "      <td>0.049914</td>\n",
       "      <td>-0.179356</td>\n",
       "      <td>-0.347860</td>\n",
       "      <td>0.228330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348304</td>\n",
       "      <td>-0.056555</td>\n",
       "      <td>-0.464889</td>\n",
       "      <td>-0.444711</td>\n",
       "      <td>-0.573235</td>\n",
       "      <td>-1.041221</td>\n",
       "      <td>-0.502578</td>\n",
       "      <td>-0.458578</td>\n",
       "      <td>-0.503277</td>\n",
       "      <td>-0.395475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-1.481978</td>\n",
       "      <td>-0.463948</td>\n",
       "      <td>-0.494377</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>-0.032057</td>\n",
       "      <td>-0.266088</td>\n",
       "      <td>-0.426501</td>\n",
       "      <td>0.152261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285322</td>\n",
       "      <td>-0.143506</td>\n",
       "      <td>-0.535887</td>\n",
       "      <td>-0.521557</td>\n",
       "      <td>-0.652785</td>\n",
       "      <td>-1.120802</td>\n",
       "      <td>-0.571369</td>\n",
       "      <td>-0.528830</td>\n",
       "      <td>-0.576704</td>\n",
       "      <td>-0.471381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Rock1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-1.447904</td>\n",
       "      <td>-0.417870</td>\n",
       "      <td>-0.446734</td>\n",
       "      <td>0.060785</td>\n",
       "      <td>0.021407</td>\n",
       "      <td>-0.208734</td>\n",
       "      <td>-0.374423</td>\n",
       "      <td>0.201287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324742</td>\n",
       "      <td>-0.086219</td>\n",
       "      <td>-0.488893</td>\n",
       "      <td>-0.470538</td>\n",
       "      <td>-0.599790</td>\n",
       "      <td>-1.067573</td>\n",
       "      <td>-0.525841</td>\n",
       "      <td>-0.482366</td>\n",
       "      <td>-0.528016</td>\n",
       "      <td>-0.421093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          names  Density  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0    Cardboard1      0.7   -1.222089   -0.731775   -0.753505   -0.737678   \n",
       "1    Cardboard1      0.7   -1.222889   -0.732071   -0.753856   -0.738047   \n",
       "2    Cardboard1      0.7   -1.223594   -0.732304   -0.754133   -0.738336   \n",
       "3    Cardboard1      0.7   -1.253176   -0.715537   -0.734651   -0.712910   \n",
       "4    Cardboard1      0.7   -1.286577   -0.697994   -0.714912   -0.681122   \n",
       "..          ...      ...         ...         ...         ...         ...   \n",
       "445       Rock1      3.2   -0.890006   -0.713101   -0.731134   -0.724364   \n",
       "446       Rock1      3.2   -0.885503   -0.712588   -0.730527   -0.723925   \n",
       "447       Rock1      3.2   -1.431018   -0.394084   -0.422334    0.090835   \n",
       "448       Rock1      3.2   -1.481978   -0.463948   -0.494377    0.003633   \n",
       "449       Rock1      3.2   -1.447904   -0.417870   -0.446734    0.060785   \n",
       "\n",
       "     spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0     -0.716104   -0.746348   -0.753370   -0.716866  ...    -0.717742   \n",
       "1     -0.716408   -0.746739   -0.753741   -0.717155  ...    -0.718031   \n",
       "2     -0.716645   -0.747047   -0.754034   -0.717382  ...    -0.718257   \n",
       "3     -0.695747   -0.722589   -0.732372   -0.696245  ...    -0.696845   \n",
       "4     -0.669507   -0.694998   -0.709313   -0.667894  ...    -0.667802   \n",
       "..          ...         ...         ...         ...  ...          ...   \n",
       "445   -0.704868   -0.728169   -0.732157   -0.707552  ...    -0.709225   \n",
       "446   -0.704497   -0.727629   -0.731567   -0.707226  ...    -0.708916   \n",
       "447    0.049914   -0.179356   -0.347860    0.228330  ...     0.348304   \n",
       "448   -0.032057   -0.266088   -0.426501    0.152261  ...     0.285322   \n",
       "449    0.021407   -0.208734   -0.374423    0.201287  ...     0.324742   \n",
       "\n",
       "     spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0      -0.725527    -0.771206    -0.781378    -0.881510    -1.188793   \n",
       "1      -0.725854    -0.771593    -0.781812    -0.882156    -1.190055   \n",
       "2      -0.726112    -0.771899    -0.782154    -0.882668    -1.191074   \n",
       "3      -0.704232    -0.750996    -0.758113    -0.850779    -1.152454   \n",
       "4      -0.677601    -0.730416    -0.733973    -0.822202    -1.129839   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "445    -0.712104    -0.745627    -0.754517    -0.833966    -1.030883   \n",
       "446    -0.711679    -0.744948    -0.753794    -0.832799    -1.027887   \n",
       "447    -0.056555    -0.464889    -0.444711    -0.573235    -1.041221   \n",
       "448    -0.143506    -0.535887    -0.521557    -0.652785    -1.120802   \n",
       "449    -0.086219    -0.488893    -0.470538    -0.599790    -1.067573   \n",
       "\n",
       "     spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0      -0.782123    -0.772902    -0.811379    -0.755607  \n",
       "1      -0.782506    -0.773298    -0.811857    -0.755978  \n",
       "2      -0.782807    -0.773610    -0.812235    -0.756272  \n",
       "3      -0.762656    -0.752706    -0.787926    -0.735098  \n",
       "4      -0.743140    -0.732515    -0.765458    -0.713138  \n",
       "..           ...          ...          ...          ...  \n",
       "445    -0.755103    -0.745954    -0.777340    -0.733455  \n",
       "446    -0.754397    -0.745252    -0.776474    -0.732846  \n",
       "447    -0.502578    -0.458578    -0.503277    -0.395475  \n",
       "448    -0.571369    -0.528830    -0.576704    -0.471381  \n",
       "449    -0.525841    -0.482366    -0.528016    -0.421093  \n",
       "\n",
       "[450 rows x 66 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [\"Cardboard1\", \"Ceramic1\", \"Glass1\", \"Plastic1\", \"Rock1\"]\n",
    "e_test = e_data[(e_data[\"names\"]==\"Cardboard1\" )|(e_data[\"names\"]==\"Ceramic1\" )|(e_data[\"names\"]==\"Glass1\") |(e_data[\"names\"]==\"Plastic1\") |(e_data[\"names\"]==\"Rock1\")].reset_index(drop=True)\n",
    "e_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ff2945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>Density</th>\n",
       "      <th>spectrum_0</th>\n",
       "      <th>spectrum_1</th>\n",
       "      <th>spectrum_2</th>\n",
       "      <th>spectrum_3</th>\n",
       "      <th>spectrum_4</th>\n",
       "      <th>spectrum_5</th>\n",
       "      <th>spectrum_6</th>\n",
       "      <th>spectrum_7</th>\n",
       "      <th>...</th>\n",
       "      <th>spectrum_54</th>\n",
       "      <th>spectrum_55</th>\n",
       "      <th>spectrum_56</th>\n",
       "      <th>spectrum_57</th>\n",
       "      <th>spectrum_58</th>\n",
       "      <th>spectrum_59</th>\n",
       "      <th>spectrum_60</th>\n",
       "      <th>spectrum_61</th>\n",
       "      <th>spectrum_62</th>\n",
       "      <th>spectrum_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.115912</td>\n",
       "      <td>-0.728643</td>\n",
       "      <td>-0.749686</td>\n",
       "      <td>-0.736475</td>\n",
       "      <td>-0.715072</td>\n",
       "      <td>-0.743912</td>\n",
       "      <td>-0.749965</td>\n",
       "      <td>-0.716269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717374</td>\n",
       "      <td>-0.724037</td>\n",
       "      <td>-0.766685</td>\n",
       "      <td>-0.776774</td>\n",
       "      <td>-0.872160</td>\n",
       "      <td>-1.148111</td>\n",
       "      <td>-0.777188</td>\n",
       "      <td>-0.768012</td>\n",
       "      <td>-0.804966</td>\n",
       "      <td>-0.751950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.107021</td>\n",
       "      <td>-0.728564</td>\n",
       "      <td>-0.749586</td>\n",
       "      <td>-0.736574</td>\n",
       "      <td>-0.715155</td>\n",
       "      <td>-0.743936</td>\n",
       "      <td>-0.749900</td>\n",
       "      <td>-0.716370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717494</td>\n",
       "      <td>-0.724098</td>\n",
       "      <td>-0.766545</td>\n",
       "      <td>-0.776651</td>\n",
       "      <td>-0.871762</td>\n",
       "      <td>-1.145388</td>\n",
       "      <td>-0.777012</td>\n",
       "      <td>-0.767850</td>\n",
       "      <td>-0.804733</td>\n",
       "      <td>-0.751868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.107631</td>\n",
       "      <td>-0.728518</td>\n",
       "      <td>-0.749531</td>\n",
       "      <td>-0.736507</td>\n",
       "      <td>-0.715099</td>\n",
       "      <td>-0.743868</td>\n",
       "      <td>-0.749841</td>\n",
       "      <td>-0.716316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717439</td>\n",
       "      <td>-0.724039</td>\n",
       "      <td>-0.766487</td>\n",
       "      <td>-0.776585</td>\n",
       "      <td>-0.871679</td>\n",
       "      <td>-1.145386</td>\n",
       "      <td>-0.776958</td>\n",
       "      <td>-0.767793</td>\n",
       "      <td>-0.804666</td>\n",
       "      <td>-0.751810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.113432</td>\n",
       "      <td>-0.728751</td>\n",
       "      <td>-0.749813</td>\n",
       "      <td>-0.736657</td>\n",
       "      <td>-0.715224</td>\n",
       "      <td>-0.744087</td>\n",
       "      <td>-0.750108</td>\n",
       "      <td>-0.716417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717527</td>\n",
       "      <td>-0.724194</td>\n",
       "      <td>-0.766816</td>\n",
       "      <td>-0.776929</td>\n",
       "      <td>-0.872326</td>\n",
       "      <td>-1.147828</td>\n",
       "      <td>-0.777308</td>\n",
       "      <td>-0.768141</td>\n",
       "      <td>-0.805111</td>\n",
       "      <td>-0.752090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardboard</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-1.110347</td>\n",
       "      <td>-0.728303</td>\n",
       "      <td>-0.749277</td>\n",
       "      <td>-0.736200</td>\n",
       "      <td>-0.714843</td>\n",
       "      <td>-0.743558</td>\n",
       "      <td>-0.749570</td>\n",
       "      <td>-0.716072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717189</td>\n",
       "      <td>-0.723770</td>\n",
       "      <td>-0.766220</td>\n",
       "      <td>-0.776280</td>\n",
       "      <td>-0.871300</td>\n",
       "      <td>-1.145354</td>\n",
       "      <td>-0.776703</td>\n",
       "      <td>-0.767523</td>\n",
       "      <td>-0.804350</td>\n",
       "      <td>-0.751541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.279136</td>\n",
       "      <td>-0.699879</td>\n",
       "      <td>-0.715272</td>\n",
       "      <td>-0.721783</td>\n",
       "      <td>-0.702883</td>\n",
       "      <td>-0.720305</td>\n",
       "      <td>-0.718238</td>\n",
       "      <td>-0.706856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709624</td>\n",
       "      <td>-0.708081</td>\n",
       "      <td>-0.725917</td>\n",
       "      <td>-0.734978</td>\n",
       "      <td>-0.789022</td>\n",
       "      <td>-0.810365</td>\n",
       "      <td>-0.732375</td>\n",
       "      <td>-0.724863</td>\n",
       "      <td>-0.749657</td>\n",
       "      <td>-0.718353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.332007</td>\n",
       "      <td>-0.696639</td>\n",
       "      <td>-0.711489</td>\n",
       "      <td>-0.718338</td>\n",
       "      <td>-0.699946</td>\n",
       "      <td>-0.716421</td>\n",
       "      <td>-0.714437</td>\n",
       "      <td>-0.704214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707049</td>\n",
       "      <td>-0.704845</td>\n",
       "      <td>-0.721901</td>\n",
       "      <td>-0.730574</td>\n",
       "      <td>-0.783617</td>\n",
       "      <td>-0.812966</td>\n",
       "      <td>-0.728413</td>\n",
       "      <td>-0.720826</td>\n",
       "      <td>-0.744929</td>\n",
       "      <td>-0.714494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.114494</td>\n",
       "      <td>-0.151241</td>\n",
       "      <td>-0.154344</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>0.164377</td>\n",
       "      <td>0.050968</td>\n",
       "      <td>-0.078866</td>\n",
       "      <td>0.267358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302501</td>\n",
       "      <td>0.125445</td>\n",
       "      <td>-0.181908</td>\n",
       "      <td>-0.134064</td>\n",
       "      <td>-0.216604</td>\n",
       "      <td>-0.629094</td>\n",
       "      <td>-0.212757</td>\n",
       "      <td>-0.191827</td>\n",
       "      <td>-0.206766</td>\n",
       "      <td>-0.117266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.310687</td>\n",
       "      <td>-0.306069</td>\n",
       "      <td>-0.322686</td>\n",
       "      <td>0.125911</td>\n",
       "      <td>0.071682</td>\n",
       "      <td>-0.097453</td>\n",
       "      <td>-0.247510</td>\n",
       "      <td>0.210284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281922</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>-0.358556</td>\n",
       "      <td>-0.325485</td>\n",
       "      <td>-0.435267</td>\n",
       "      <td>-0.881566</td>\n",
       "      <td>-0.393971</td>\n",
       "      <td>-0.359431</td>\n",
       "      <td>-0.391404</td>\n",
       "      <td>-0.290328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Wood</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1.383952</td>\n",
       "      <td>-0.373431</td>\n",
       "      <td>-0.395249</td>\n",
       "      <td>0.070616</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>-0.168501</td>\n",
       "      <td>-0.322609</td>\n",
       "      <td>0.175854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266022</td>\n",
       "      <td>-0.061889</td>\n",
       "      <td>-0.433370</td>\n",
       "      <td>-0.407333</td>\n",
       "      <td>-0.526199</td>\n",
       "      <td>-0.982008</td>\n",
       "      <td>-0.469225</td>\n",
       "      <td>-0.431036</td>\n",
       "      <td>-0.469437</td>\n",
       "      <td>-0.366072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         names  Density  spectrum_0  spectrum_1  spectrum_2  spectrum_3  \\\n",
       "0    Cardboard      0.7   -1.115912   -0.728643   -0.749686   -0.736475   \n",
       "1    Cardboard      0.7   -1.107021   -0.728564   -0.749586   -0.736574   \n",
       "2    Cardboard      0.7   -1.107631   -0.728518   -0.749531   -0.736507   \n",
       "3    Cardboard      0.7   -1.113432   -0.728751   -0.749813   -0.736657   \n",
       "4    Cardboard      0.7   -1.110347   -0.728303   -0.749277   -0.736200   \n",
       "..         ...      ...         ...         ...         ...         ...   \n",
       "805       Wood      0.8   -0.279136   -0.699879   -0.715272   -0.721783   \n",
       "806       Wood      0.8   -0.332007   -0.696639   -0.711489   -0.718338   \n",
       "807       Wood      0.8   -1.114494   -0.151241   -0.154344    0.229621   \n",
       "808       Wood      0.8   -1.310687   -0.306069   -0.322686    0.125911   \n",
       "809       Wood      0.8   -1.383952   -0.373431   -0.395249    0.070616   \n",
       "\n",
       "     spectrum_4  spectrum_5  spectrum_6  spectrum_7  ...  spectrum_54  \\\n",
       "0     -0.715072   -0.743912   -0.749965   -0.716269  ...    -0.717374   \n",
       "1     -0.715155   -0.743936   -0.749900   -0.716370  ...    -0.717494   \n",
       "2     -0.715099   -0.743868   -0.749841   -0.716316  ...    -0.717439   \n",
       "3     -0.715224   -0.744087   -0.750108   -0.716417  ...    -0.717527   \n",
       "4     -0.714843   -0.743558   -0.749570   -0.716072  ...    -0.717189   \n",
       "..          ...         ...         ...         ...  ...          ...   \n",
       "805   -0.702883   -0.720305   -0.718238   -0.706856  ...    -0.709624   \n",
       "806   -0.699946   -0.716421   -0.714437   -0.704214  ...    -0.707049   \n",
       "807    0.164377    0.050968   -0.078866    0.267358  ...     0.302501   \n",
       "808    0.071682   -0.097453   -0.247510    0.210284  ...     0.281922   \n",
       "809    0.022354   -0.168501   -0.322609    0.175854  ...     0.266022   \n",
       "\n",
       "     spectrum_55  spectrum_56  spectrum_57  spectrum_58  spectrum_59  \\\n",
       "0      -0.724037    -0.766685    -0.776774    -0.872160    -1.148111   \n",
       "1      -0.724098    -0.766545    -0.776651    -0.871762    -1.145388   \n",
       "2      -0.724039    -0.766487    -0.776585    -0.871679    -1.145386   \n",
       "3      -0.724194    -0.766816    -0.776929    -0.872326    -1.147828   \n",
       "4      -0.723770    -0.766220    -0.776280    -0.871300    -1.145354   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "805    -0.708081    -0.725917    -0.734978    -0.789022    -0.810365   \n",
       "806    -0.704845    -0.721901    -0.730574    -0.783617    -0.812966   \n",
       "807     0.125445    -0.181908    -0.134064    -0.216604    -0.629094   \n",
       "808     0.000922    -0.358556    -0.325485    -0.435267    -0.881566   \n",
       "809    -0.061889    -0.433370    -0.407333    -0.526199    -0.982008   \n",
       "\n",
       "     spectrum_60  spectrum_61  spectrum_62  spectrum_63  \n",
       "0      -0.777188    -0.768012    -0.804966    -0.751950  \n",
       "1      -0.777012    -0.767850    -0.804733    -0.751868  \n",
       "2      -0.776958    -0.767793    -0.804666    -0.751810  \n",
       "3      -0.777308    -0.768141    -0.805111    -0.752090  \n",
       "4      -0.776703    -0.767523    -0.804350    -0.751541  \n",
       "..           ...          ...          ...          ...  \n",
       "805    -0.732375    -0.724863    -0.749657    -0.718353  \n",
       "806    -0.728413    -0.720826    -0.744929    -0.714494  \n",
       "807    -0.212757    -0.191827    -0.206766    -0.117266  \n",
       "808    -0.393971    -0.359431    -0.391404    -0.290328  \n",
       "809    -0.469225    -0.431036    -0.469437    -0.366072  \n",
       "\n",
       "[810 rows x 66 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train = e_data[(e_data[\"names\"]!=\"Cardboard1\" )&(e_data[\"names\"]!=\"Ceramic1\" )&(e_data[\"names\"]!=\"Glass1\")&(e_data[\"names\"]!=\"Plastic1\") &(e_data[\"names\"]!=\"Rock1\")].reset_index(drop=True)\n",
    "e_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fca8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_test_name = e_test[\"names\"].to_numpy()\n",
    "e_test = e_test.drop(columns = [\"names\"])\n",
    "e_train = e_train.drop(columns = [\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb9e429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad2f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_test_y = e_test[\"Density\"]\n",
    "e_test_x = e_test.drop(columns = [\"Density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c13ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train_y = e_train[\"Density\"]\n",
    "e_train_x = e_train.drop(columns = [\"Density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f85fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra, x_val, y_tra, y_val = train_test_split(e_train_x, e_train_y, test_size=0.15, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61fbd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_tra.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29dd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n",
    "# Note that, in this example, the we prior distribution is not trainable,\n",
    "# as we fix its parameters.\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.DistributionLambda(\n",
    "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return prior_model\n",
    "\n",
    "\n",
    "# Define variational posterior weight distribution as multivariate Gaussian.\n",
    "# Note that the learnable parameters for this distribution are the means,\n",
    "# variances, and covariances.\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.VariableLayer(\n",
    "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "            ),\n",
    "            tfp.layers.MultivariateNormalTriL(n),\n",
    "        ]\n",
    "    )\n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 10000\n",
    "learning_rate = 0.0001\n",
    "encoding_dim = 64\n",
    "\n",
    "def bnn():\n",
    "    model = Sequential()\n",
    "    model.add(tfp.layers.DenseVariational(\n",
    "            units=encoding_dim,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1 / train_size,\n",
    "            activation=\"sigmoid\",\n",
    "        ))\n",
    "\n",
    "    model.add(tfp.layers.DenseVariational(\n",
    "            units=256,\n",
    "            make_prior_fn=prior,\n",
    "            make_posterior_fn=posterior,\n",
    "            kl_weight=1 / train_size,\n",
    "            activation=\"sigmoid\",\n",
    "        ))\n",
    "\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e680f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b359c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 10:33:19.763839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 10:33:20.306755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11290 MB memory:  -> device: 0, name: NVIDIA TITAN X (Pascal), pci bus id: 0000:05:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = bnn()\n",
    "loss = keras.losses.MeanSquaredError()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "    loss=loss,\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e92893e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "WARNING:tensorflow:From /home/rilab/anaconda3/envs/huiEnv/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 10:33:22.654559: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f372c04bf20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-03 10:33:22.654595: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA TITAN X (Pascal), Compute Capability 6.1\n",
      "2023-03-03 10:33:22.658389: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-03 10:33:22.758319: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 46s 126ms/step - loss: 19.0383 - root_mean_squared_error: 3.9631 - val_loss: 21.9966 - val_root_mean_squared_error: 4.3108\n",
      "Epoch 2/10000\n",
      "344/344 [==============================] - 41s 120ms/step - loss: 22.5556 - root_mean_squared_error: 4.3614 - val_loss: 19.6156 - val_root_mean_squared_error: 4.0027\n",
      "Epoch 3/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 20.0254 - root_mean_squared_error: 4.0405 - val_loss: 19.6461 - val_root_mean_squared_error: 3.9811\n",
      "Epoch 4/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 15.6865 - root_mean_squared_error: 3.4437 - val_loss: 20.2542 - val_root_mean_squared_error: 4.0321\n",
      "Epoch 5/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 16.6371 - root_mean_squared_error: 3.5593 - val_loss: 16.1764 - val_root_mean_squared_error: 3.4841\n",
      "Epoch 6/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.5351 - root_mean_squared_error: 3.2285 - val_loss: 13.8830 - val_root_mean_squared_error: 3.1087\n",
      "Epoch 7/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.4068 - root_mean_squared_error: 3.0285 - val_loss: 11.8689 - val_root_mean_squared_error: 2.7684\n",
      "Epoch 8/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.0127 - root_mean_squared_error: 2.9436 - val_loss: 12.2480 - val_root_mean_squared_error: 2.7961\n",
      "Epoch 9/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.2843 - root_mean_squared_error: 2.8005 - val_loss: 11.6415 - val_root_mean_squared_error: 2.6829\n",
      "Epoch 10/10000\n",
      "344/344 [==============================] - 43s 125ms/step - loss: 11.9511 - root_mean_squared_error: 2.7284 - val_loss: 10.3399 - val_root_mean_squared_error: 2.4200\n",
      "Epoch 11/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.6552 - root_mean_squared_error: 2.6515 - val_loss: 10.3400 - val_root_mean_squared_error: 2.3886\n",
      "Epoch 12/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.9223 - root_mean_squared_error: 2.6877 - val_loss: 11.1909 - val_root_mean_squared_error: 2.5413\n",
      "Epoch 13/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.8118 - root_mean_squared_error: 2.6563 - val_loss: 10.7999 - val_root_mean_squared_error: 2.4542\n",
      "Epoch 14/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.5611 - root_mean_squared_error: 2.5970 - val_loss: 10.0130 - val_root_mean_squared_error: 2.2904\n",
      "Epoch 15/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.1569 - root_mean_squared_error: 2.5041 - val_loss: 10.7897 - val_root_mean_squared_error: 2.4219\n",
      "Epoch 16/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.4016 - root_mean_squared_error: 2.5409 - val_loss: 11.1449 - val_root_mean_squared_error: 2.4815\n",
      "Epoch 17/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.3509 - root_mean_squared_error: 2.5169 - val_loss: 10.4164 - val_root_mean_squared_error: 2.3305\n",
      "Epoch 18/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.3142 - root_mean_squared_error: 2.5029 - val_loss: 10.1479 - val_root_mean_squared_error: 2.2703\n",
      "Epoch 19/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.1665 - root_mean_squared_error: 2.4683 - val_loss: 10.3897 - val_root_mean_squared_error: 2.2941\n",
      "Epoch 20/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.4044 - root_mean_squared_error: 2.4987 - val_loss: 10.3715 - val_root_mean_squared_error: 2.3058\n",
      "Epoch 21/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.1438 - root_mean_squared_error: 2.4452 - val_loss: 10.3970 - val_root_mean_squared_error: 2.2849\n",
      "Epoch 22/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.2033 - root_mean_squared_error: 2.4406 - val_loss: 10.6694 - val_root_mean_squared_error: 2.3441\n",
      "Epoch 23/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.2471 - root_mean_squared_error: 2.4459 - val_loss: 10.4328 - val_root_mean_squared_error: 2.2553\n",
      "Epoch 24/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.2148 - root_mean_squared_error: 2.4320 - val_loss: 10.3493 - val_root_mean_squared_error: 2.2492\n",
      "Epoch 25/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.2146 - root_mean_squared_error: 2.4251 - val_loss: 10.5702 - val_root_mean_squared_error: 2.2786\n",
      "Epoch 26/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.4462 - root_mean_squared_error: 2.4635 - val_loss: 10.8984 - val_root_mean_squared_error: 2.3353\n",
      "Epoch 27/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.5527 - root_mean_squared_error: 2.4779 - val_loss: 10.6171 - val_root_mean_squared_error: 2.2706\n",
      "Epoch 28/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.4868 - root_mean_squared_error: 2.4581 - val_loss: 10.6403 - val_root_mean_squared_error: 2.2770\n",
      "Epoch 29/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.6688 - root_mean_squared_error: 2.4831 - val_loss: 10.5661 - val_root_mean_squared_error: 2.2437\n",
      "Epoch 30/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.6128 - root_mean_squared_error: 2.4694 - val_loss: 10.6230 - val_root_mean_squared_error: 2.2572\n",
      "Epoch 31/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.5134 - root_mean_squared_error: 2.4332 - val_loss: 10.6631 - val_root_mean_squared_error: 2.2483\n",
      "Epoch 32/10000\n",
      "344/344 [==============================] - 44s 128ms/step - loss: 11.6953 - root_mean_squared_error: 2.4691 - val_loss: 11.1633 - val_root_mean_squared_error: 2.3481\n",
      "Epoch 33/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.6723 - root_mean_squared_error: 2.4494 - val_loss: 10.7004 - val_root_mean_squared_error: 2.2495\n",
      "Epoch 34/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.8853 - root_mean_squared_error: 2.4908 - val_loss: 11.1142 - val_root_mean_squared_error: 2.3254\n",
      "Epoch 35/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.7946 - root_mean_squared_error: 2.4707 - val_loss: 11.0390 - val_root_mean_squared_error: 2.2802\n",
      "Epoch 36/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.8118 - root_mean_squared_error: 2.4532 - val_loss: 11.1391 - val_root_mean_squared_error: 2.3162\n",
      "Epoch 37/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.7948 - root_mean_squared_error: 2.4500 - val_loss: 11.1709 - val_root_mean_squared_error: 2.3121\n",
      "Epoch 38/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.8917 - root_mean_squared_error: 2.4617 - val_loss: 11.4170 - val_root_mean_squared_error: 2.3654\n",
      "Epoch 39/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 11.9717 - root_mean_squared_error: 2.4689 - val_loss: 10.8091 - val_root_mean_squared_error: 2.2126\n",
      "Epoch 40/10000\n",
      "344/344 [==============================] - 42s 123ms/step - loss: 12.0032 - root_mean_squared_error: 2.4675 - val_loss: 11.4984 - val_root_mean_squared_error: 2.3629\n",
      "Epoch 41/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.1365 - root_mean_squared_error: 2.4797 - val_loss: 10.5468 - val_root_mean_squared_error: 2.1448\n",
      "Epoch 42/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.1665 - root_mean_squared_error: 2.4825 - val_loss: 11.1529 - val_root_mean_squared_error: 2.2689\n",
      "Epoch 43/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.0623 - root_mean_squared_error: 2.4566 - val_loss: 11.0705 - val_root_mean_squared_error: 2.2479\n",
      "Epoch 44/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.3168 - root_mean_squared_error: 2.4996 - val_loss: 11.2739 - val_root_mean_squared_error: 2.2840\n",
      "Epoch 45/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.1655 - root_mean_squared_error: 2.4627 - val_loss: 11.7469 - val_root_mean_squared_error: 2.3622\n",
      "Epoch 46/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 12.2393 - root_mean_squared_error: 2.4763 - val_loss: 11.2474 - val_root_mean_squared_error: 2.2731\n",
      "Epoch 47/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.2368 - root_mean_squared_error: 2.4659 - val_loss: 11.4571 - val_root_mean_squared_error: 2.2864\n",
      "Epoch 48/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.2724 - root_mean_squared_error: 2.4628 - val_loss: 11.4582 - val_root_mean_squared_error: 2.2736\n",
      "Epoch 49/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.3242 - root_mean_squared_error: 2.4637 - val_loss: 11.1585 - val_root_mean_squared_error: 2.2236\n",
      "Epoch 50/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.3709 - root_mean_squared_error: 2.4623 - val_loss: 11.8804 - val_root_mean_squared_error: 2.3524\n",
      "Epoch 51/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.2925 - root_mean_squared_error: 2.4463 - val_loss: 11.6306 - val_root_mean_squared_error: 2.2879\n",
      "Epoch 52/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.5154 - root_mean_squared_error: 2.4837 - val_loss: 11.8120 - val_root_mean_squared_error: 2.3132\n",
      "Epoch 53/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.3690 - root_mean_squared_error: 2.4438 - val_loss: 11.5676 - val_root_mean_squared_error: 2.2764\n",
      "Epoch 54/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.6576 - root_mean_squared_error: 2.4958 - val_loss: 11.7213 - val_root_mean_squared_error: 2.2860\n",
      "Epoch 55/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.5376 - root_mean_squared_error: 2.4670 - val_loss: 11.5583 - val_root_mean_squared_error: 2.2536\n",
      "Epoch 56/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.5692 - root_mean_squared_error: 2.4634 - val_loss: 11.7854 - val_root_mean_squared_error: 2.3113\n",
      "Epoch 57/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.4842 - root_mean_squared_error: 2.4479 - val_loss: 11.3479 - val_root_mean_squared_error: 2.2215\n",
      "Epoch 58/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.6638 - root_mean_squared_error: 2.4701 - val_loss: 11.8264 - val_root_mean_squared_error: 2.2857\n",
      "Epoch 59/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.7287 - root_mean_squared_error: 2.4727 - val_loss: 11.8467 - val_root_mean_squared_error: 2.2689\n",
      "Epoch 60/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.5928 - root_mean_squared_error: 2.4482 - val_loss: 12.3683 - val_root_mean_squared_error: 2.3977\n",
      "Epoch 61/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.7093 - root_mean_squared_error: 2.4631 - val_loss: 11.9212 - val_root_mean_squared_error: 2.2986\n",
      "Epoch 62/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.6564 - root_mean_squared_error: 2.4498 - val_loss: 12.0982 - val_root_mean_squared_error: 2.3167\n",
      "Epoch 63/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.6433 - root_mean_squared_error: 2.4374 - val_loss: 11.6460 - val_root_mean_squared_error: 2.2314\n",
      "Epoch 64/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.8025 - root_mean_squared_error: 2.4647 - val_loss: 11.8350 - val_root_mean_squared_error: 2.2700\n",
      "Epoch 65/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.7631 - root_mean_squared_error: 2.4537 - val_loss: 11.8437 - val_root_mean_squared_error: 2.2492\n",
      "Epoch 66/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.9039 - root_mean_squared_error: 2.4610 - val_loss: 12.2054 - val_root_mean_squared_error: 2.2938\n",
      "Epoch 67/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.0686 - root_mean_squared_error: 2.5002 - val_loss: 11.6949 - val_root_mean_squared_error: 2.2147\n",
      "Epoch 68/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.9702 - root_mean_squared_error: 2.4720 - val_loss: 12.1984 - val_root_mean_squared_error: 2.3246\n",
      "Epoch 69/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.0618 - root_mean_squared_error: 2.4845 - val_loss: 12.1029 - val_root_mean_squared_error: 2.2911\n",
      "Epoch 70/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.9433 - root_mean_squared_error: 2.4575 - val_loss: 12.3986 - val_root_mean_squared_error: 2.3426\n",
      "Epoch 71/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.1640 - root_mean_squared_error: 2.4985 - val_loss: 12.4268 - val_root_mean_squared_error: 2.3242\n",
      "Epoch 72/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 12.9654 - root_mean_squared_error: 2.4540 - val_loss: 12.6956 - val_root_mean_squared_error: 2.4017\n",
      "Epoch 73/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.0954 - root_mean_squared_error: 2.4702 - val_loss: 12.4332 - val_root_mean_squared_error: 2.3204\n",
      "Epoch 74/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.3402 - root_mean_squared_error: 2.5161 - val_loss: 11.9337 - val_root_mean_squared_error: 2.2314\n",
      "Epoch 75/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.2312 - root_mean_squared_error: 2.4912 - val_loss: 12.4917 - val_root_mean_squared_error: 2.3332\n",
      "Epoch 76/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.2300 - root_mean_squared_error: 2.4896 - val_loss: 12.3250 - val_root_mean_squared_error: 2.2842\n",
      "Epoch 77/10000\n",
      "344/344 [==============================] - 43s 125ms/step - loss: 13.2044 - root_mean_squared_error: 2.4740 - val_loss: 12.1934 - val_root_mean_squared_error: 2.2430\n",
      "Epoch 78/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.2150 - root_mean_squared_error: 2.4704 - val_loss: 12.4319 - val_root_mean_squared_error: 2.3106\n",
      "Epoch 79/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.1817 - root_mean_squared_error: 2.4627 - val_loss: 12.6753 - val_root_mean_squared_error: 2.3338\n",
      "Epoch 80/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.1243 - root_mean_squared_error: 2.4448 - val_loss: 12.5020 - val_root_mean_squared_error: 2.2990\n",
      "Epoch 81/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.3804 - root_mean_squared_error: 2.4842 - val_loss: 12.3403 - val_root_mean_squared_error: 2.2634\n",
      "Epoch 82/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.2968 - root_mean_squared_error: 2.4707 - val_loss: 11.9722 - val_root_mean_squared_error: 2.2103\n",
      "Epoch 83/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.2100 - root_mean_squared_error: 2.4421 - val_loss: 12.4454 - val_root_mean_squared_error: 2.2735\n",
      "Epoch 84/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.4160 - root_mean_squared_error: 2.4836 - val_loss: 12.4146 - val_root_mean_squared_error: 2.2626\n",
      "Epoch 85/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.1621 - root_mean_squared_error: 2.4346 - val_loss: 12.4438 - val_root_mean_squared_error: 2.2841\n",
      "Epoch 86/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.3554 - root_mean_squared_error: 2.4505 - val_loss: 12.2433 - val_root_mean_squared_error: 2.2285\n",
      "Epoch 87/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.2642 - root_mean_squared_error: 2.4447 - val_loss: 12.7226 - val_root_mean_squared_error: 2.3103\n",
      "Epoch 88/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.4733 - root_mean_squared_error: 2.4807 - val_loss: 12.6285 - val_root_mean_squared_error: 2.2900\n",
      "Epoch 89/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.5679 - root_mean_squared_error: 2.4949 - val_loss: 12.9872 - val_root_mean_squared_error: 2.3389\n",
      "Epoch 90/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.3445 - root_mean_squared_error: 2.4485 - val_loss: 12.2410 - val_root_mean_squared_error: 2.1999\n",
      "Epoch 91/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6165 - root_mean_squared_error: 2.4909 - val_loss: 13.0385 - val_root_mean_squared_error: 2.3639\n",
      "Epoch 92/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6384 - root_mean_squared_error: 2.4962 - val_loss: 13.0423 - val_root_mean_squared_error: 2.3790\n",
      "Epoch 93/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6356 - root_mean_squared_error: 2.4945 - val_loss: 12.5141 - val_root_mean_squared_error: 2.2765\n",
      "Epoch 94/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.5798 - root_mean_squared_error: 2.4802 - val_loss: 12.3967 - val_root_mean_squared_error: 2.2304\n",
      "Epoch 95/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.4380 - root_mean_squared_error: 2.4503 - val_loss: 12.6246 - val_root_mean_squared_error: 2.2789\n",
      "Epoch 96/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6241 - root_mean_squared_error: 2.4838 - val_loss: 12.6917 - val_root_mean_squared_error: 2.2774\n",
      "Epoch 97/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.3582 - root_mean_squared_error: 2.4178 - val_loss: 12.8651 - val_root_mean_squared_error: 2.3043\n",
      "Epoch 98/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.4633 - root_mean_squared_error: 2.4381 - val_loss: 12.4858 - val_root_mean_squared_error: 2.2396\n",
      "Epoch 99/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.5825 - root_mean_squared_error: 2.4621 - val_loss: 13.2485 - val_root_mean_squared_error: 2.3835\n",
      "Epoch 100/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8624 - root_mean_squared_error: 2.5052 - val_loss: 13.1774 - val_root_mean_squared_error: 2.3567\n",
      "Epoch 101/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6304 - root_mean_squared_error: 2.4567 - val_loss: 13.0673 - val_root_mean_squared_error: 2.3485\n",
      "Epoch 102/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8444 - root_mean_squared_error: 2.4999 - val_loss: 12.9815 - val_root_mean_squared_error: 2.3066\n",
      "Epoch 103/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6709 - root_mean_squared_error: 2.4597 - val_loss: 12.7870 - val_root_mean_squared_error: 2.2717\n",
      "Epoch 104/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9040 - root_mean_squared_error: 2.5005 - val_loss: 12.6358 - val_root_mean_squared_error: 2.2599\n",
      "Epoch 105/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7839 - root_mean_squared_error: 2.4751 - val_loss: 12.4565 - val_root_mean_squared_error: 2.2040\n",
      "Epoch 106/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8984 - root_mean_squared_error: 2.4957 - val_loss: 13.0782 - val_root_mean_squared_error: 2.3391\n",
      "Epoch 107/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7001 - root_mean_squared_error: 2.4518 - val_loss: 12.6700 - val_root_mean_squared_error: 2.2567\n",
      "Epoch 108/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8441 - root_mean_squared_error: 2.4836 - val_loss: 12.9609 - val_root_mean_squared_error: 2.2976\n",
      "Epoch 109/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8123 - root_mean_squared_error: 2.4723 - val_loss: 13.0374 - val_root_mean_squared_error: 2.3122\n",
      "Epoch 110/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7898 - root_mean_squared_error: 2.4664 - val_loss: 12.5885 - val_root_mean_squared_error: 2.2106\n",
      "Epoch 111/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.6116 - root_mean_squared_error: 2.4321 - val_loss: 13.0107 - val_root_mean_squared_error: 2.2890\n",
      "Epoch 112/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7451 - root_mean_squared_error: 2.4580 - val_loss: 12.5593 - val_root_mean_squared_error: 2.1896\n",
      "Epoch 113/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0029 - root_mean_squared_error: 2.4983 - val_loss: 12.8279 - val_root_mean_squared_error: 2.2793\n",
      "Epoch 114/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9412 - root_mean_squared_error: 2.4780 - val_loss: 12.8924 - val_root_mean_squared_error: 2.2798\n",
      "Epoch 115/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7074 - root_mean_squared_error: 2.4423 - val_loss: 13.2326 - val_root_mean_squared_error: 2.3423\n",
      "Epoch 116/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7462 - root_mean_squared_error: 2.4374 - val_loss: 12.8598 - val_root_mean_squared_error: 2.2523\n",
      "Epoch 117/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9452 - root_mean_squared_error: 2.4736 - val_loss: 13.1383 - val_root_mean_squared_error: 2.3238\n",
      "Epoch 118/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8899 - root_mean_squared_error: 2.4572 - val_loss: 12.8503 - val_root_mean_squared_error: 2.2484\n",
      "Epoch 119/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8939 - root_mean_squared_error: 2.4666 - val_loss: 12.7424 - val_root_mean_squared_error: 2.2070\n",
      "Epoch 120/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9560 - root_mean_squared_error: 2.4684 - val_loss: 13.0551 - val_root_mean_squared_error: 2.2764\n",
      "Epoch 121/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8370 - root_mean_squared_error: 2.4413 - val_loss: 12.9689 - val_root_mean_squared_error: 2.2767\n",
      "Epoch 122/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9523 - root_mean_squared_error: 2.4720 - val_loss: 13.2113 - val_root_mean_squared_error: 2.3232\n",
      "Epoch 123/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9811 - root_mean_squared_error: 2.4705 - val_loss: 13.1097 - val_root_mean_squared_error: 2.2985\n",
      "Epoch 124/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9900 - root_mean_squared_error: 2.4631 - val_loss: 13.2596 - val_root_mean_squared_error: 2.3158\n",
      "Epoch 125/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.7648 - root_mean_squared_error: 2.4223 - val_loss: 13.1703 - val_root_mean_squared_error: 2.3115\n",
      "Epoch 126/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0038 - root_mean_squared_error: 2.4645 - val_loss: 13.3700 - val_root_mean_squared_error: 2.3411\n",
      "Epoch 127/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.8866 - root_mean_squared_error: 2.4399 - val_loss: 13.3764 - val_root_mean_squared_error: 2.3216\n",
      "Epoch 128/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1358 - root_mean_squared_error: 2.4893 - val_loss: 13.1076 - val_root_mean_squared_error: 2.2631\n",
      "Epoch 129/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0136 - root_mean_squared_error: 2.4683 - val_loss: 13.5750 - val_root_mean_squared_error: 2.3351\n",
      "Epoch 130/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1859 - root_mean_squared_error: 2.4897 - val_loss: 13.1354 - val_root_mean_squared_error: 2.2801\n",
      "Epoch 131/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0664 - root_mean_squared_error: 2.4739 - val_loss: 13.0803 - val_root_mean_squared_error: 2.2636\n",
      "Epoch 132/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0359 - root_mean_squared_error: 2.4641 - val_loss: 13.1609 - val_root_mean_squared_error: 2.2853\n",
      "Epoch 133/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1461 - root_mean_squared_error: 2.4867 - val_loss: 12.9818 - val_root_mean_squared_error: 2.2328\n",
      "Epoch 134/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9397 - root_mean_squared_error: 2.4388 - val_loss: 13.5282 - val_root_mean_squared_error: 2.3426\n",
      "Epoch 135/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9230 - root_mean_squared_error: 2.4332 - val_loss: 13.3102 - val_root_mean_squared_error: 2.2944\n",
      "Epoch 136/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9463 - root_mean_squared_error: 2.4415 - val_loss: 13.2370 - val_root_mean_squared_error: 2.2751\n",
      "Epoch 137/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0485 - root_mean_squared_error: 2.4528 - val_loss: 13.3166 - val_root_mean_squared_error: 2.3038\n",
      "Epoch 138/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9503 - root_mean_squared_error: 2.4429 - val_loss: 13.2388 - val_root_mean_squared_error: 2.2741\n",
      "Epoch 139/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0331 - root_mean_squared_error: 2.4514 - val_loss: 13.2545 - val_root_mean_squared_error: 2.2918\n",
      "Epoch 140/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1554 - root_mean_squared_error: 2.4769 - val_loss: 13.1416 - val_root_mean_squared_error: 2.2552\n",
      "Epoch 141/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0526 - root_mean_squared_error: 2.4513 - val_loss: 13.3800 - val_root_mean_squared_error: 2.3019\n",
      "Epoch 142/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0622 - root_mean_squared_error: 2.4478 - val_loss: 13.1886 - val_root_mean_squared_error: 2.2614\n",
      "Epoch 143/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1637 - root_mean_squared_error: 2.4660 - val_loss: 13.1956 - val_root_mean_squared_error: 2.2777\n",
      "Epoch 144/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0280 - root_mean_squared_error: 2.4438 - val_loss: 13.2924 - val_root_mean_squared_error: 2.2949\n",
      "Epoch 145/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0588 - root_mean_squared_error: 2.4540 - val_loss: 13.6638 - val_root_mean_squared_error: 2.3694\n",
      "Epoch 146/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1093 - root_mean_squared_error: 2.4573 - val_loss: 13.1942 - val_root_mean_squared_error: 2.2603\n",
      "Epoch 147/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0703 - root_mean_squared_error: 2.4491 - val_loss: 13.2429 - val_root_mean_squared_error: 2.2557\n",
      "Epoch 148/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1227 - root_mean_squared_error: 2.4525 - val_loss: 13.6086 - val_root_mean_squared_error: 2.3445\n",
      "Epoch 149/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0316 - root_mean_squared_error: 2.4410 - val_loss: 13.3419 - val_root_mean_squared_error: 2.2816\n",
      "Epoch 150/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0968 - root_mean_squared_error: 2.4423 - val_loss: 13.0401 - val_root_mean_squared_error: 2.2404\n",
      "Epoch 151/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9854 - root_mean_squared_error: 2.4225 - val_loss: 13.3013 - val_root_mean_squared_error: 2.2923\n",
      "Epoch 152/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 13.9992 - root_mean_squared_error: 2.4250 - val_loss: 13.3774 - val_root_mean_squared_error: 2.3028\n",
      "Epoch 153/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1371 - root_mean_squared_error: 2.4536 - val_loss: 13.3218 - val_root_mean_squared_error: 2.2793\n",
      "Epoch 154/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0792 - root_mean_squared_error: 2.4347 - val_loss: 13.4874 - val_root_mean_squared_error: 2.3220\n",
      "Epoch 155/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2895 - root_mean_squared_error: 2.4765 - val_loss: 13.1489 - val_root_mean_squared_error: 2.2402\n",
      "Epoch 156/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3118 - root_mean_squared_error: 2.4825 - val_loss: 12.9660 - val_root_mean_squared_error: 2.2168\n",
      "Epoch 157/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1652 - root_mean_squared_error: 2.4511 - val_loss: 13.4080 - val_root_mean_squared_error: 2.2700\n",
      "Epoch 158/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3104 - root_mean_squared_error: 2.4781 - val_loss: 13.4708 - val_root_mean_squared_error: 2.3193\n",
      "Epoch 159/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1583 - root_mean_squared_error: 2.4503 - val_loss: 13.1039 - val_root_mean_squared_error: 2.2113\n",
      "Epoch 160/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2286 - root_mean_squared_error: 2.4669 - val_loss: 13.1477 - val_root_mean_squared_error: 2.2452\n",
      "Epoch 161/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3188 - root_mean_squared_error: 2.4813 - val_loss: 13.1293 - val_root_mean_squared_error: 2.2288\n",
      "Epoch 162/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2004 - root_mean_squared_error: 2.4510 - val_loss: 13.2493 - val_root_mean_squared_error: 2.2456\n",
      "Epoch 163/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2939 - root_mean_squared_error: 2.4689 - val_loss: 13.4226 - val_root_mean_squared_error: 2.2739\n",
      "Epoch 164/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1295 - root_mean_squared_error: 2.4390 - val_loss: 13.6188 - val_root_mean_squared_error: 2.3105\n",
      "Epoch 165/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2221 - root_mean_squared_error: 2.4544 - val_loss: 13.7268 - val_root_mean_squared_error: 2.3523\n",
      "Epoch 166/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1563 - root_mean_squared_error: 2.4459 - val_loss: 13.5687 - val_root_mean_squared_error: 2.3094\n",
      "Epoch 167/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2429 - root_mean_squared_error: 2.4580 - val_loss: 13.4173 - val_root_mean_squared_error: 2.2798\n",
      "Epoch 168/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1217 - root_mean_squared_error: 2.4333 - val_loss: 13.2813 - val_root_mean_squared_error: 2.2684\n",
      "Epoch 169/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3380 - root_mean_squared_error: 2.4715 - val_loss: 13.2077 - val_root_mean_squared_error: 2.2453\n",
      "Epoch 170/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1440 - root_mean_squared_error: 2.4362 - val_loss: 13.3606 - val_root_mean_squared_error: 2.2573\n",
      "Epoch 171/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2674 - root_mean_squared_error: 2.4558 - val_loss: 13.3151 - val_root_mean_squared_error: 2.2597\n",
      "Epoch 172/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2145 - root_mean_squared_error: 2.4413 - val_loss: 13.3861 - val_root_mean_squared_error: 2.2634\n",
      "Epoch 173/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2036 - root_mean_squared_error: 2.4433 - val_loss: 13.2658 - val_root_mean_squared_error: 2.2368\n",
      "Epoch 174/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3134 - root_mean_squared_error: 2.4725 - val_loss: 13.3201 - val_root_mean_squared_error: 2.2609\n",
      "Epoch 175/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2773 - root_mean_squared_error: 2.4475 - val_loss: 13.6518 - val_root_mean_squared_error: 2.3251\n",
      "Epoch 176/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1687 - root_mean_squared_error: 2.4322 - val_loss: 13.4553 - val_root_mean_squared_error: 2.2812\n",
      "Epoch 177/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2593 - root_mean_squared_error: 2.4542 - val_loss: 13.3414 - val_root_mean_squared_error: 2.2532\n",
      "Epoch 178/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1865 - root_mean_squared_error: 2.4319 - val_loss: 13.4609 - val_root_mean_squared_error: 2.3001\n",
      "Epoch 179/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2123 - root_mean_squared_error: 2.4378 - val_loss: 13.4325 - val_root_mean_squared_error: 2.2827\n",
      "Epoch 180/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1724 - root_mean_squared_error: 2.4357 - val_loss: 13.2773 - val_root_mean_squared_error: 2.2256\n",
      "Epoch 181/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3305 - root_mean_squared_error: 2.4592 - val_loss: 13.2300 - val_root_mean_squared_error: 2.2311\n",
      "Epoch 182/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2308 - root_mean_squared_error: 2.4365 - val_loss: 13.2625 - val_root_mean_squared_error: 2.2390\n",
      "Epoch 183/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4410 - root_mean_squared_error: 2.4782 - val_loss: 13.3742 - val_root_mean_squared_error: 2.2635\n",
      "Epoch 184/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3448 - root_mean_squared_error: 2.4642 - val_loss: 13.2598 - val_root_mean_squared_error: 2.2611\n",
      "Epoch 185/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3750 - root_mean_squared_error: 2.4685 - val_loss: 13.6960 - val_root_mean_squared_error: 2.3284\n",
      "Epoch 186/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2835 - root_mean_squared_error: 2.4521 - val_loss: 13.3356 - val_root_mean_squared_error: 2.2677\n",
      "Epoch 187/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3040 - root_mean_squared_error: 2.4554 - val_loss: 13.7124 - val_root_mean_squared_error: 2.3245\n",
      "Epoch 188/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4086 - root_mean_squared_error: 2.4689 - val_loss: 13.4581 - val_root_mean_squared_error: 2.2806\n",
      "Epoch 189/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3623 - root_mean_squared_error: 2.4653 - val_loss: 13.3172 - val_root_mean_squared_error: 2.2600\n",
      "Epoch 190/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3761 - root_mean_squared_error: 2.4625 - val_loss: 13.4206 - val_root_mean_squared_error: 2.2831\n",
      "Epoch 191/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1876 - root_mean_squared_error: 2.4336 - val_loss: 13.2322 - val_root_mean_squared_error: 2.2391\n",
      "Epoch 192/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1916 - root_mean_squared_error: 2.4362 - val_loss: 13.3458 - val_root_mean_squared_error: 2.2684\n",
      "Epoch 193/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1961 - root_mean_squared_error: 2.4348 - val_loss: 13.6865 - val_root_mean_squared_error: 2.3084\n",
      "Epoch 194/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3660 - root_mean_squared_error: 2.4658 - val_loss: 13.4557 - val_root_mean_squared_error: 2.2747\n",
      "Epoch 195/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1637 - root_mean_squared_error: 2.4307 - val_loss: 13.4475 - val_root_mean_squared_error: 2.2785\n",
      "Epoch 196/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2845 - root_mean_squared_error: 2.4465 - val_loss: 13.8869 - val_root_mean_squared_error: 2.3528\n",
      "Epoch 197/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4236 - root_mean_squared_error: 2.4670 - val_loss: 13.2467 - val_root_mean_squared_error: 2.2167\n",
      "Epoch 198/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2772 - root_mean_squared_error: 2.4463 - val_loss: 13.4869 - val_root_mean_squared_error: 2.2761\n",
      "Epoch 199/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.5044 - root_mean_squared_error: 2.4881 - val_loss: 13.7410 - val_root_mean_squared_error: 2.3439\n",
      "Epoch 200/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3525 - root_mean_squared_error: 2.4687 - val_loss: 13.4257 - val_root_mean_squared_error: 2.2664\n",
      "Epoch 201/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3614 - root_mean_squared_error: 2.4637 - val_loss: 13.5216 - val_root_mean_squared_error: 2.2961\n",
      "Epoch 202/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2486 - root_mean_squared_error: 2.4412 - val_loss: 13.4806 - val_root_mean_squared_error: 2.2878\n",
      "Epoch 203/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3148 - root_mean_squared_error: 2.4613 - val_loss: 13.0685 - val_root_mean_squared_error: 2.1960\n",
      "Epoch 204/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4197 - root_mean_squared_error: 2.4666 - val_loss: 13.1941 - val_root_mean_squared_error: 2.2307\n",
      "Epoch 205/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3333 - root_mean_squared_error: 2.4536 - val_loss: 13.5260 - val_root_mean_squared_error: 2.2919\n",
      "Epoch 206/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2608 - root_mean_squared_error: 2.4387 - val_loss: 13.7109 - val_root_mean_squared_error: 2.3217\n",
      "Epoch 207/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3266 - root_mean_squared_error: 2.4542 - val_loss: 13.5032 - val_root_mean_squared_error: 2.2809\n",
      "Epoch 208/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3795 - root_mean_squared_error: 2.4627 - val_loss: 13.4686 - val_root_mean_squared_error: 2.2632\n",
      "Epoch 209/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4045 - root_mean_squared_error: 2.4689 - val_loss: 13.6037 - val_root_mean_squared_error: 2.2871\n",
      "Epoch 210/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2721 - root_mean_squared_error: 2.4472 - val_loss: 13.4220 - val_root_mean_squared_error: 2.2781\n",
      "Epoch 211/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3154 - root_mean_squared_error: 2.4533 - val_loss: 13.5362 - val_root_mean_squared_error: 2.2799\n",
      "Epoch 212/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2929 - root_mean_squared_error: 2.4455 - val_loss: 13.5240 - val_root_mean_squared_error: 2.3003\n",
      "Epoch 213/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3010 - root_mean_squared_error: 2.4495 - val_loss: 13.2762 - val_root_mean_squared_error: 2.2484\n",
      "Epoch 214/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3725 - root_mean_squared_error: 2.4619 - val_loss: 13.3375 - val_root_mean_squared_error: 2.2517\n",
      "Epoch 215/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3008 - root_mean_squared_error: 2.4447 - val_loss: 13.5319 - val_root_mean_squared_error: 2.2783\n",
      "Epoch 216/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3741 - root_mean_squared_error: 2.4660 - val_loss: 13.5051 - val_root_mean_squared_error: 2.2683\n",
      "Epoch 217/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2908 - root_mean_squared_error: 2.4531 - val_loss: 13.2863 - val_root_mean_squared_error: 2.2246\n",
      "Epoch 218/10000\n",
      "344/344 [==============================] - 45s 129ms/step - loss: 14.2568 - root_mean_squared_error: 2.4398 - val_loss: 13.2265 - val_root_mean_squared_error: 2.2319\n",
      "Epoch 219/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2271 - root_mean_squared_error: 2.4313 - val_loss: 13.7621 - val_root_mean_squared_error: 2.3285\n",
      "Epoch 220/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1975 - root_mean_squared_error: 2.4324 - val_loss: 13.6866 - val_root_mean_squared_error: 2.2856\n",
      "Epoch 221/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3236 - root_mean_squared_error: 2.4541 - val_loss: 13.4693 - val_root_mean_squared_error: 2.2635\n",
      "Epoch 222/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1978 - root_mean_squared_error: 2.4329 - val_loss: 13.5534 - val_root_mean_squared_error: 2.2969\n",
      "Epoch 223/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2227 - root_mean_squared_error: 2.4304 - val_loss: 13.5982 - val_root_mean_squared_error: 2.2849\n",
      "Epoch 224/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3306 - root_mean_squared_error: 2.4592 - val_loss: 13.6640 - val_root_mean_squared_error: 2.3190\n",
      "Epoch 225/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2377 - root_mean_squared_error: 2.4332 - val_loss: 13.5015 - val_root_mean_squared_error: 2.2781\n",
      "Epoch 226/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2388 - root_mean_squared_error: 2.4398 - val_loss: 13.4739 - val_root_mean_squared_error: 2.2747\n",
      "Epoch 227/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2505 - root_mean_squared_error: 2.4445 - val_loss: 13.3126 - val_root_mean_squared_error: 2.2414\n",
      "Epoch 228/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2887 - root_mean_squared_error: 2.4499 - val_loss: 13.4734 - val_root_mean_squared_error: 2.2756\n",
      "Epoch 229/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3096 - root_mean_squared_error: 2.4442 - val_loss: 13.3655 - val_root_mean_squared_error: 2.2375\n",
      "Epoch 230/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2584 - root_mean_squared_error: 2.4425 - val_loss: 13.7220 - val_root_mean_squared_error: 2.3191\n",
      "Epoch 231/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3849 - root_mean_squared_error: 2.4614 - val_loss: 13.6547 - val_root_mean_squared_error: 2.3053\n",
      "Epoch 232/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2335 - root_mean_squared_error: 2.4333 - val_loss: 13.6686 - val_root_mean_squared_error: 2.2847\n",
      "Epoch 233/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3236 - root_mean_squared_error: 2.4511 - val_loss: 13.7624 - val_root_mean_squared_error: 2.3376\n",
      "Epoch 234/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2901 - root_mean_squared_error: 2.4479 - val_loss: 13.6077 - val_root_mean_squared_error: 2.2823\n",
      "Epoch 235/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2675 - root_mean_squared_error: 2.4472 - val_loss: 13.2885 - val_root_mean_squared_error: 2.2277\n",
      "Epoch 236/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4012 - root_mean_squared_error: 2.4704 - val_loss: 13.3809 - val_root_mean_squared_error: 2.2609\n",
      "Epoch 237/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3112 - root_mean_squared_error: 2.4488 - val_loss: 13.3822 - val_root_mean_squared_error: 2.2648\n",
      "Epoch 238/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1485 - root_mean_squared_error: 2.4304 - val_loss: 13.8307 - val_root_mean_squared_error: 2.3517\n",
      "Epoch 239/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3617 - root_mean_squared_error: 2.4580 - val_loss: 13.6572 - val_root_mean_squared_error: 2.3199\n",
      "Epoch 240/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3223 - root_mean_squared_error: 2.4595 - val_loss: 13.2564 - val_root_mean_squared_error: 2.2324\n",
      "Epoch 241/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4281 - root_mean_squared_error: 2.4610 - val_loss: 13.4804 - val_root_mean_squared_error: 2.2675\n",
      "Epoch 242/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1571 - root_mean_squared_error: 2.4287 - val_loss: 13.4649 - val_root_mean_squared_error: 2.2673\n",
      "Epoch 243/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3165 - root_mean_squared_error: 2.4481 - val_loss: 13.4041 - val_root_mean_squared_error: 2.2522\n",
      "Epoch 244/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2520 - root_mean_squared_error: 2.4387 - val_loss: 13.2367 - val_root_mean_squared_error: 2.2557\n",
      "Epoch 245/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2468 - root_mean_squared_error: 2.4369 - val_loss: 13.6974 - val_root_mean_squared_error: 2.3364\n",
      "Epoch 246/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3211 - root_mean_squared_error: 2.4518 - val_loss: 13.4413 - val_root_mean_squared_error: 2.2671\n",
      "Epoch 247/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2252 - root_mean_squared_error: 2.4316 - val_loss: 13.3708 - val_root_mean_squared_error: 2.2522\n",
      "Epoch 248/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2229 - root_mean_squared_error: 2.4372 - val_loss: 13.6753 - val_root_mean_squared_error: 2.3049\n",
      "Epoch 249/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2214 - root_mean_squared_error: 2.4325 - val_loss: 13.5760 - val_root_mean_squared_error: 2.2963\n",
      "Epoch 250/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2392 - root_mean_squared_error: 2.4339 - val_loss: 13.5148 - val_root_mean_squared_error: 2.2905\n",
      "Epoch 251/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1558 - root_mean_squared_error: 2.4182 - val_loss: 13.8486 - val_root_mean_squared_error: 2.3400\n",
      "Epoch 252/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1471 - root_mean_squared_error: 2.4201 - val_loss: 13.5195 - val_root_mean_squared_error: 2.2776\n",
      "Epoch 253/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3006 - root_mean_squared_error: 2.4470 - val_loss: 13.2847 - val_root_mean_squared_error: 2.2474\n",
      "Epoch 254/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3334 - root_mean_squared_error: 2.4567 - val_loss: 13.5600 - val_root_mean_squared_error: 2.3029\n",
      "Epoch 255/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1477 - root_mean_squared_error: 2.4136 - val_loss: 13.2402 - val_root_mean_squared_error: 2.2353\n",
      "Epoch 256/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0905 - root_mean_squared_error: 2.4181 - val_loss: 13.4972 - val_root_mean_squared_error: 2.2664\n",
      "Epoch 257/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2153 - root_mean_squared_error: 2.4384 - val_loss: 13.3611 - val_root_mean_squared_error: 2.2604\n",
      "Epoch 258/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1919 - root_mean_squared_error: 2.4279 - val_loss: 13.8797 - val_root_mean_squared_error: 2.3605\n",
      "Epoch 259/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2629 - root_mean_squared_error: 2.4353 - val_loss: 13.5525 - val_root_mean_squared_error: 2.2802\n",
      "Epoch 260/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2660 - root_mean_squared_error: 2.4442 - val_loss: 13.4218 - val_root_mean_squared_error: 2.2663\n",
      "Epoch 261/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3034 - root_mean_squared_error: 2.4524 - val_loss: 13.2362 - val_root_mean_squared_error: 2.2191\n",
      "Epoch 262/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3236 - root_mean_squared_error: 2.4562 - val_loss: 13.2966 - val_root_mean_squared_error: 2.2453\n",
      "Epoch 263/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2967 - root_mean_squared_error: 2.4466 - val_loss: 13.4861 - val_root_mean_squared_error: 2.2850\n",
      "Epoch 264/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3156 - root_mean_squared_error: 2.4553 - val_loss: 13.3473 - val_root_mean_squared_error: 2.2386\n",
      "Epoch 265/10000\n",
      "344/344 [==============================] - 45s 130ms/step - loss: 14.2773 - root_mean_squared_error: 2.4487 - val_loss: 13.6596 - val_root_mean_squared_error: 2.3216\n",
      "Epoch 266/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2479 - root_mean_squared_error: 2.4484 - val_loss: 13.3479 - val_root_mean_squared_error: 2.2696\n",
      "Epoch 267/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3146 - root_mean_squared_error: 2.4556 - val_loss: 13.5267 - val_root_mean_squared_error: 2.2957\n",
      "Epoch 268/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3882 - root_mean_squared_error: 2.4669 - val_loss: 13.2580 - val_root_mean_squared_error: 2.2184\n",
      "Epoch 269/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1628 - root_mean_squared_error: 2.4211 - val_loss: 13.6469 - val_root_mean_squared_error: 2.3113\n",
      "Epoch 270/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4535 - root_mean_squared_error: 2.4806 - val_loss: 13.2442 - val_root_mean_squared_error: 2.2257\n",
      "Epoch 271/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 14.4102 - root_mean_squared_error: 2.4680 - val_loss: 13.2622 - val_root_mean_squared_error: 2.2499\n",
      "Epoch 272/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2664 - root_mean_squared_error: 2.4502 - val_loss: 13.3522 - val_root_mean_squared_error: 2.2537\n",
      "Epoch 273/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2233 - root_mean_squared_error: 2.4378 - val_loss: 13.8862 - val_root_mean_squared_error: 2.3703\n",
      "Epoch 274/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2591 - root_mean_squared_error: 2.4523 - val_loss: 13.5468 - val_root_mean_squared_error: 2.2932\n",
      "Epoch 275/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1238 - root_mean_squared_error: 2.4156 - val_loss: 13.2161 - val_root_mean_squared_error: 2.2328\n",
      "Epoch 276/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2285 - root_mean_squared_error: 2.4390 - val_loss: 13.5628 - val_root_mean_squared_error: 2.2922\n",
      "Epoch 277/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1900 - root_mean_squared_error: 2.4299 - val_loss: 13.4749 - val_root_mean_squared_error: 2.2855\n",
      "Epoch 278/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2043 - root_mean_squared_error: 2.4322 - val_loss: 13.5489 - val_root_mean_squared_error: 2.3121\n",
      "Epoch 279/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1074 - root_mean_squared_error: 2.4182 - val_loss: 13.3405 - val_root_mean_squared_error: 2.2458\n",
      "Epoch 280/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1882 - root_mean_squared_error: 2.4271 - val_loss: 13.3286 - val_root_mean_squared_error: 2.2486\n",
      "Epoch 281/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3248 - root_mean_squared_error: 2.4544 - val_loss: 13.4434 - val_root_mean_squared_error: 2.2712\n",
      "Epoch 282/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1642 - root_mean_squared_error: 2.4325 - val_loss: 13.1904 - val_root_mean_squared_error: 2.2133\n",
      "Epoch 283/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1740 - root_mean_squared_error: 2.4388 - val_loss: 13.3661 - val_root_mean_squared_error: 2.2477\n",
      "Epoch 284/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2230 - root_mean_squared_error: 2.4448 - val_loss: 13.3729 - val_root_mean_squared_error: 2.2524\n",
      "Epoch 285/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1375 - root_mean_squared_error: 2.4155 - val_loss: 13.2335 - val_root_mean_squared_error: 2.2498\n",
      "Epoch 286/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1334 - root_mean_squared_error: 2.4131 - val_loss: 13.5324 - val_root_mean_squared_error: 2.3166\n",
      "Epoch 287/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1277 - root_mean_squared_error: 2.4183 - val_loss: 13.4628 - val_root_mean_squared_error: 2.2991\n",
      "Epoch 288/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2593 - root_mean_squared_error: 2.4446 - val_loss: 13.2507 - val_root_mean_squared_error: 2.2350\n",
      "Epoch 289/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2158 - root_mean_squared_error: 2.4403 - val_loss: 13.2036 - val_root_mean_squared_error: 2.2208\n",
      "Epoch 290/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1977 - root_mean_squared_error: 2.4317 - val_loss: 12.9558 - val_root_mean_squared_error: 2.1673\n",
      "Epoch 291/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1621 - root_mean_squared_error: 2.4295 - val_loss: 13.2144 - val_root_mean_squared_error: 2.2342\n",
      "Epoch 292/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2305 - root_mean_squared_error: 2.4421 - val_loss: 13.1451 - val_root_mean_squared_error: 2.2141\n",
      "Epoch 293/10000\n",
      "344/344 [==============================] - 45s 129ms/step - loss: 14.2754 - root_mean_squared_error: 2.4467 - val_loss: 13.3906 - val_root_mean_squared_error: 2.2595\n",
      "Epoch 294/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2533 - root_mean_squared_error: 2.4446 - val_loss: 13.3982 - val_root_mean_squared_error: 2.2638\n",
      "Epoch 295/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1329 - root_mean_squared_error: 2.4245 - val_loss: 13.6941 - val_root_mean_squared_error: 2.3339\n",
      "Epoch 296/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2134 - root_mean_squared_error: 2.4346 - val_loss: 13.2363 - val_root_mean_squared_error: 2.2265\n",
      "Epoch 297/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1656 - root_mean_squared_error: 2.4299 - val_loss: 13.6871 - val_root_mean_squared_error: 2.3129\n",
      "Epoch 298/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2217 - root_mean_squared_error: 2.4414 - val_loss: 13.1267 - val_root_mean_squared_error: 2.2118\n",
      "Epoch 299/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3126 - root_mean_squared_error: 2.4599 - val_loss: 13.3744 - val_root_mean_squared_error: 2.2716\n",
      "Epoch 300/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2174 - root_mean_squared_error: 2.4428 - val_loss: 13.3885 - val_root_mean_squared_error: 2.2559\n",
      "Epoch 301/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2608 - root_mean_squared_error: 2.4559 - val_loss: 13.4706 - val_root_mean_squared_error: 2.2825\n",
      "Epoch 302/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2388 - root_mean_squared_error: 2.4441 - val_loss: 13.4981 - val_root_mean_squared_error: 2.2715\n",
      "Epoch 303/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2598 - root_mean_squared_error: 2.4484 - val_loss: 13.3965 - val_root_mean_squared_error: 2.2634\n",
      "Epoch 304/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.3299 - root_mean_squared_error: 2.4669 - val_loss: 13.4382 - val_root_mean_squared_error: 2.2805\n",
      "Epoch 305/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1841 - root_mean_squared_error: 2.4344 - val_loss: 13.3252 - val_root_mean_squared_error: 2.2417\n",
      "Epoch 306/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1270 - root_mean_squared_error: 2.4262 - val_loss: 13.2564 - val_root_mean_squared_error: 2.2397\n",
      "Epoch 307/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0990 - root_mean_squared_error: 2.4308 - val_loss: 13.3547 - val_root_mean_squared_error: 2.2555\n",
      "Epoch 308/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1815 - root_mean_squared_error: 2.4292 - val_loss: 13.2911 - val_root_mean_squared_error: 2.2406\n",
      "Epoch 309/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2406 - root_mean_squared_error: 2.4469 - val_loss: 12.8742 - val_root_mean_squared_error: 2.1631\n",
      "Epoch 310/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1864 - root_mean_squared_error: 2.4422 - val_loss: 13.3403 - val_root_mean_squared_error: 2.2601\n",
      "Epoch 311/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1772 - root_mean_squared_error: 2.4340 - val_loss: 13.4153 - val_root_mean_squared_error: 2.2795\n",
      "Epoch 312/10000\n",
      "344/344 [==============================] - 45s 129ms/step - loss: 14.1357 - root_mean_squared_error: 2.4266 - val_loss: 13.2931 - val_root_mean_squared_error: 2.2552\n",
      "Epoch 313/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0753 - root_mean_squared_error: 2.4198 - val_loss: 13.2234 - val_root_mean_squared_error: 2.2562\n",
      "Epoch 314/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1324 - root_mean_squared_error: 2.4318 - val_loss: 13.6020 - val_root_mean_squared_error: 2.2939\n",
      "Epoch 315/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.1426 - root_mean_squared_error: 2.4400 - val_loss: 13.2781 - val_root_mean_squared_error: 2.2527\n",
      "Epoch 316/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 44s 129ms/step - loss: 14.0451 - root_mean_squared_error: 2.4162 - val_loss: 13.4124 - val_root_mean_squared_error: 2.2616\n",
      "Epoch 317/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2397 - root_mean_squared_error: 2.4468 - val_loss: 13.1285 - val_root_mean_squared_error: 2.2214\n",
      "Epoch 318/10000\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 14.2460 - root_mean_squared_error: 2.4524 - val_loss: 13.1732 - val_root_mean_squared_error: 2.2246\n",
      "Epoch 319/10000\n",
      "227/344 [==================>...........] - ETA: 14s - loss: 14.0051 - root_mean_squared_error: 2.4070"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16977/652742571.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \"\"\"\n\u001b[1;32m   1154\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/huiEnv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_tra, y_tra, batch_size = BATCH_SIZE, epochs=EPOCHS, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a287b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_tra, y_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810a348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted = []\n",
    "iterations = 5\n",
    "for i in range(iterations):\n",
    "    predicted.append(model.predict(e_test_x))\n",
    "predicted = np.concatenate(predicted, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e109517",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_mean = np.mean(predicted, axis=1).tolist()\n",
    "prediction_min = np.min(predicted, axis=1).tolist()\n",
    "prediction_max = np.max(predicted, axis=1).tolist()\n",
    "prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.evaluate(x_tra, y_tra)\n",
    "t = model.evaluate(e_test_x, e_test_y)\n",
    "line_test = [str(v), str(t)]\n",
    "for idx in range(len(e_test_x)):\n",
    "    s = e_test_name[idx] + \" \" +f\"Predictions mean: {round(prediction_mean[idx], 2)}, \" + f\"min: {round(prediction_min[idx], 2)}, \" + f\"max: {round(prediction_max[idx], 2)}, \" + f\"range: {round(prediction_range[idx], 2)} - \" + f\"Actual: {e_test_y[idx]}\"\n",
    "    line_test.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('bnn_density_d.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bnn_density_d.txt', 'w') as f:\n",
    "    for line in line_test:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f76b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e43e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b7400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
